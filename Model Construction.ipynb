{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071505dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annotator</th>\n",
       "      <th>Paper</th>\n",
       "      <th>Cited-by</th>\n",
       "      <th>Follow-up</th>\n",
       "      <th>Citing Sentence</th>\n",
       "      <th>Tagged Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>A00-1043</td>\n",
       "      <td>C00-2140</td>\n",
       "      <td>0</td>\n",
       "      <td>Since we only use shallow methods for textual ...</td>\n",
       "      <td>Since we only use shallow methods for textual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>A00-1043</td>\n",
       "      <td>P02-1057</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentence simplification systems (Chandrasekar ...</td>\n",
       "      <td>Sentence simplification systems (GTREF) are ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>W09-1118</td>\n",
       "      <td>1</td>\n",
       "      <td>Each token is represented using a fairly stand...</td>\n",
       "      <td>Each token is represented using a fairly stand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>A00-2017</td>\n",
       "      <td>1</td>\n",
       "      <td>The training and the test data were processed ...</td>\n",
       "      <td>The training and the test data were processed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>C00-2099</td>\n",
       "      <td>0</td>\n",
       "      <td>The only other high-\f",
       "delity computational rend...</td>\n",
       "      <td>The only other high-_x005f\f",
       "delity computationa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Annotator     Paper  Cited-by  Follow-up  \\\n",
       "0         A  A00-1043  C00-2140          0   \n",
       "1         A  A00-1043  P02-1057          0   \n",
       "2         A  A97-1011  W09-1118          1   \n",
       "3         A  A97-1011  A00-2017          1   \n",
       "4         A  A97-1011  C00-2099          0   \n",
       "\n",
       "                                     Citing Sentence  \\\n",
       "0  Since we only use shallow methods for textual ...   \n",
       "1  Sentence simplification systems (Chandrasekar ...   \n",
       "2  Each token is represented using a fairly stand...   \n",
       "3  The training and the test data were processed ...   \n",
       "4  The only other high-\n",
       "delity computational rend...   \n",
       "\n",
       "                                     Tagged Sentence  \n",
       "0  Since we only use shallow methods for textual ...  \n",
       "1  Sentence simplification systems (GTREF) are ca...  \n",
       "2  Each token is represented using a fairly stand...  \n",
       "3  The training and the test data were processed ...  \n",
       "4  The only other high-_x005f\n",
       "delity computationa...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import math\n",
    "import os\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "df = pd.read_csv(\"/Users/revglue/study/main_work/my_working/my_data_set/my_data_set.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175ee5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Citation Type']=df['Follow-up'].apply(lambda x: 'Related work' if x==0 else ('Comparison' if x==1 else ('Using the work' if x==2 else 'Extending the work')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "675efe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def softmax(z):\n",
    "    z_exp = [math.exp(i) for i in z]\n",
    "    sum_z_exp = sum(z_exp)\n",
    "    return [i / sum_z_exp for i in z_exp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0fce35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annotator</th>\n",
       "      <th>Paper</th>\n",
       "      <th>Cited-by</th>\n",
       "      <th>Follow-up</th>\n",
       "      <th>Citing Sentence</th>\n",
       "      <th>Tagged Sentence</th>\n",
       "      <th>Citation Type</th>\n",
       "      <th>Coarse Label</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>A00-1043</td>\n",
       "      <td>C00-2140</td>\n",
       "      <td>0</td>\n",
       "      <td>Since we only use shallow methods for textual ...</td>\n",
       "      <td>Since we only use shallow methods for textual ...</td>\n",
       "      <td>Related work</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>A00-1043</td>\n",
       "      <td>P02-1057</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentence simplification systems (Chandrasekar ...</td>\n",
       "      <td>Sentence simplification systems (GTREF) are ca...</td>\n",
       "      <td>Related work</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>W09-1118</td>\n",
       "      <td>1</td>\n",
       "      <td>Each token is represented using a fairly stand...</td>\n",
       "      <td>Each token is represented using a fairly stand...</td>\n",
       "      <td>Comparison</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.001380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>A00-2017</td>\n",
       "      <td>1</td>\n",
       "      <td>The training and the test data were processed ...</td>\n",
       "      <td>The training and the test data were processed ...</td>\n",
       "      <td>Comparison</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.001380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>C00-2099</td>\n",
       "      <td>0</td>\n",
       "      <td>The only other high-\f",
       "delity computational rend...</td>\n",
       "      <td>The only other high-_x005f\f",
       "delity computationa...</td>\n",
       "      <td>Related work</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>W04-1505</td>\n",
       "      <td>0</td>\n",
       "      <td>The Statistical Dependency Model Most successf...</td>\n",
       "      <td>The Statistical Dependency Model Most successf...</td>\n",
       "      <td>Related work</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>P99-1033</td>\n",
       "      <td>0</td>\n",
       "      <td>J~irvinen and Tapananinen have demonstrated an...</td>\n",
       "      <td>J~irvinen and Tapananinen have demonstrated an...</td>\n",
       "      <td>Related work</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>W06-0202</td>\n",
       "      <td>3</td>\n",
       "      <td>Generating Dependency Patterns Three dependenc...</td>\n",
       "      <td>Generating Dependency Patterns Three dependenc...</td>\n",
       "      <td>Extending the work</td>\n",
       "      <td>Important</td>\n",
       "      <td>0.010196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>P01-1006</td>\n",
       "      <td>3</td>\n",
       "      <td>Pre-processing tools Parser The current versio...</td>\n",
       "      <td>Pre-processing tools Parser The current versio...</td>\n",
       "      <td>Extending the work</td>\n",
       "      <td>Important</td>\n",
       "      <td>0.010196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>E12-1072</td>\n",
       "      <td>0</td>\n",
       "      <td>The corpus was parsed by Connexor’s Machinese ...</td>\n",
       "      <td>The corpus was parsed by Connexor’s Machinese ...</td>\n",
       "      <td>Related work</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A</td>\n",
       "      <td>C00-1072</td>\n",
       "      <td>C02-1130</td>\n",
       "      <td>2</td>\n",
       "      <td>A topic signature, as described in (Lin and Ho...</td>\n",
       "      <td>A topic signature, as described in TREF, is a ...</td>\n",
       "      <td>Using the work</td>\n",
       "      <td>Important</td>\n",
       "      <td>0.003751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A</td>\n",
       "      <td>C00-1072</td>\n",
       "      <td>C04-1077</td>\n",
       "      <td>1</td>\n",
       "      <td>This is an extension of Lin’s method (Lin and ...</td>\n",
       "      <td>This is an extension of Lin’s method TREF.</td>\n",
       "      <td>Comparison</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.001380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A</td>\n",
       "      <td>C00-1072</td>\n",
       "      <td>C08-1021</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic Signatures Topic Signatures (TS) are wor...</td>\n",
       "      <td>Topic Signatures Topic Signatures (TS) are wor...</td>\n",
       "      <td>Comparison</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.001380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A</td>\n",
       "      <td>C00-1072</td>\n",
       "      <td>C08-1124</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently, content features were also well stud...</td>\n",
       "      <td>Recently, content features were also well stud...</td>\n",
       "      <td>Comparison</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.001380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A</td>\n",
       "      <td>C00-1072</td>\n",
       "      <td>D08-1080</td>\n",
       "      <td>0</td>\n",
       "      <td>More advanced methods for query expansion use ...</td>\n",
       "      <td>More advanced methods for query expansion use ...</td>\n",
       "      <td>Related work</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Annotator     Paper  Cited-by  Follow-up  \\\n",
       "0          A  A00-1043  C00-2140          0   \n",
       "1          A  A00-1043  P02-1057          0   \n",
       "2          A  A97-1011  W09-1118          1   \n",
       "3          A  A97-1011  A00-2017          1   \n",
       "4          A  A97-1011  C00-2099          0   \n",
       "5          A  A97-1011  W04-1505          0   \n",
       "6          A  A97-1011  P99-1033          0   \n",
       "7          A  A97-1011  W06-0202          3   \n",
       "8          A  A97-1011  P01-1006          3   \n",
       "9          A  A97-1011  E12-1072          0   \n",
       "10         A  C00-1072  C02-1130          2   \n",
       "11         A  C00-1072  C04-1077          1   \n",
       "12         A  C00-1072  C08-1021          1   \n",
       "13         A  C00-1072  C08-1124          1   \n",
       "14         A  C00-1072  D08-1080          0   \n",
       "\n",
       "                                      Citing Sentence  \\\n",
       "0   Since we only use shallow methods for textual ...   \n",
       "1   Sentence simplification systems (Chandrasekar ...   \n",
       "2   Each token is represented using a fairly stand...   \n",
       "3   The training and the test data were processed ...   \n",
       "4   The only other high-\n",
       "delity computational rend...   \n",
       "5   The Statistical Dependency Model Most successf...   \n",
       "6   J~irvinen and Tapananinen have demonstrated an...   \n",
       "7   Generating Dependency Patterns Three dependenc...   \n",
       "8   Pre-processing tools Parser The current versio...   \n",
       "9   The corpus was parsed by Connexor’s Machinese ...   \n",
       "10  A topic signature, as described in (Lin and Ho...   \n",
       "11  This is an extension of Lin’s method (Lin and ...   \n",
       "12  Topic Signatures Topic Signatures (TS) are wor...   \n",
       "13  Recently, content features were also well stud...   \n",
       "14  More advanced methods for query expansion use ...   \n",
       "\n",
       "                                      Tagged Sentence       Citation Type  \\\n",
       "0   Since we only use shallow methods for textual ...        Related work   \n",
       "1   Sentence simplification systems (GTREF) are ca...        Related work   \n",
       "2   Each token is represented using a fairly stand...          Comparison   \n",
       "3   The training and the test data were processed ...          Comparison   \n",
       "4   The only other high-_x005f\n",
       "delity computationa...        Related work   \n",
       "5   The Statistical Dependency Model Most successf...        Related work   \n",
       "6   J~irvinen and Tapananinen have demonstrated an...        Related work   \n",
       "7   Generating Dependency Patterns Three dependenc...  Extending the work   \n",
       "8   Pre-processing tools Parser The current versio...  Extending the work   \n",
       "9   The corpus was parsed by Connexor’s Machinese ...        Related work   \n",
       "10  A topic signature, as described in TREF, is a ...      Using the work   \n",
       "11         This is an extension of Lin’s method TREF.          Comparison   \n",
       "12  Topic Signatures Topic Signatures (TS) are wor...          Comparison   \n",
       "13  Recently, content features were also well stud...          Comparison   \n",
       "14  More advanced methods for query expansion use ...        Related work   \n",
       "\n",
       "   Coarse Label  normalized  \n",
       "0    Incidental    0.000508  \n",
       "1    Incidental    0.000508  \n",
       "2    Incidental    0.001380  \n",
       "3    Incidental    0.001380  \n",
       "4    Incidental    0.000508  \n",
       "5    Incidental    0.000508  \n",
       "6    Incidental    0.000508  \n",
       "7     Important    0.010196  \n",
       "8     Important    0.010196  \n",
       "9    Incidental    0.000508  \n",
       "10    Important    0.003751  \n",
       "11   Incidental    0.001380  \n",
       "12   Incidental    0.001380  \n",
       "13   Incidental    0.001380  \n",
       "14   Incidental    0.000508  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Coarse Label']=df['Follow-up'].apply(lambda x: 'Incidental' if x==0 or x==1 else 'Important')\n",
    "df['normalized']=softmax(df['Follow-up'])\n",
    "\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03aa5e13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    283\n",
       "0    252\n",
       "2     55\n",
       "3     27\n",
       "Name: Follow-up, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Follow-up'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd5c0872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Comparison': 283, 'Related work': 252, 'Using the work': 55, 'Extending the work': 27})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "categories = df[['Citation Type']].values.reshape(-1)\n",
    "\n",
    "counter_categories = Counter(categories)\n",
    "category_names = counter_categories.keys()\n",
    "category_values = counter_categories.values()\n",
    "\n",
    "y_pos = np.arange(len(category_names))\n",
    "\n",
    "plt.figure(1, figsize=(10, 5))\n",
    "plt.bar(y_pos, category_values, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, category_names)\n",
    "plt.ylabel('Number of citations')\n",
    "plt.title('Distribution of citations per category')\n",
    "plt.gca().yaxis.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(counter_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "989cbad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(df, split_char=' '):\n",
    "    categories = df['Citation Type'].unique()\n",
    "    \n",
    "    all_lengths = []\n",
    "    per_category = {\n",
    "        'lengths': {c:[] for c in categories},\n",
    "        'mean': {c:0 for c in categories},\n",
    "        'stdev': {c:0 for c in categories}\n",
    "    }\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['Tagged Sentence']\n",
    "        text = re.sub(r\"\\s+\", ' ', text) # Normalize\n",
    "        text = text.split(split_char)\n",
    "        l = len(text)\n",
    "        \n",
    "        category = row['Citation Type']\n",
    "        \n",
    "        all_lengths.append(l)\n",
    "        per_category['lengths'][category].append(l)\n",
    "    \n",
    "    for c in categories:\n",
    "        per_category['mean'][c] = statistics.mean(per_category['lengths'][c])\n",
    "        per_category['stdev'][c] = statistics.stdev(per_category['lengths'][c])\n",
    "    \n",
    "    global_stats = {\n",
    "        'mean': statistics.mean(all_lengths),\n",
    "        'stdev': statistics.stdev(all_lengths),\n",
    "        'lengths': all_lengths\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'global': global_stats,\n",
    "        'per_category': pd.DataFrame(per_category)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d87bbf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_lengths_histograms(df_stats, n_cols=3):\n",
    "    categories = df['Citation Type'].unique()\n",
    "    n_rows = math.ceil(len(categories) / n_cols)\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.suptitle('Distribution of lengths')\n",
    "    \n",
    "    # Subplot of all lengths\n",
    "    plt.subplot(n_rows, n_cols, 1)\n",
    "    plt.title('All categories')\n",
    "    lengths = df_stats['global']['lengths']\n",
    "    plt.hist(lengths, color='r')\n",
    "\n",
    "    # Subplot of each category\n",
    "    index_subplot = 2\n",
    "    for c in categories:\n",
    "        plt.subplot(n_rows, n_cols, index_subplot)\n",
    "        plt.title('Category: %s' % c)\n",
    "        \n",
    "        lengths = df_stats['per_category']['lengths'][c]\n",
    "        plt.hist(lengths, color='b')\n",
    "\n",
    "        index_subplot += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61aa82c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lengths</th>\n",
       "      <th>mean</th>\n",
       "      <th>stdev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Related work</th>\n",
       "      <td>[29, 16, 26, 16, 13, 30, 36, 21, 23, 45, 15, 1...</td>\n",
       "      <td>26.674603</td>\n",
       "      <td>12.731571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comparison</th>\n",
       "      <td>[69, 13, 8, 14, 32, 18, 43, 51, 37, 47, 27, 17...</td>\n",
       "      <td>28.028269</td>\n",
       "      <td>17.926542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extending the work</th>\n",
       "      <td>[26, 24, 26, 162, 269, 140, 91, 127, 204, 46, ...</td>\n",
       "      <td>113.814815</td>\n",
       "      <td>100.246633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Using the work</th>\n",
       "      <td>[68, 12, 133, 56, 109, 175, 14, 132, 23, 142, ...</td>\n",
       "      <td>59.218182</td>\n",
       "      <td>54.376496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              lengths  \\\n",
       "Related work        [29, 16, 26, 16, 13, 30, 36, 21, 23, 45, 15, 1...   \n",
       "Comparison          [69, 13, 8, 14, 32, 18, 43, 51, 37, 47, 27, 17...   \n",
       "Extending the work  [26, 24, 26, 162, 269, 140, 91, 127, 204, 46, ...   \n",
       "Using the work      [68, 12, 133, 56, 109, 175, 14, 132, 23, 142, ...   \n",
       "\n",
       "                          mean       stdev  \n",
       "Related work         26.674603   12.731571  \n",
       "Comparison           28.028269   17.926542  \n",
       "Extending the work  113.814815  100.246633  \n",
       "Using the work       59.218182   54.376496  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats = calculate_stats(df)\n",
    "df_stats['per_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98582728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_lengths_histograms(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "610e7a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa0aaa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 10:17:56.781324: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "token = tokenizer.encode_plus(\n",
    "    df['Tagged Sentence'].iloc[0], \n",
    "    max_length=256, \n",
    "    truncation=True, \n",
    "    padding='max_length', \n",
    "    add_special_tokens=True,\n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80a09094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "------\n",
      "\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_input_ids = np.zeros((len(df), 256))\n",
    "X_attn_masks = np.zeros((len(df), 256))\n",
    "\n",
    "print(X_input_ids)\n",
    "print(\"\\n------\\n\")\n",
    "print(X_attn_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f4fee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(df, ids, masks, tokenizer):\n",
    "    for i, text in tqdm(enumerate(df['Tagged Sentence'])):\n",
    "        print(text)\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=256, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            add_special_tokens=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        ids[i, :] = tokenized_text.input_ids\n",
    "        masks[i, :] = tokenized_text.attention_mask\n",
    "    return ids, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2f17765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba847ef294c465ab5b42bc4f5fef494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since we only use shallow methods for textual analysis that do not generate a dependency structure, we cannot use complex methods for text reduction as described, e.g., in TREF.\n",
      "Sentence simplification systems (GTREF) are capable of compressing long sentences by deleting unimportant words and phrases.\n",
      "Each token is represented using a fairly standard menagerie of features, including such stemming from the surface appearance of the token (e.g., Contains dollar? Length in characters), calculated based on linguistic pre-processing made with the English Functional Dependency Grammar TREF (e.g., Case, Part-of-speech), fetched from precompiled lists of information (e.g., Is first name?), and features based on predictions concerning the context of the token (e.g, Class of previous token).\n",
      "The training and the test data were processed by the FDG parser TREF.\n",
      "The only other high-_x005f\f",
      "delity computational rendering of Tesniere's dependency syntax that we are aware of is that of TREF, which is neither generative nor statistical.\n",
      "The Statistical Dependency Model Most successful deep-linguistic Dependency Parsers (GTREF) do not have a statistical base.\n",
      "J~irvinen and Tapananinen have demonstrated an efficient wide-coverage dependency parser for English (GTREF).\n",
      "Generating Dependency Patterns Three dependency parsers were used for these experiments: MINIPAR3 REF, the Machinese Syntax4 parser from Connexor Oy TREF and the Stanford5 parser REF.\n",
      "Pre-processing tools Parser The current version of the evaluation workbench employs one of the high performance ”super-taggers” for English - Conexor’s FDG Parser TREF.\n",
      "The corpus was parsed by Connexor’s Machinese Syntax REF, which returns lexical and morphological information as well as the dependency relations between words by employing a functional dependency grammar TREF.\n",
      "A topic signature, as described in TREF, is a list of terms that can be used to signal the membership of a text in the relevant topic or category. We implemented the algorithm described in TREF with the addition of a cutoff, such that the topic signatures for a term are only included if the p1/p2 for that term is greater than the mean p1/p2 over all terms.\n",
      "This is an extension of Lin’s method TREF.\n",
      "Topic Signatures Topic Signatures (TS) are word vectors related to a particular topic TREF.\n",
      "Recently, content features were also well studied, including centroid REF, signature terms TREF and high frequency words REF. REF identified signature terms that were strongly associated with documents based on statistics measures.\n",
      "More advanced methods for query expansion use “topic signatures” – words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as relevant or not (GTREF).\n",
      "Topic signatures are words highly descriptive of the input, as determined by the application of loglikelihood test TREF.\n",
      "Log-likelihood ratio for words in the input Number of topic signature words (GTREF) and percentage of signature words in the vocabulary.\n",
      "NP-rewrite enhanced frequency summarizer Frequency and frequency-related measures of importance have been traditionally used in text summarization as indicators of importance (GTREF ).\n",
      "Comparing the Machine-Made Summaries and the Manually Created Extracts Measuring sentence co-selection between extractive summaries created by humans and those created by automatic summarizers has a long tradition in the text summarization community (GTREF), but this family of measures has a number of well-known shortcomings.\n",
      "ROUGE TREF compares any summary to any other (typically human-generated) summary using a recall-oriented approach.\n",
      "This is similar to the idea of topic signature introduced in TREF.\n",
      "The notion of topic signatures was first introduced in TREF.\n",
      "To date, researchers have harvested, with varying success, several resources, including concept lists REF, topic signatures TREF, facts REF, and word similarity lists REF.\n",
      "Under this approach, topic representations like those introduced in TREF and REF are used to identify a set of text passages that are relevant to a user’s domain of interest.\n",
      "The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (GTREF), etc.\n",
      "In summarization, topic signatures are a set of terms indicative of a topic TREF.\n",
      "A similar approach is explored in REF, which uses Topic Signatures TREF constructed around the target individual’s name to identify sentences to be included in the biography.\n",
      "In addition, infobox could be considered as topic signature TREF or keywords about the topic.\n",
      "Among them, query relevance, centroid REF and signature term TREF are most remarkable.\n",
      "For single document summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (GTREF).\n",
      "In the task of single document summarization, various features have been investigated for ranking sentences in a document, including term frequency, sentence position, cue words, stigma words, and topic signature (GTREF).\n",
      "In such cases, neither global features TREF nor aggregated contexts REF can help.\n",
      "More recently, machine learning approaches have been used for IE from semi-structured texts (GREF), named entity extraction TREF, template element extraction, and template relation extraction REF.\n",
      "TREF propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.\n",
      "REF made a second tagging pass which uses information on token sequences tagged in the first pass; TREF used as features information about features assigned to other instances of the same token.\n",
      "Global information is known to be useful in other NLP tasks, especially in the named entity recognition task, and several studies successfully used global features (GTREF). TREF conducted named entity recognition using global features as well as local features.\n",
      "Most work has looked to model non-local dependencies only within a document ( GTREF). TREF propose a solution to this problem: for each token, they define additional features based on known information, taken from other occurrences of the same token in the document.\n",
      "Impressive results have been achieved culminating in the state-of-the-art parser of REF which has been used as the parser for the Pascal Rich Textual Entailment Challenge entry of TREF.\n",
      "Metaphor in RTE Challenges Even though annotators aimed to filter out metaphorical uses of language from the RTE datasets REF, some metaphorical texts have eluded the annotators’ selection policies TREF. Although the sample of metaphor pairs is fairly small, table 1 shows that there is a trend for the accuracy to be significantly lower when metaphor is involved than for the overall results (which agrees with TREF diagnostic).\n",
      "Different approaches have been developed, for example, based on logic proving (GTREF) and graph match REF.\n",
      "Furthermore, REF and REF have demonstrated that the output of CCGbank parsers can be successfully translated into REF Discourse Representation Theory structures, to support question answering and the textual entailment task TREF.\n",
      "The RTE problem as presented in the PASCAL RTE dataset is particularly attractive in that it is a reasonably simple task for human annotators with high inter-annotator agreement (95.1% in one independent labeling TREF), but an extremely challenging task for automated systems. For example, two high-accuracy systems are those described in REF, achieving 60.4% accuracy with no task-specific information, and TREF, which achieves 61.2% task-dependent accuracy, i.e. when able to use the specific task labels as input. Many previous approaches have used a logical form representation of the text and hypothesis sentences, focusing on deriving a proof by which one can infer the hypothesis logical form from the text logical form (GTREF). Attempts have been made to remedy this deficit through various techniques, including modelbuilding TREF and the addition of semantic axioms REF.\n",
      "Finally, a few efforts (GTREF) have tried to translate sentences into formulas of first-order logic, in order to test logical entailment with a theorem prover. We show comparable results from recent systems based on lexical similarity REF, graph alignment REF, weighted abduction REF, and a mixed system including theorem proving TREF.\n",
      "Early deep semantic models (e.g., REF) as well as more recent ones (e.g., (GTREF)) rely on specific world knowledge encoded in rules for drawing decisions.\n",
      "These transformations are logical rules in TREF or sequences of allowed rewrite rules in REF.\n",
      "TREF represents and into a first-order logic translation of the DRS language used in Discourse Representation Theory REF and uses a theorem prover and a model builder with some generic, lexical and geographical background knowledge to prove the entailment between the two texts.\n",
      "Some NLP applications deal indirectly with negation, e.g., machine translation REF, text classification REF and recognizing entailments TREF.\n",
      "NutCracker3 TREF is a system based on logical representation and automatic theorem proving, but utilizes only WordNet REF as a lexical knowledge resource.\n",
      "Conversely, computational models of formal semantics have shown low recall on practical applications, stemming from their reliance on ontologies such as WordNet REF to model the meanings of content words (GTREF)\n",
      "Related methods incorporate measurements of similarity at various levels: lexical REF, syntactic REF, and semantic (GTREF)\n",
      "Logic-based approach is to map the language expressions to logical meaning representations, and then rely on logical entailment checks, possibly by invoking theorem provers ( GTREF).\n",
      "Of existing RTE approaches, the closest to ours is by TREF, who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover. This approach can be viewed as a bridge between TREF’s purely logical approach, which relies purely on hard logical rules and theorem proving, and distributional approaches, which support graded similarity between concepts but have no notion of logical operators or entailment. Task 1: Recognizing Textual Entailment 4.1 Dataset In order to compare directly to the logic-based system of TREF, we focus on the RTE-1 dataset REF, which includes 567 Text-Hypothesis (T-H) pairs in the development set and 800 pairs in the test set. For the RTE task, systems were evaluated using both accuracy and confidence-weighted score (cws) as used by TREF and the RTE- 1 challenge REF. In particular, they perform worse than strict entailment from TREF, a system that uses only logic.\n",
      "Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical REF, syntactic REF, and semantic ( GTREF).\n",
      "Interestingly, a number of works (e.g. (GTREF)) applied or utilized lexical based word overlap measures.\n",
      "Several models, ranging from the simple lexical similarity between T and H to advanced Logic Form Representations, have been proposed (GREF).\n",
      "They, therefore, tend to have high precision at the cost of low recall TREF. TREF describe a system for Recognizing Textual Entailment (RTE) that uses Boxer to convert both the premise and hypothesis of an RTE pair into first-order logical semantic representations and then uses a theorem prover to check for logical entailment. Note that when running an example in the theorem prover, weights are not possible, so any rule that would be weighted in an MLN is simply treated as a “hard clause” following TREF.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For preprocessing, all the sentences in the Bioscope corpus are tokenized and then parsed using the Berkeley parser2 REF trained on the GENIA TreeBank (GTB) 1.0 TREF3 , which is a bracketed corpus in (almost) PTB style.\n",
      "The BioNLP’09 shared task involved documents contained also in the GENIA treebank TREF, creating an opportunity for direct study of intrinsic and task-oriented evaluation results. GENIA treebank processing For comparison and evaluation, the texts in the GENIA treebank TREF are converted to the various formats as follows.\n",
      "We used the Stanford parser REF, and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank TREF for biomedical text;\n",
      "Our biomedical data comes from the GENIA treebank8 TREF, a corpus of abstracts from the Medline database.9\n",
      "Because our target is biomedical texts, we re-trained a parser REF with the GENIA treebank TREF, and also applied a bidirectional part-ofspeech tagger REF trained with the GENIA treebank as a preprocessor.\n",
      "The GENIA treebank TREF consists of 500 abstracts (4,446 sentences) extracted from MEDLINE. Automatically Annotated Corpus First, we applied a POS analyzer and then Enju. The POS analyzer and HPSG parser are trained by using the GENIA corpus (GREF), which comprises around 2,000 MEDLINE abstracts annotated with POS and Penn Treebank style syntactic parse trees TREF.\n",
      "Automatic identification of sources has also been addressed indirectly by TREF work on semantic role identification in that finding sources often corresponds to finding the filler of the agent role for verbs.\n",
      "Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation REF, word sense disambiguation REF and semantic role labeling TREF.\n",
      "Because verbs play a central role in the syntactic and semantic interpretation of a sentence, much research has focused on automatically learning properties of verbs from text corpora, such as their subcategorization REF, argument roles ( GTREF), selectional preferences REF, and lexical semantic classification REF.\n",
      "Several machine learning approaches for argument identification and classification have been developed (GTREF). These standard features, firstly proposed in TREF, refer to a flat information derived from parse trees, i.e. Phrase Type, Predicate Word, Head Word, Governing Category, Position and Voice. This latter choice allows us to compare the results with previous literature works, e.g. (GTREF).\n",
      "Many current solutions are complicated, consist of several stages and handbuilt features, and are too slow to be applied as part of real applications that require such semantic labels, partly because of their use of a syntactic parser ( GTREF). In TREF the authors presented a statistical approach to learning (for FrameNet), with some success. Argument Classification Accuracy So far we have not used the same accuracy measures as in previous work (GTREF). For example, simply adding whether each word is part of a noun or verb phrase using the handannotated parse tree (the so-called “GOV” feature from TREF) improves the performance of our system from 83.95% to 85.8%.\n",
      "Techniques for weakening the independence assumptions made by the IBM models 1 and 2 have been proposed in recent work (GTREF).\n",
      "The only trainable approaches (known to the author) to surface generation are the purely statistical machine translation (MT) systems such as TREF and the corpus-based generation system described in REF. The MT systems of TREF learn to generate text in the target language straight from the source language, without the aid of an explicit semantic representation. The form of the maximum entropy probability model is identical to the one used in (GTREF): where wi ranges over V t3 .stop. and {wi-l,wi-2,attri} is the history, where wi denotes the ith word in the phrase, and attri denotes the attributes that remain to be generated at position i in the phrase. e features used in NLG2 are described in the next section, and the feature weights aj, obtained from the Improved Iterative Scaling algorithm TREF, are set to maximize the likelihood of the training data. Our approach differs from the corpus-based surface generation approaches of REF and TREF. TREF describes a statistical machine translation approach that generates text in the target language directly from the source text.\n",
      "This is concordant with the usage in the maximum entropy literature TREF.\n",
      "Because their joint distributions have such closed-form expressions, the parameters can be estimated directly from the training data without the need for an iterative fitting procedure (as is required, for example, to estimate the parameters of maximum entropy models; TREF. Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., TREF), but within this framework no systematic study of interactions has been proposed.\n",
      "We report that our parsing framework achieved high accuracy (88.6%) in dependency analysis of Japanese with a combination of an underspecified HPSG-based Japanese grammar, SLUNG REF and the maximum entropy method TREF.\n",
      "REF use up to _x005f\f",
      "ve phonemes in feature function TREF.\n",
      "Thus, a lot of alignment techniques have been suggested at the sentence REF, phrase REF, noun phrase REF, word (GTREF), collocation REF and terminology level. Recently, many approaches based on the maximum entropy model have been applied to natural language processing (GTREF). We referred to the studies of (GTREF).\n",
      "For every class the weights of the active features are combined and the best scoring class is chosen TREF.\n",
      "We implemented these models within an maximum entropy framework (GTREF).\n",
      "Specifically we use the maximum entropy model TREF for this task: = exp?, =1 exp( (𝑐, )?=1 𝑐 ) (2) where exp( (𝑐, 𝑞) =1 𝑐 ) is the normalization factor; , is a feature function defined over query q and correction candidate c , while is the corresponding feature weight.\n",
      "Classifier and Features For our AL framework we decided to employ a Maximum Entropy (ME) classifier TREF.\n",
      "A maximum entropy model TREF is adopted here.\n",
      "Learning algorithms. We consider three learning algorithms, namely, the C4.5 decision tree induction system REF, the RIPPER rule learning algorithm REF, and maximum entropy classification TREF.\n",
      "In recent years, statistical learners such as maximum entropy models TREF, voted perceptrons REF,and support vector machines REF have been increasingly used, in part due to their ability to provide a confidence value (e.g., in the form of a probability) associated with a classification, and in part due to the fact that they can be easily adapted to train recently proposed rankingbased coreference models (see Section 3.3).\n",
      "As a model learning method, we adopt the max\u0002imum entropy model learning method (GTREF).\n",
      "TREF and REF make this same point and arrive at the same estimator, albeit through a maximum entropy argument.\n",
      "Conditional Maximum Entropy models have been used for a variety of natural language tasks, including Language Modeling REF, part\u0002of-speech tagging, prepositional phrase attachment, and parsing REF, word selection for machine translation TREF, and find\u0002ing sentence boundaries REF.\n",
      "A number of other researchers (GTREF) have described previ\u0002ous work on preprocessing methods. (TREF) describe an approach that targets translation of French phrases of the form NOUN de NOUN (e.g., conflit d’interet ).\n",
      "In previous work REF, I described a Maximum Entropy/Minimum Divergence (MEMD) model TREF for p(w[hi, s) which incorporates a trigram language model and a translation component which is an analog of the well-known IBM translation model 1 REF. For a given choice of q and f, the IIS algorithm TREF can be used to find maximum likelihood values for the parameters ~.\n",
      "Under the maximum entropy framework TREF, evidence from different features can be combined with no assumptions of feature independence.\n",
      "Previous uses of this model include language modeling REF, machine translation TREF, prepositional phrase attachment REF, and word morphology REF.\n",
      "However, our implementation uses a quasi-Newton gradient-climber BFGS for optimization, which has been shown to converge much faster (GTREF).\n",
      "We trained log linear models with the perceptron algorithm REF using features similar to those used for NP chunking in TREF, including surrounding POStags (provided by a separately trained log linear POS-tagger) and surrounding words, up to 2 before and 2 after the current word position.\n",
      "As the representative problem in shallow parsing, noun phrase chunking has received much attention, with the development of standard evaluation datasets and with extensive comparisons among methods (GTREF). As for the task of shallow parsing, CRFs also outperform many other state-of-the-art models (GTREF).  We employ similar predicate sets defined in TREF. Since the CRF model is one of the successful models in sequential labeling tasks (GTREF), in this section, we also compare LDCRFs with CRFs. Nevertheless, since testing the significance of shallow parsers’ F-measures is tricky, individual labeling accuracy provides a more convenient basis for statistical significance tests TREF. We observe that the L-BFGS optimizer is slightly faster than CG on LDCRFs (see Figure 3), which echoes the comparison between the L-BFGS and the CG optimizing technique on the CRF model TREF.\n",
      "Although this non-concavity prevents efficient global maximization of equation (3), it still allows us to incorporate incomplete annotations using gradient ascent iterations TREF.\n",
      "CRFs have been successfully applied to many sequence labeling tasks (GTREF).\n",
      "Many machine learning techniques have been successfully applied to chunking tasks, such as Regularized Winnow REF, SVMs REF, CRFs (GTREF), Maximum Entropy Model REF, Memory Based Learning REF and SNoW REF.\n",
      "To address this difficulty, we use the forward-backward algorithm (GTREF) to estimate separately for each position the probability of a hyphen at that position.\n",
      "CRFs have been applied with impressive empirical results to the tasks of named entity recognition REF, part-of-speech (PoS) tagging REF, noun phrase chunking TREF and extraction of table data REF, among other tasks.\n",
      "Relation to Previous Work There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (GTREF).\n",
      "SVMHMMs and CRFs have been successfully applied to a range of sequential tagging tasks such as syllabification REF, chunk parsing TREF and word segmentation REF.\n",
      "Segment confidence is estimated using constrained forward-backward TREF.\n",
      "It is calculated by constrained forwardbackward algorithm TREF, and confident segments are added to the dictionary in order to improve segmentation accuracy.\n",
      "A similar approach was used by TREF in order to associate confidence values with sequences of contiguous tokens identified by a CRF model as fields in an information extraction task.\n",
      "Related Work: Most previous work has focused on confidence estimation for an entire example or some fields of an entry TREF using CRFs.\n",
      "TREF describe the constrained forward-backward algorithm to efficiently estimate the conditional probability that a segment of text is correctly extracted by a CRF.\n",
      "To obtain the probability at each position of a linear-chain CRF, the constrained forward-backward technique described in TREF is used.\n",
      "This is analogous to the task of estimating record confi- dence using field confidence scores in information extraction TREF.\n",
      "To address this difficulty, we use the forward-backward algorithm (GTREF) to estimate separately for each position the probability of a hyphen at that position.\n",
      "Quality assessment of a learned model output was explored by many previous works (see REF for a survey), and applied to several NL processing tasks such as syntactic parsing REF, machine translation REF, speech REF, relation extraction REF, IE TREF, QA REF and dialog systems REF.\n",
      "Applications of this algorithm include k-best parsing (GTREF) and machine translation REF.\n",
      "By focusing on weighted loss as opposed to arc frequency, the classifier discovers structural zeros TREF, events which could have been observed, but were not.\n",
      "In the present work, we exploit L1-regularization, though other techniques such as structural zeros TREF could also potentially be used.\n",
      "It is interesting to note that these grammars capture many of the “structural zeros” described by TREF and pruning rules with probability below e −10 reduces the grammar size drastically without influencing parsing performance.\n",
      "Although (GTREF) have hand-labeled naturally-occurring utterances in a variety of corpora for various emotions, then extracted acoustic, prosodic and lexical features and used machine-learning techniques to develop predictive models, little work to date has addressed emotion detection in computer-based educational settings.\n",
      "As a result of this mismatch, recent work motivated by spoken dialogue applications has started to use naturally-occurring speech to train emotion predictors (GTREF), but often predicts emotions using only acoustic-prosodic features that would be automatically available to a dialogue system in real-time. Lexical information has been shown to improve speech-based emotion prediction in other domains (GTREF), so our first non-acoustic-prosodic feature represents the transcription3 of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn). The number of words and syllables in a turn provide alternative ways to quantify turn duration TREF. Adding Context-Level Features Research in other domains (GTREF) has shown that features representing the di- alogue context can sometimes improve the accuracy of predicting negative user states, compared to the use of features computed from only the turn to be predicted.\n",
      "In other papers (GTREF), we have already given some descriptive statistics on corrections and aware sites and we have been looking at methods to automatically predict these two utterance categories. These experiments show that corrections and aware sites can be classi ed as such automatically, with a considerable degree of accuracy (GTREF).\n",
      "Moreover, speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states GTREF and user responses to system errors TREF that are useful for triggering system adaptation.\n",
      "For more information on our tasks and features, see (GTREF).\n",
      "One such augmented parser, trained on data available from the PropBank project has been recently presented in TREF. In this paper we describe a domain-independent IE paradigm that is based on predicate-argument structures identified automatically by two different methods: (1) the statistical method reported in TREF; and (2) a new method based on inductive learning which obtains 17% higher Fscore over the first method when tested on the same data. Section 2 reports on the parser that produces predicate-argument labels and compares it against the parser introduced in TREF. The Model In previous work using the PropBank corpus, TREF proposed a model predicting argument roles using the same statistical method as the one employed by REF for predicting semantic roles based on the FrameNet corpus REF. To achieve high accuracy and resolve the data sparsity problem the method reported in (GTREF) employed a backoff solution based on a lattice that combines the model features. For example, the backoff lattice in TREF consists of eight connected nodes for a five-feature set. Our model considers two sets of features: Feature Set 1 (FS1): features used in the work reported in TREF and REF ; and Feature Set 2 (FS2): a novel set of features introduced in this paper. We also were interested in comparing the results of the decision-tree-based method against the results obtained by the statistical approach reported in TREF. TREF report the results listed on the first line of Table 2. Because predicate lexical information is used for less than 5% of the branching decisions, the generated classifier scales better than the statistical method from TREF  to unknown predicates.\n",
      "In recent work, TREF shows that the independence assumption can be relaxed, and co-training is still effective under a weaker independence assumption. However, as theoretically shown in TREF, and then empirically in REF, co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations.\n",
      "Polarity Classification There is a large body of work on classifying the polarity of a document (e.g., GTREF), a sentence (e.g., REF), a phrase (e.g., REF), and a specific object (such as a product) mentioned in a document (e.g., REF).  Much work has been performed on learning to identify and classify polarity terms (i.e., terms expressing a positive sentiment (e.g., happy) or a negative sentiment (e.g., terrible)) and exploiting them to do polarity classification (e.g., GTREF). For instance, instead of representing the polarity of a term using a binary value, REF use TREF method to assign a real value to represent term polarity and introduce a variety of numerical features that are aggregate measures of the polarity values of terms selected from the document under consideration.\n",
      "Introduction Sentiment detection and classification has received considerable attention recently (GTREF ).\n",
      "Similarly for summarization, systems that have employed language models trained only on unsummarized text (GTREF).\n",
      "TREF applied semantic parsing to capture the predicate-argument sentence structure.\n",
      "This basic kind of verb knowledge has been shown to be useful in many NLP tasks such as information extraction (GTREF), parsing REF and word sense disambiguation REF.\n",
      "Abstract In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of REF, TREF and others. In recent work, a number of researchers have cast this problem as a tagging problem and have applied various supervised machine learning techniques to it (GTREF). Named Entities in Constituents – Following TREF, we tagged 7 named entities (PERSON, ORGANIZATION, LOCATION, PERCENT, MONEY, TIME, DATE) using IdentiFinder REF and added them as 7 binary features. Head Word POS – TREF showed that using the part of speech (POS) of the head word gave a significant performance boost to their system. The Surdeanu et al. System. TREF report results on two systems using a decision tree classifier. G&P system estimates the posterior probabilities using several different feature sets and interpolate the estimates, while TREF use a decision tree classifier.\n",
      "However, the parsing results of TREF involving unlexicalized grammars suggest that gains may be limited.\n",
      "However, the parsing results of TREF involving unlexicalized grammars suggest that gains may be limited.\n",
      "Lapata TREF proposes another approach to information ordering based on a probabilistic model that assumes the probability of any given sentence is determined by its adjacent sentence and learns constraints on sentence order from a corpus of domain specific texts.\n",
      "Features proposed to create the appropriate order include publication date of document REF, content words (GTREF), and syntactic role of words REF.\n",
      "A simple ordering criterion is the chronological order of the events represented in the sentences, which is often augmented with other ordering criteria such as lexical overlap REF, lexical cohesion REF or syntactic features TREF.\n",
      "Most of the existing algorithms represent text structure as a linear sequence and are driven by local coherence constraints (GTREF).  To identify the exact location of the sentence within the chosen paragraph, local ordering methods such as TREF could be used. Features Features used in our experiments are inspired by previous work on corpus-based approaches for discourse analysis (GTREF). This measure was first introduced in the context of sentence ordering by TREF. Relative to other corpora used in text structuring research (GTREF), texts in our collection are long: an average article has 32.9 sentences, organized in 3.61 sections and 10.9 paragraphs. The degree of variability observed in this experiment is consistent with human performance on other text structuring tasks such as sentence ordering (GTREF). To make sentencelevel insertion decisions, we use a local model in line with previous sentence-ordering work (GTREF).\n",
      "In accordance with recent work in the emerging field of text-to-text generation (GTREF), we assume that the input to text structuring is a set of clauses.\n",
      "It is typically applicable in the text generation field, both for concept-to-text generation and text-totext generation TREF, such as multiple document summarization (MDS), question answering and so on. Related Work For works taking no use of source document, TREF proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts. The probability model originates from TREF, and we implement the model with four features of lemmatized noun, verb, adjective or adverb, and verb and noun related dependency.\n",
      "As to analysis of NPs, there have been a lot of work on statistical techniques for lexical dependency parsing of sentences (GTREF), and these techniques potentially can be used for analysis of NPs if appropriate resources for NPs are available.\n",
      "We also view this as one of the key ideas of the incremental perceptron algorithm of TREF, which searches through a complex decision space step-by-step and is immediately updated at the first wrong move.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment REF, sequence analysis REF and phrase-structure parsing TREF.\n",
      "In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (GTREF). For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq≥20) and several other statistical parsers (GTREF). When compared to other kernel methods, our approach performs better than those based on the Tree kernel (GTREF), and is only 0.2% worse than the best results achieved by a kernel method for parsing REF.\n",
      "Perceptron-based training To tune the parameters w of the model, we use the averaged perceptron algorithm TREF because of its efficiency and past success on various NLP tasks (GTREF).\n",
      "Following recent work applying global discriminative models to large-scale structured prediction problems (GTREF), we build our shift-reduce parser using a global linear model, and compare it with the chartbased C&C parser. Following TREF, we apply the “early update” strategy to perceptron training: at any step during decoding, if neither the candidate output nor any item in the agenda is correct, decoding is stopped and the parameters are updated using the current highest scored item in the agenda or the candidate output, whichever has the higher score.\n",
      "Another alternative for future work is to compare the dynamic programming approach taken here with the beam-search approach of TREF, which allows more “global” features.\n",
      "Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (GTREF) to current research REF, kernel methods have been showing more and more potential in relation extraction. While kernel methods using the dependency tree TREF and the shortest dependency path REF suffer from low recall performance, convolution tree kernels REF over syntactic parse trees achieve comparable or even better performance than feature-based methods. TREF proposed a slightly generalized version of this kernel between dependency trees, in which a successful match of two relation instances requires the nodes to be at the same layer and in the identical path starting from the roots to the current nodes. Similar to TREF, this method also suffers from high precision but low recall.\n",
      "In relation extraction, typical work on kernel methods includes: REF, TREF and REF. Related Work Many techniques on relation extraction, such as rule-based REF, feature-based REF and kernel-based (GTREF), have been proposed in the literature. TREF generalized it to estimate similarity between dependency trees. Therefore, although this kernel shows performance improvement over the previous one TREF, the constraint makes the two dependency kernels share the similar behavior: good precision but much lower recall on the ACE corpus. This may be due to two reasons: 1) the dependency tree TREF and the shortest path REF lack the internal hierarchical phrase structure information, so their corresponding kernels can only carry out node-matching directly over the nodes with word tokens; 2) the parse tree kernel has less constraints. Compared with Previous Kernels: Since our method only counts the occurrence of each sub-tree without considering the layer and the ancestors of the root node of the sub-tree, our method is not limited by the constraints (identical layer and ancestors for the matchable nodes, as discussed in Section 2) in TREF.\n",
      "TREF extended this work to estimate similarity between augmented dependency trees.\n",
      "During the last years, many authors have focused on resolving TE detection, as solutions to this problem have proved to be useful in many natural language processing tasks, such as question answering TREF or machine translation (MT) REF.\n",
      "TE has been successfully applied to a variety of natural language processing applications, including information extraction REF and question answering TREF.\n",
      "Extensive research has been done in questionanswering, e.g. (GTREF).\n",
      "In order to build models that perform well in new (target) domains we usually find two settings TREF. In the supervised setting, a recent paper by TREF shows that, using a very simple ´ feature augmentation method coupled with Support Vector Machines, he is able to effectively use both labeled target and source data to provide the best results in a number of NLP tasks.\n",
      "To benefit from Wikipedia data, we introduce a domain adaption approach TREF which is suitable for this work since we have enough “target” domain data.\n",
      "Inspired by recent work on domain adaptation, we tested whether the performance of the out-of-domain models can be improved when training includes a small amount of data from the target domain, by applying the method of TREF.\n",
      "In this paper we investigate a recently proposed Bayesian adaptation approach (GTREF) for adapting a conditional maximum entropy (ME) LM REF to a new domain, given a large corpus of out-of-domain training data and a small corpus of in-domain data. Domain Adaptation of Maximum Entropy Models Recently, a hierarchical Bayesian adaptation method was proposed that can be applied to a large family of discriminative learning tasks (such as ME models, SVMs) (GTREF). Discussion In this paper we have tested a hierarchical adaptation method (GTREF) on building style-adapted LMs for speech recognition.\n",
      "For extra training data, we pool material from different datasets and use the multi-domain split feature space approach to learn dataset specific behaviors TREF.\n",
      "TREF investigate domain adaptation for sentiment classifiers using structural correspondence learning\n",
      "We selected four binary NLP datasets for evaluation: 20 Newsgroups1 and Reuters REF (used by Tong and Koller) and sentiment classification TREF and spam REF. For each dataset we extracted binary unigram features and sentiment was prepared according to TREF.\n",
      "It is worth noting that TREF deal with the domain adaptation problem for sentiment classification where labeled data from one domain is used to train a classifier for classifying data from a different domain. As the training data from DVDs is much more similar to books than that from kitchen TREF, we should give the data from DVDs a higher weight. Although the size of our training data are smaller than that reported in TREF (70% vs. 80%), the classification performance is comparative to theirs.\n",
      "Work in cross-domain sentiment classification TREF focuses on the challenge of training a classifier from one or more domains (source domains) and applying the trained classi- fier in a different domain (target domain).  Previous work on sentiment classification has shown that both unigrams and bigrams are useful for training a sentiment classifier TREF. Experiments 5.1 Dataset To evaluate our method we use the cross-domain sentiment classification dataset prepared by TREF. Effect of Feature Expansion To study the effect of feature expansion at train time compared to test time, we used Amazon reviews for two further domains, music and video, which were also collected by TREF but are not part of the benchmark dataset. TREF apply the structural correspondence learning (SCL) algorithm to train a crossdomain sentiment classifier.\n",
      "In addition, such an approach relies heavily on the availability of accurately parsed sentences, which could be problematic for domains such as biomedical texts (GTREF).\n",
      "Here in place of annotated text, only an existing knowledge base (KB) is needed to train a relation extractor (GTREF). This room is not as large as in previous work TREF where target text and training KB are closely related. First we follow TREF, use Freebase as source of distant supervision, and employ Wikipedia as source of unlabelled text—we will call this an in-domain setting. We will follow TREF and call the term R (c1, . . . cn) with c ∈ R a relation instance. Following previous work (GTREF) we make one more simplifying assumption: every candidate tuple can be member of at most one relation. On the other end, we have methods that take relation mentions from several documents and use these as input features (GTREF).  TREF refer to this as the distant supervision assumption. On inspection, we find that these preferences are often not satis- fied in a baseline distant supervision system akin to TREF. The feature functions of this template are taken from TREF. Our work is inspired by TREF who also use Freebase as distant supervision source. We also heuristically align our knowledge base to text by making the distant supervision assumption (GTREF). Experimental Setup We follow TREF and perform two types of evaluation: held-out and manual. For each candidate tuple c with arity 2 and each of its mention tuples i we extract a set of features Xi c similar to those used in TREF: lexical, Part-Of-Speech (POS), named entity and syntactic features, i.e. features obtained from the dependency parsing tree of a sentence. As our baseline, and roughly equivalent to previous work TREF, we pick the templates TBias and TMen. This is similar to the setting of TREF.\n",
      "A particularly attractive approach, called distant supervision (DS), creates labeled data by heuristically aligning entities in text with those in a knowledge base, such as Freebase TREF. We applied our method to Wikipedia articles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS TREF and MultiR REF, which is a state-of-the-art multiinstance learning system for relation extraction (see Section 7). Our work was inspired by TREF who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia. Dataset Following TREF, we carried out our experiments using Wikipedia as the target corpus and Freebase (September, 2009, REF) as the knowledge base. Following TREF, we performed an automatic held-out evaluation and a manual evaluation. Configuration of Classifiers Following TREF, we used a multiclass logistic classifier optimized using L-BFGS with Gaussian regularization to classify entity pairs to the predefined 24 relations and NONE. We used syntactic features (i.e., features obtained from the dependency parse tree of a sentence) and lexical features, and entity types, which essentially correspond to the ones developed by TREF.\n",
      "Here one aligns existing database records with the sentences in which these records have been “rendered”––effectively labeling the text—and from this labeling we can train a machine learning system as before (GTREF). Distant Supervision In Distant Supervision (DS) a set of facts from pre-existing structured sources is aligned with surface patterns mentioned in text (GTREF), and this alignment is then used to train a relation extractor. Our first baseline is MI09, a distantly supervised classifier based on the work of TREF.\n",
      "Some of the previous methods tried to alleviate the problem of the limited seed lexicon size REF, while others did not require any seed lexicon (GTREF).\n",
      "Some researchers have attempted to do this by adding a do\u0002main specific dictionary REF, or mining unseen words TREF using one of several translation lexicon induction tech_x0002_niques REF.\n",
      "TREF mine translations for high frequency OOV words in NEW\u0002domain text in order to do domain adaptation.\n",
      "When a linear model does not fit well, re\u0002searchers are careful to manually add important feature conjunctions, as for example, (GTREF).\n",
      "TREF has exploited this complementarity by combining the two methods using \n",
      "decision lists. The idea is to pool the evidence provided by the component methods, and to then \n",
      "solve a target problem by applying the single strongest piece of evidence, whatever type it happens \n",
      "to be.\n",
      "The first method uses a similar algorithm to that of TREF. Recent results (e.g., (GTREF) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision. The first method builds on results from TREF and REF. TREF describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance. The approach builds from an initial seed set for a category, and is quite similar to the decision list approach described in TREF. Unsupervised Algorithms based on Decision Lists 3.1 Supervised Decision List Learning The first unsupervised algorithm we describe is based on the decision list method from TREF. TREF describes the use of more sophisticated smoothing methods. The Algorithm in TREF We can now compare this algorithm to that of TREF. It was motivated by the observation that the TREF algorithm added a very large number of rules in the first few iterations.\n",
      "The plen\u0002itude of unlabeled natural language data, and the paucity of labeled data, have made boot\u0002strapping a topic of interest in computational linguistics. Current work has been spurred by two papers, TREF and REF.\n",
      "The TREF algorithm was one of the first bootstrapping algorithms to become widely known in computational linguistics.\n",
      "This method of co-training has been previously applied to a variety of natural language tasks, such as word sense disambiguation TREF, lexicon construction for information ex_x005f_x005f_x0002_traction REF, and named en_x0002_tity classification REF.\n",
      "TREF describes a 'semi-unsuper\u0002vised' approach to the problem of sense disam\u0002biguation of words, also using a set of initial seeds, in this case a few high quality sense annotations.\n",
      "Self-training is an alterna\u0002tive single-view algorithm in which a labelled pool is incrementally enlarged with unlabelled samples for which the learner is most confident. Early work by TREF falls within this framework.\n",
      "Previous works on statistical dependency analysis include REF and REF in Japanese analysis as well as REF, REF, and TREF in English analysis. TOur modeling is slightly different from those of other standard approaches to statistical dependency analysis (GTREF) which simply distinguish the two cases: the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold. The model (b) corresponds to a model in which standard approaches to statistical dependency analysis (GTREF) are applied to our task of deciding dependency between two subordinate clauses. REF's lexicalized dependency analyzer is similar to that of TREF, where various features were evaluated through performance test and an optimal feature set was manually selected.\n",
      "Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g. (GTREF).\n",
      "Our investigation of these models was motivated rather by our desire to obtain a generalizable result for these simple and well-understood models, since obtaining similar results for more sophisticated models (e.g. (GTREF) might have been attributed to special properties of these models.\n",
      "Further work will look at how to integrate probabilities such as p(cjv; r) into a model of dependency structure, similar to that of TREF and REF, which can be used for parse selection.\n",
      "Semantic and Logic Transformations Semantic Transformations Instead of producing only a phrasal parse for the question and answer, we make use of one of the new statistical parsers of large real-world text coverage TREF. The label propagation rules are identical to the rules for mapping from trees to dependency structures used my Michael Collins (cf. TREF).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that, while we restrict our discussion to analysis of Japanese sentences in this paper, what we present below should also be straightforwardly applicable to more wideranged tasks such as English dependency analysis just like the problem setting considered by TREF. Estimation of DPs Some of the state-of-the-art probabilistic language models such as the bottomup models P (Rjs) proposed by TREF and REF directly estimate DPs for a given input, whereas other models such as PCFGbased topdown generation models P (R; s) do not (GTREF). CHAGAKE REF: an extension of the bottom-up model proposed by Collins TREF.\n",
      "As the development of corpus linguistics, many statistics-based parsers were proposed, such as REF’s statistical decision tree parser, TREF’s bigram dependency model parser, REF’s maximum entropy model parser.\n",
      "(GTREF) use the lexical informationand REF use the contextual information for structural disambiguation.\n",
      "The features are extracted using a statistical parser TREF, and consist of the head and modifiers of each phrase.\n",
      "A typical current parser (e.g., statistical parsers such as (GTREF) interleaves PP attachment with all its other disambiguation tasks.\n",
      "With the emergence of the important role of word-to-word relations in parsing (GTREF), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, GTREF, REF for Japanese, REF for Korean, REF for Swedish, REF for Czech, among others. TREF employs this distance ∆i,H(i) in the computation of word-toword dependency probabilities P(dep (wi ,wH(i) )| S) ≈ (2) P(link(wi ,wH(i) )| ∆i,H(i) ) suggesting that distance is a crucial variable when deciding whether two words are related, along with other features such as intervening punctuation. This is a modified version of the backed-off smoothing used by TREF to alleviate sparse data problems.\n",
      "To calculate the edge weights, we adapt the definition of TREF to use direction rather than relation type (represented in the original as triples of non-terminals). We adopt the same smoothing strategy as TREF, which backs off to PoS for unseen dependency events.\n",
      "There have been two main robust parsing paradigms: Finite State Grammar-based approaches (such as REF, REF, and REF) and Statistical Parsing (such as REF, REF, and TREF).\n",
      "In such cases, the program splits a sentence into coordinated clauses (or coordinated verb phrases) by using a parser TREF to distinguish when a coordinating conjunction (and, but, whereas) is conjoining two main clauses or two parts of a complex verb phrase.\n",
      "A conditional probability model to score the analyses TREF. Maximum likelihood estimation to make inferences about the underlying probability models (GTREF). For the probability model,we adopt the approach by REF,which itself is a modified version of the statistical model used in TREF. The statistical model in TREF is actually used in a phrase-structure-based parsing approach,but it uses the same idea of computing probabilities between dependents and head units. We use a strategy similar to TREF and we interpolate with estimates based on less context: P(x|y) ≈ λ · P1(x|y) + (1 − λ) · P2(x) (3) where λ = δ/(δ + 1) and δ is the count of the x occurrences.\n",
      "TREF One of the most straightforward projective dependency parsing strategies was introduced by REF, and is based on the CYK bottom–up parsing strategy REF. COMBINER steps follow the same mechanism as those in the algorithm of REF, and LINK steps work analogously to those of TREF, so this schema can be seen as being intermediate between those two algorithms. By generalizing the linking steps in Eisner and Satta’s parser so that the head of each item can be in any position, we obtain an O(n5) parser which can be filtered into the parser of TREF by eliminating the COMBINER steps.\n",
      "Statistical Model In SIFT’s statistical model, augmented parse trees are generated according to a process similar to that described in (GTREF).\n",
      "However, in order to utilize some syntax information between the pair of words, we adopt the syntactic distance representation of TREF, named Collins distance for convenience.\n",
      "This was done trying to overcome the limitations of lexicalized approaches to parsing (GTREF), where related words, like scissors and knife cannot be generalized.\n",
      "Examples of formalisms using this approach include the work of REF, REF, TREF, and REF.\n",
      "Recently, it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words (see, e.g., TREF), which is what dependency grammars model explicitly do, but context-free phrasestructure grammars do not.\n",
      "Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models (GTREF).\n",
      "The 75.4% results may seen low compared to parsing results like the 88% precision and recall in TREF, but those parsing results include many easier-to-parse constructs.\n",
      "proved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars (GTREF). three models in TREF are susceptible to the O(n 3) method (cf. Collins's O(nh)).\n",
      "We describe our experience in building on the parsing model of TREF. This paper first describes a baseline approach, based on the parsing model of TREF, which recovers dependencies with 72% accuracy. A Sketch of the Parsing Model The parsing model builds on Model 1 of TREF; this section briefly describes the model. In TREF, P(/~lo~) is defined as a product of terms, by assuming that the right-hand-side of the rule is generated in three steps: TREF describes a series of refinements to this basic model: the addition of \"distance\" (a conditioning feature indicating whether or not a modifier is adjacent to the head); the addition of subcategorization parameters (Model 2), and parameters that model wh-movement (Model 3); estimation techniques that smooth various levels of back-off (in particular using POS tags as word-classes, allowing the model to learn generalizations about POS classes of words). Parsing the Czech PDT Many statistical parsing methods developed for English use lexicalized trees as a representation (e.g., (GTREF)); several (e.g., (GREF) emphasize the use of parameters associated with dependencies between pairs of words. Preferences for dependencies that do not cross verbs The model of TREF had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs. The parsers of (GTREF) encoded this as a hard constraint. REF describes results of 91% accuracy in recovering dependencies on section 0 of the Penn Wall Street Journal Treebank, using Model 2 of TREF.\n",
      "Inducing a stochastic grammar from the Treebank 4.1 Reconstructing derivations We want to extract from the Penn Treebank an LTAG whose derivations mirror the dependency analysis implicit in the head-percolation rules of (GTREF). Following TREF, words occurring fewer than four times in training were replaced with the symbol *UNKNOWN* and tagged with the output of the part-of-speech tagger described in REF.\n",
      "Introduction In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types REF REF REF REF REF TREF REF REF REF REF. Many probabilistic evaluation models have been published inspired by one or more of these feature types REF REF REF REF TREF REF REF, but discrepancies between training sets, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively.\n",
      "Such word-based lexicalizations of probability models are used successfully in the statistical parsing models of, e.g., TREF, REF, or REF.\n",
      "The system is trained by _x005f_x000c_rst using the Collins parser TREF to parse the 36,995 training sentences, matching annotated frame elements to parse constituents, and extracting various features from the string of words and the parse tree.\n",
      "TREF discusses the recovery of one kind of empty node, viz., WH-traces).\n",
      "In this paper, we examine how the information provided by modern statistical parsers such as TREF and REF contributes to solving this problem. The Experiments In previous work using the FrameNet corpus, REF developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of TREF.\n",
      "Like the models of REF, the additional features in our model are generated probabilistically, whereas in the parser of TREF distance measures are assumed to be a function of the already generated structure and are not generated explicitly. Distance measures for CCG Our distance measures are related to those proposed by REF, which are appropriate for binary trees (unlike those of TREF). Adding lexical information REF shows that removing the lexical dependencies in Model 1 of TREF (that is, not conditioning on wh when generating ws) decreases labeled precision and recall by only 0.5%.\n",
      "This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in TREF. At last, the dependency parser presented in TREF is used to generate the full parse.\n",
      "Introduction Treebank-based probabilistic parsing has been the subject of intensive research over the past few years, resulting in parsing models that achieve both broad coverage and high parsing accuracy (e.g., GTREF). Lexicalization can increase parsing performance dramatically for English (GTREF), and the lexicalized model proposed by TREF has been successfully applied to Czech REF and Chinese REF. Section 3 describes two standard lexicalized models (GTREF), as well as an unlexicalized baseline model. Section 5 presents an error analysis for TREF lexicalized model, which shows that the head-head dependencies used in this model fail to cope well with the flat structures in Negra. Table 1: Results for the TREF model for various languages (dependency precision for Czech).  Probabilistic Parsing Models 3.1 Probabilistic Context-Free Grammars Lexicalization has been shown to improve parsing performance for the Penn Treebank (e.g., TREF). Collins’s Head-Lexicalized Model In contrast to REF approach, the model proposed by TREF does not compute rule probabilities directly. The lexicalized model proposed by TREF (henceforth Collins model) was re-implemented by one of the authors. The reader is referred to REF and TREF for details. Table 4: Linguistic features in the current model compared to the models of REF, TREF, and REF. In a second series of experiments, we investigated a more general way of dealing with the flatness of Negra, based on TREF model for nonrecursive NPs in the Penn Treebank (which are also flat). For non-recursive NPs, TREF does not use the probability function in (5), but instead substitutes Pr (and, by analogy, Pl) by: Pr(Ri (8) ,t(Ri),l(Ri)|P,Ri−1,t(Ri−1),l(Ri−1),d(i)). Table 4 shows the linguistic features of the resulting model compared to the models of REF, TREF, and REF. The TREF model does not use context-free rules, but generates the next category using zeroth order Markov chains (see Section 3.3), hence no information about the previous sisters is included.We first added sister-head dependencies for NPs (following TREF original proposal) and then for PPs, which are flat in Negra, and thus similar in structure to NPs (see Section 2.2). The progression in the probabilistic parsing literature has been to start with lexical head-head dependencies TREF and then add non-lexical sister information REF, as illustrated in Table 4. The work by REF and Bikel and REF has demonstrated the applicability of the TREF model for Czech and Chinese. However, the learning curve for Negra (see Figure 1) indicates that the performance of the TREF model is stable, even for small training sets. In Experiment 1, we applied three standard parsing models from the literature to Negra: an unlexicalized PCFG model (the baseline), REF head-lexicalized model, and TREF model based on head-head dependencies.\n",
      "However, such constructions prove to be difficult for stochastic parsers REF and they either avoid tackling the problem REF or only deal with a subset of the problematic cases TREF. First, we extend the mechanism of adding gap variables for nodes dominating a site of discontinuity TREF. For the parsing and antecedent recovery experiments, in the case of WH-traces (WH–     ) and controlled NP-traces (NP–NP), we follow the standard technique of marking nodes dominating the empty element up to but not including the parent of the antecedent as defective (missing an argument) with a gap feature (GTREF). The idea of threading EEs to their antecedents in a stochastic parser was proposed by TREF, following the GPSG tradition REF.\n",
      "The sentences were processed using Collins’ parser TREF to generate parse-trees automatically.\n",
      "Both techniques implement variations on the approaches of REF and TREF for the purpose of differentiating between complement and adjunct.\n",
      "Previous approaches to the problem (GTREF) have all been learning-based; the primary difference between the present algorithm and earlier ones is that it is not learned, but explicitly incorporates principles of GovernmentBinding theory REF, since that theory underlies the annotation. Both TREF and REF are explicit about this predisposition. TREF Model 3 integrates the detection and resolution of WH-traces in relative clauses into a lexicalized PCFG.\n",
      "In order to extract the linguistic features necessary for the models, all sentences containing the target word were automatically part-of-speech-tagged using a maximum entropy tagger REF and parsed using the Collins parser TREF.\n",
      "We employ a robust statistical parser TREF to determine the constituent structure for each sentence, from which subjects (s), objects (o), and relations other than subject or object (x) are identified.\n",
      "The second step is to estimate the semantic orientation of each extracted phrase TREF. TREF have also developed an algorithm for predicting semantic orientation. Related Work This work is most closely related to TREF work on predicting the semantic orientation of adjectives. As an example, they present the following three sentences TREF. TREF use a four-step supervised learning algorithm to infer the semantic orientation of adjectives from constraints on conjunctions. Another area for future work is to empirically compare PMI-IR and the algorithm of TREF. On the other hand, it would be interesting to evaluate PMI-IR on the collection of 1,336 hand-labeled adjectives that were used in the experiments of TREF. Previous work on determining the semantic orientation of adjectives has used a complex algorithm that does not readily extend beyond isolated adjectives to adverbs or longer phrases TREF.\n",
      "Hockenmaier also found the dependencies to be very beneficial — in contrast to recent results from the lexicalised PCFG parsing literature TREF — but did not gain from the use of distance measures.\n",
      "However, the work which is most directly comparable to ours is that of (GTREF). We concentrate particularly on the work of (GTREF) as they provide results which are directly comparable to those presented in this paper. Lacking alternatives, both TREF and REF give up on adapting a pure WSJ trained system, instead looking at the issue of how much of an improvement one gets over a pure Brown system by adding WSJ data (as seen in the last two lines of Table 1). We use the same divisions as REF, who base their divisions on TREF. We have already seen the results TREF and REF achieve in Table 1. The BROWN parsing model is naturally better than the WSJ model for this task, but combining the two training corpora results in a better model (as in TREF)).\n",
      "Previous work has shown that parsers typically perform poorly outside of their training domain TREF. Cross-domain speed improvement When applying parsers out of domain they are typically slower and less accurate TREF.\n",
      "It was found in TREF that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output.\n",
      "Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains TREF.\n",
      "TREF adapted the perceptron learning algorithm to tagging tasks, via sentence-based global feedback.\n",
      "The classifier consists of two components based on the averaged multiclass perceptron (GTREF). Here we used the averaged perceptron TREF, where the weight matrix used to classify the test data is the average of all of the matrices posited during training, i.e.,   > 4 1 > 1 *3254  * . 4.\n",
      "We adopt the basic feature set used in REF and TREF. For parameter estimation of θ, we use the averaged perceptron as described in TREF. Following most previous work, e.g. TREF and REF, we divide this corpus into training set (sections 0-18), development set (sections 19-21) and the final test set (sections 22-24). Our implementation of this feature set is basically the same as the version used in TREF.\n",
      "There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm TREF, and regularized structured SVMs trained using FOBOS REF.\n",
      "Indeed, as for the voted per_x005f_x005f_x0002_ceptron of TREF, we can get performance gains by reducing the support threshold for features to be in\u0002cluded in the model.\n",
      "It is usually modeled as a sequence model, often in combination with part-of-speech tagging and lemmatization (GTREF, and others).\n",
      "We use the common method of setting the final weight vector as the average of the weight vectors after each iteration TREF, which has been shown to alleviate overfitting.  This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs TREF.\n",
      "Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing REF, and tagging or segmentation tasks (GTREF).\n",
      "CRFs have been applied with impressive empirical results to the tasks of named entity recognition TREF, simplified part-of-speech (POS) tagging REF, noun phrase chunking REF and extraction of tabular data REF, among other tasks.\n",
      "1 Introduction In recent years, conditional random fields (CRFs) REF have shown success on a number of natural language processing (NLP) tasks, including shallow parsing REF, named entity recognition TREF and information extraction from research papers REF.  In order to make training time manageable4 , we collapse the number of POS tags from 48 to 5 following the procedure used in TREF.\n",
      "Abstract Conditional random fields REF are quite effective at sequence labeling tasks like shallow parsing REF and namedentity extraction TREF.\n",
      "The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing REF and information extraction TREF.\n",
      "Many discriminative methods have been applied to NER, such as decision trees REF, ME models REF, and CRFs TREF.\n",
      "These belong to two main categories based on machine learning (GTREF) and language or domain specific rules REF.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear CRF model has been successfully applied in NLP and text mining tasks (GTREF).\n",
      "Information extraction Finally, there exist large bodies of work on information extraction using models based on Markov and semi-Markov CRFs REF, and in particular for the task of named entity recognition TREF.\n",
      "Current studies of NER mainly focus on formal text such as news articles (GTREF).\n",
      "A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models REF, Maximum Entropy methods REF, Decision Trees REF, Conditional Random Fields TREF, Class-based Language Model REF, Agent-based Approach REF and Support Vector Machines.\n",
      "Such models are well suited to sequence analysis, and CRFs in particular have been shown to be useful in partof-speech tagging REF, shallow parsing REF, and named entity recognition for newswire data TREF.\n",
      "Note that our formulation of CRFs is different from the widely-used formulations (e.g., (GTREF).\n",
      "While global statistical approaches, such as sequential averaged perceptrons or CRFs TREF, appear better suited to the NER problem than local symbolic learners, the two approaches search different hypothesis spaces.\n",
      "Here we adapt self-training, a simple tech\u0002nique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (GTREF).\n",
      "The most clearly relevant study is TREF where the focus is on introducing the problem, exploring annotation issues and outlining potential applications rather than on the specificities of the ML approach, though they do present some results using a manually crafted substring matching classifier and a supervised SVM on a collection of Medline abstracts. To further elucidate the nature of the task and improve annotation consistency, we have developed a new set of guidelines, building on the work of TREF. SVM classifier Baseline denotes substring matching classifier of TREF. Baselines As a baseline classifier we use the substring matching technique of TREF, which labels a sentence as spec if it contains one or more of the following:suggest, potential, likely, may, at least,in part, possibl, further investigation, unlikely, putative,insights, point toward, promise and propose.\n",
      "The development data was 200 sentences of labeled biomedical oncology text (BIO, the ONCO portion of the Penn Biomedical Treebank), as well as 200K unlabeled sentences TREF.\n",
      "We have achieved average results in the CoNLL domain adaptation track open submission (GTREF).\n",
      "For the second adaptation task we were given a large collection of unlabeled data in the chemistry domain TREF as well as a test set of 5000 tokens (200 sentences) to parse (eng\u0002lish_pchemtbtb_test.conll).\n",
      "In the biomedical domain, for example, several annotated corpora such as GENIA REF, PennBioIE TREF, and GENETAG REF have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address.\n",
      "The pchemtb-closed shared task (GTREF) is used to illustrate our models.\n",
      "Due in large part to the creation of biomedical treebanks (GTREF) and rapid progress of data-driven parsers REF, there are now fast, robust and accurate syntactic parsers for text in the biomedical domain.\n",
      "For out-of-domain data, we get 21K examples from the Brown portion of the Treebank and 6296 examples from tagged Medline abstracts in the PennBioIE corpus TREF.\n",
      "Our annotation guidelines1 are based on those de\u0002veloped for annotating full sub-NP structure in the biomedical domain REF. The anno\u0002tation guidelines for this biomedical corpus (an ad\u0002dendum to the Penn Treebank guidelines) introduce the use of NML nodes to mark internal NP structure.\n",
      "From the sub\u0002language biology domain, we used the oncology part of the PENNBIOIE corpus TREF and removed all but three gene entity sub\u0002types (generic, protein, and rna).\n",
      "Similar attempt of con\u0002structing integrated corpora is being done in University of Pennsylvania, where a corpus of MEDLINE abstracts in CYP450 and oncology domains where annotated for named entities, POS, and tree structure of sentences TREF.\n",
      "For example it has been used to measure centrality in hyperlinked web pages networks REF, lexical networks (GTREF), and semantic networks REF. Our method is based on the ones described in (GTREF).  Computing Speaker Salience The method we used is similar to the methods described in (GTREF), which were originally used for ranking sentences and documents in extractive summarization and information retrieval systems.\n",
      "First, we show how to extend the recently proposed structural correspondence learning (SCL) domain adaptation algorithm TREF for use in sentiment classification. Then, it models the correlations between the pivot features and all other features by training linear pivot predictors to predict occurrences of each pivot in the unlabeled data from both domains (GTREF). For the part of speech tagging problem studied by TREF, frequently-occurring words in both domains were good choices, since they often correspond to function words such as prepositions and determiners, which are good indicators of parts of speech. REF and TREF suggest λ = 10−4 , µ = 0, which we have used in our results so far. We augment each labeled target instance xj with the label assigned by the source domain classifier (GTREF). As we noted in Section 5, we are able to significantly outperform basic structural correspondence learning TREF. We also note that while REFand TREF observe that including the label of a source classifier as a feature on small amounts of target data tends to improve over using either the source alone or the target alone, we did not observe that for our data. Finally we note that while TREF did combine SCL with labeled target domain data, they only compared using the label of SCL or non-SCL source classifiers as features, following the work of REF. First, we showed that for a given source and target domain, we can significantly improve for sentiment classification the structural correspondence learning model of TREF.\n",
      "Most RTE systems are based on advanced NLP components, machine learning techniques, and/or syntactic transformations (GTREF).\n",
      "Most entailment systems function as weak proof theory (GTREF), but contradictions require deeper inferences and model building.\n",
      "TREF used a kernel method on syntactic tree pairs (GTREF).\n",
      "Almost all existing RTE models align the linguistic material of the premise and hypothesis and base at least part of their decision on properties of this alignment (GTREF).\n",
      "In the past, RTEs Challenges machine learning algorithms were widely used for the task of recognizing textual entailment (GTREF) and they have reported goods results for English language.\n",
      "Systems addressing TE exploiting machine learning techniques with a variety of features, including lexical-syntactic and semantic features (e.g. GTREF) tend towards the opposite extreme of this framework, since even if linguistic features are used, they bring information about a specific aspect relevant to the inference task but they do not provide an independent judgment on it.\n",
      "For instance, it is not clear whether the availability of larger amounts of training data correlates with better performance REF or not (GTREF), even within the same evaluation setting.\n",
      "First of all, we believe that research on CLTE can employ inference mechanisms and semantic knowledge sources to augment existing MT methods, leading to improvements in the translation quality (e.g. TREF).\n",
      "TREF uses Textual Entailment fea- ´ tures extracted from the Standford Entailment Recognizer REF.\n",
      "Recent comparisons of approaches that can be trained on corpora REF have shown that in most cases statistical aproaches (GTREF) yield better results than finite-state, rule-based, or memory-based taggers REF.\n",
      "English Part of Speech (POS) tagging has been widely described in the recent past, starting with the REF paper, followed by numerous others using various methods: neural networks REF, HMM tagging REF, decision trees REF, transformation-based error-driven learning REF, and maximum entropy TREF, to select just a few. However different the methods were, English dominated in these tests\n",
      "REF applied boosting to part of speech tagging. Ad_x005f_x005f_x0002_wait TREF estimates a probabil\u0002ity distribution for tagging using a maximum entropy approach.\n",
      "Maxilnum Entropy The model used here for sentence-boundary detection is based on the maximum entropy model used for POS tagging in TREF.\n",
      "Many statistical parsers (GTREF) are based on a history-based probability model REF, where the probability of each decision in a parse is conditioned on the previous decisions in the parse.\n",
      "Our method was applied to 23 million words of the WSJ that were automatically tagged with Ratnaparkhi's maximum entropy tagger TREF and chunked with the partial parser CASS REF.\n",
      "For example, since the Collins parser depends on a prior part-of-speech tagger TREF, we included the time for POS tagging in our Collins mea\u0002surements.\n",
      "Support Vector Machines (SVMs) REF and Maximum Entropy (ME) method REF are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (TREF).\n",
      "The ME Tagger The ME tagger is based on TREF’s POS tagger and is described in REF .\n",
      "Finally, in section 4 we add additional features to the maxent model, and chain these models into a conditional markov model (CMM), as used for tag_x005f_x005f_x0002_ging TREF or earlier NER work REF.\n",
      "In modern lexicalized parsers, POS tagging is of\u0002ten interleaved with parsing proper instead of be\u0002ing a separate preprocessing module (GTREF).\n",
      "Much research has been done to improve tagging accuracy using several dif\u0002ferent models and methods, including: hidden Markov models (HMMs) REF, REF; rule-based systems REF, REF; memory-based systems REF; maximum-entropy systems TREF; path voting con_x0002_straint systems REF; linear separator systems REF;  and majority voting systems REF.\n",
      "TREF dealt with some of tile limitations in Hirst and St-Onge's algorithm by examining every possible lexical chain which could be computed, not just those possible at a given point in the text. A Linear Time Algorithm for Computing Lexical Chains Overview Our research on lexical chains as an intermediate representation for automatic text summarization fol- lows the research of TREF. Table 1 denotes sample metrics tuned to simulate the system devised by TREF. Comparison with Previous Work As mentioned above, this research is based on the work of TREF on lexical chains.\n",
      "Previous approaches include su\u0002pervised learning TREF, vectorial similarity computed between an initial abstract and sentences in the given document, or intra-document similarities REF.\n",
      "This method of co-training has been previously applied to a variety of natural language tasks, such as word sense disambiguation REF, lexicon construction for information extraction REF, and named entity classification TREF. Our model is based on the DLCoTrain algorithm proposed by TREF, which applies a co-training procedure to decision list classifiers for two independent sets of features. Following TREF filtering is based on the strength of the context and its frequency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The empirical investigations described here and below use the data set of TREF. The task is to classify names in text as person, location, or organization.\n",
      "Similar approaches are used among others in REF for learning semantic lexicons, in TREF for named\u0002entity recognition, and in REF for hierarchical text categorization.\n",
      "TREF and REF apply bootstrapping to the related task of named-entity recognition.\n",
      "Co-training has also been used for named entity recognition (NER) TREF, coreference resolution REF, text categorization REF and improving gene name data REF.\n",
      "The first-order features φ1(x, h, m) are the exact same implementation as in previous CoNLL system TREF.\n",
      "Graph-based (GTREF) and transition-based (REF) parsing algorithms offer two dif\u0002ferent approaches to data-driven dependency parsing.\n",
      "For testing, all training data are used for training, with the number of training iterations set to be the number which gave the highest accuracy during the development experiments. This method was used by TREF in their parsing model.\n",
      "Most of the participants took language-independent approaches toward leveraging this complexity into better performance: generating machine learning features based on each item in a token’s list of morphological attributes (GTREF); using the entire list as an atomic feature REF; or generating features based on each pair of attributes in the cross-product of the lists of a potential head and de_x005f\u0002pendent REF.\n",
      "Both the graph-based (GTREF) and the transition-based REF parsing algo\u0002rithms are related to our word-pair classification model.\n",
      "The perceptron has been used in previous work on dependency parsing by TREF, with a parser based on Eisner’s algorithm REF, and also on incremental constituent parsing REF.\n",
      "The ML classi- fier used for this experiment is a conditional Markov model tagger which is designed for, and proved successful in, named entity recognition in newspaper and biomedical text (GTREF).\n",
      "While multiple machine learning approaches have been proposed for information extraction in recent years (GTREF), manually created regexes remain a widely adopted practical solution for information extraction REF.\n",
      "The use of char N-gram (N-gram substring) fea\u0002tures was inspired by the work of TREF, where the introduction of such features has been shown to improve the overall F1 score by over 20%.\n",
      "Named entity chunking. F-measure on the test sets. Previous best results: FIJZ03 REF, CN03 REF, KSNM03 TREF.\n",
      "We ex\u0002perimented with a conditional Markov model tagger that performed well on language-independent NER TREF and the identification of gene and protein names REF.\n",
      "Another approach we will also focus is dividing words into characters and applying character-level models TREF.\n",
      "Sometimes, these types of features are referred to as word\u0002external and word-internal TREF\n",
      "German morphological analysis and POS tagging was done using LoPar TREF English POS tagging was done with Brill’s tagger REF, followed by a simple lemmatizer based on tagging results.\n",
      "Inside-out alignments REF, such as the one in Example 1.3, cannot be induced by any of these theories; in fact, there seems to be no useful synchronous grammar formalisms available that handle inside-out alignments, with the possible exceptions of synchronous tree-adjoining grammars REF, Bertsch and REF and generalized multitext grammars TREF, which are all way more complex than ITG, STSG and (2,2)-BRCG.\n",
      "It differs from the more strict synchronous grammar formalisms (GTREF) because it does not try to perform simultaneous pars\u0002ing on parallel grammars; instead, the model learns an augmented target-language gram\u0002mar whose rules make “soft alignments” with a given source tree.\n",
      "Several researchers (GTREF) have already proposed methods for bi\u0002narizing synchronous grammars in the context of machine translation.\n",
      "We shall limit our attention to MTGs in Generalized Chomsky Normal Form (GCNF) TREF.\n",
      "Our measure of alignment complexity is analo_x0002_gous to what TREF call “fan\u0002out.”\n",
      "REF therefore ar\u0002gue that in order to extract as many rules as possi\u0002ble, a more powerful formalism than synchronous CFG/TSG is required: for example, generalized multitext grammar TREF, which is equivalent to synchronous set-local multicom_x0002_ponent CFG/TSG REF.\n",
      "We utilized an off the-shelf system, Stanford Named Entity Recognizer TREF for detecting entity mentions on the English sentences.\n",
      "We first extract named entities from scattered opinions DT using Stanford Named Entity Recognizer TREF.\n",
      "Preprocessing First, all documents are parsed and processed with standard tools for named en_x0002_tity recognition TREF and coreference resolution.\n",
      "the annotations from the Stanford Named Entity Recognizer TREF that labels PERSON, ORGANIZATION, and LOCATION entities.\n",
      "First, if NPi is an NE, we create a feature whose value is the NE label of NPi, as determined by the Stanford CRF-based NE recognizer TREF.\n",
      "A large body of literature in joint learning has demonstrated that such an approach can suffer from cascaded errors at testing, and does not ben\u0002efit from the potential for joint learning TREF.\n",
      "Some prior work REF demonstrated the effec\u0002tiveness of using semantic relations to improve entity coreference resolution; while (GTREF) experimented with in\u0002formation fusion of relations across multiple documents.\n",
      "To compute the features which we extract in the next section, all instances in our data sets were part-of-speech tagged by the MXPOST tagger REF, parsed with the Malt Parser, and named entity tagged with the Stan_x0002_ford NE tagger TREF.\n",
      "Named Entity Recognizer tagging (NER): We in\u0002tegrated Stanford’s NER tagger TREF.\n",
      "For example, non-local features such as “same phrases in a document do not have different entity classes” were shown to be useful in named entity recognition (GTREF).\n",
      "The use of annealing to obtain a maximum a posteri\u0002ori (MAP) configuration from sampling-based in_x0002_ference is common (e.g., GTREF).\n",
      "TREF and REF incorporate global information by en\u0002forcing event role or label consistency over a doc\u0002ument or across related documents.\n",
      "However, due to the lack of a fine grained NER tool at hand, we employ the Stanford NER package TREF which identifies only four types of named entities.\n",
      "Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer TREF.\n",
      "As discussed above, all state-of-the-art published methods rely on lexical features for such tasks TREF.\n",
      "While much research (GTREF) has explored how to reconcile pairwise decisions to form coherent clusters, we simply take the tran\u0002sitive closure of our pairwise decision (as in Ng and REF and Bengston and REF) which can and does cause system errors.\n",
      "Previous work in do\u0002main adaptation can be classified into two cate\u0002gories: [S+T+], where a small, labeled target do_x0002_main data is available, e.g. (GTREF), or [S+T-], where no labeled tar\u0002get domain data is available, e.g. REF.\n",
      "Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (GTREF).\n",
      "We preprocess our textual data as follows: We first use the Stanford named entity recognizer TREF to find entity mentions in the corpus.\n",
      "Then, we use the Stanford Named Entity Recognizer TREF to iden\u0002tify named entities, which we replace with a unique token (‘NE’).\n",
      "We first identify noun phrases that potentially denote named entities. We use the Stanford NER Tagger TREF to discover these and segment the text accordingly.\n",
      "We used as candidates all strings labeled in the annotated data as well as all named entities found by the Stanford NER tagger for CoNLL TREF.\n",
      "First we employ Stanford tools to tokenize, sentence-split and Part-Of-Speech tag REF a document. Next we recognize named entities TREF by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags.\n",
      "Compared with the state-of-the-art news\u0002trained Stanford Named Entity Recognizer TREF, T-SEG obtains a 52% increase in F1 score.\n",
      "Various modifications to CRF have recently been introduced to take into account of non-local dependencies TREF or broader context beyond training data REF\n",
      "The sampling distri\u0002butions are annealed, as a search technique to find the best configuration of assignments TREF.\n",
      "They used the Stanford named entity recog_x0002_nizer TREF to find entity mentions in text and constructed relation mentions only between entity mentions in the same sentence.\n",
      "To identify potential speakers they used the Stanford NER tagger TREF and a method out_x0002_lined in REF that allowed them to find nominal character references.\n",
      "We use Stanford Named Entity Recognizer TREF to collect named entities which are not in the Wikipedia list.\n",
      "We use the Stanford Core NLP suite (GTREF) to annotate each document with POS and NER tags, parse trees, and coreference chains.\n",
      "In the literature, most studies focus on English event extraction and have achieved certain success (GTREF).\n",
      "TREF also inte\u0002grated non-local information into entity annotation algorithms using Gibbs sampling.\n",
      "After post-processing (tokeniza￾tion, sentence-splitting, and part-of-speech tagging), named entities were automatically recognized and labeled with PER, ORG, LOC, and MISC TREF.\n",
      "But human editorial judgment being the bottleneck, we sampled 50% or 50,000 snippets, whichever was smaller. Starting with about 752,450 pages, we ran the Stanford NER TREF to mark per\u0002son spans.\n",
      "EXEMPLAR em\u0002ploys the Stanford NER TREF to recognize named entities.\n",
      "The QA-SYS performs Part of Speech tagging using Stanford POS tagger REF, and Named Entity Recog\u0002nition using Stanford NER TREF, and then builds a Lucene index over the set of input documents.\n",
      "For training the CRF model, we used a compre\u0002hensive set of features from TREF that gives state-of-the-art results on this task.\n",
      "Entities were found using the Stanford named entity tagger TREF, and were matched to their name in Freebase.\n",
      "NER is a fairly researched field (GTREF) and is also used in several commer\u0002cial applications such as Zemanta, OpenCalais and AlchemyAPI4 , which are able to automatically in\u0002sert links for a NE pointing to a knowledge base such as Wikipedia or IMDB.\n",
      "A common approach is to utilize a Named Entity Recognition (NER) system like Stan_x0002_ford NER TREF, which recognizes the names of things (e.g., person and product names) from texts.\n",
      "To assign types to arguments, we apply Stanford Named Entity Recognizer TREF, and also look up the argument in WordNet 2\n",
      "We identified these mentions of persons using Stanford NER TREF and treated each person mention as a single token.\n",
      "Stanford NER (or in short Stanford) associ\u0002ated to the following model provided by the tool and which was trained on different news corpora (CoNLL, MUC6, MUC7 and ACE):\n",
      "ner-eng-ie.crf-3-all2008-distsim.ser.gz TREF\n",
      "We parse the data us\u0002ing the Collins Parser REF, and then tag person, location and organization names us\u0002ing the Stanford Named Entity Recognizer TREF.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some stem from work on graphical mod\u0002els, including loopy belief propagation REF, Gibbs sampling TREF, sequential Monte Carlo methods such as particle filtering REF, and variational inference TREF. Also relevant are stacked learning REF, interpretable as approximation of non-local feature values REF, and M-estimation REF, which al\u0002lows training without inference.\n",
      "In all the experiments, our source side lan\u0002guage is English, and the Stanford Named Entity Recognizer TREF was used to ex\u0002tract NEs from the source side article.\n",
      "The features include the NP head noun and its premodifiers. We also use the Stanford NER tagger TREF to identify Named Entities within the NP.\n",
      "For semantic role labeling we use SWIRL9 , for chunk parsing CASS REF and for con\u0002stituency parsing Stanford Parser REF. Named-entity information is pro\u0002vided by Stanford Tagger TREF.\n",
      "Named Entity The replaced token should not be part of a named entity. For this purpose, we applied the Stanford NER TREF.\n",
      "In the first approach, heuristic rules are used to find the dependencies REF or penalties for label inconsistency are required to handset ad-hoc TREF.\n",
      "We have used the freely available Stanford Named Entity Recognizer (GTREF) in our engine.\n",
      "From the above corpora, we first extracted all the NEs from the English side, using the Stanford NER tool TREF.\n",
      "Furthermore, as our tasks can be regarded as a sequential labeling problem (to label if a word is an opinion word, a target, or an ordinary word), we experimented with the conditional random fields (CRF) technique REF for extraction, which is a popular information extraction method and has been success\u0002fully used in labeling tasks such as POS tagging REF and Named Entity Recognition (GTREF).\n",
      "Then, we use the Stanford Named Entity Recognizer (GTREF) to identify named entities, which we replace with a unique token (‘NE’).\n",
      "First, we test the NE alignment performance with the same Chinese NE recognizer (Wu’s system, adopted earlier) but with different English NE recognizers that include the Mallet toolkit (used before), the Stanford NE recognizer (GTREF), and Minor Third REF.\n",
      "Such techniques include Gibbs sampling TREF, a general-purpose Monte Carlo method, and integer linear program\u0002ming (ILP), REF, a general-purpose exact framework for NP-complete problems.\n",
      "Undirected graphical models such as Conditional Random Fields (CRFs) REF have shown great success for problems involving struc\u0002tured output variables (e.g. REF,TREF)).\n",
      "Others have attempted to train global scoring functions using Gibbs sampling TREF, message propagation, REF, and integer linear pro_x0002_gramming REF\n",
      "More distantly related, REF and TREF propose graphical models for combining information about a given entity from multiple mentions.\n",
      "Recent applications of statistical co\u0002reference models are beginning to show promise (GTREF).\n",
      "We construct our joint model as an extension to the discriminatively trained, feature-rich, conditional random field-based, CRF-CFG parser of TREF.\n",
      "To our knowledge, the discriminatively trained depen\u0002dency model we used has not been previously pub\u0002lished, but it is very similar to recent work on discriminative constituency parsing TREF.\n",
      "We focus on training using Gibbs sampling REF, because it has been popularly applied in the natural language literature, e.g., (GTREF).\n",
      "C’s nameEntitySet represents the set of all named entities in C’s article labeled by Stanford NER TREF.\n",
      "One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling TREF, another is (loopy) sum-product belief propagation REF.\n",
      "Named\u0002entity information was obtained by the Stanford tag_x0002_ger TREF.\n",
      "Global Constraints Previous work demonstrated the benefits of applying declarative constraints in in\u0002formation extraction (GTREF).\n",
      "The second application, information extraction from seminar announcements, has been modeled previously with skip-chain CRFs (GTREF).\n",
      "The Stanford named entity recognition (NER) software TREF is an implementation of linear chain Conditional Random Field (CRF) se\u0002quence models, which includes a three class (per\u0002son, organization, location and other) named entity recognizer for English.\n",
      "For identification of named entities, we use Stanford NER TREF.\n",
      "It uses Named Entity Recognition (NER) (GTREF) as a sub\u0002routine to identify named person entities, though we are also interested in unnamed persons such as “my teacher” and pronouns.\n",
      "More specifically, we seek a posterior distribution over latent variables that partition words in a sentence into flow and in\u0002ert groups; we estimate this posterior using Gibbs sampling TREF.\n",
      "English features were taken from TREF.\n",
      "We used Stanford NER TREF for English named entity recognition (NER), KNP for Japanese NER, and BaseNER REF for Chinese NER.\n",
      "In part-of-speech tagging, the accuracy of the Stanford tagger REF falls from 97% on Wall Street Journal text to 85% accuracy on Twitter REF. In named entity recognition, the CoNLL-trained Stan\u0002ford recognizer achieves 44% F-measure REF, down from 86% on the CoNLL test set TREF.\n",
      "For syntactic analysis we use the Stan\u0002ford Parser TREF.\n",
      "In NER tasks, however, informa￾tion about a distant entity is often more useful than information about the previous state TREF.\n",
      "Global information is known to be use\u0002ful in other NLP tasks, especially in the named en\u0002tity recognition task, and several studies success_x0002_fully used global features (GTREF).\n",
      "TREF hand-set penalties for incon\u0002sistency in entity labeling at different occurrences in the text, based on some statistics from training data.\n",
      "An additional consistent edge of a linear-chain conditional random field (CRF) ex\u0002plicitly models the dependencies between distant occurrences of similar words (GTREF).\n",
      "We employ Gibbs sampling, previously used in NLP by TREF and REF, among others.\n",
      "We added named entity (NE) tags to the data us\u0002ing the tagger of TREF.\n",
      "Starting out with a chunking pipeline, which uses a classical combination of tagger and chun\u0002ker, with the Stanford POS tagger REF, the YamCha chunker REF and the Stanford Named Entity Rec_x005f\u0002ognizer TREF, the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and John\u0002son’s reranking parser REF to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also sup\u0002ports using the Berkeley parser REF, yielding an easy-to-use Java-only solution.\n",
      "We perform named entity tagging using the Stanford four-class named entity tagger TREF.\n",
      "To implement this method, we first use the Stanford Named Entity Recognizer TREF to identify the set of person and organisation entities, E, from each article in the corpus.\n",
      "We processed each novel with the Stanford NER tagger TREF and extracted noun phrases that were categorized as persons or organizations.\n",
      "We run the Stanford Named Entity Recognizer TREF and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs.\n",
      "That is, a non-animate noun can hardly constitute an experience. In order to make a dis\u0002tinction, we use the dependency parser and a named-entity recognizer TREF that can recognize person pronouns and person names.\n",
      "To reproduce the system by REF, we substitute the cast and crew list employed by them (see Section 3.2), with a NER compo_x005f_x0002_nent TREF.\n",
      "For example, the average F1 of the Stan_x0002_ford NER TREF , which is trained on the CoNLL03 shared task data set and achieves state-of-the-art performance on that task, drops from 90.8% REF to 45.8% on tweets.\n",
      "Since knowledge extraction from web\u0002based encyclopedia is typically noisy REF, we use YAGO to determine whether two NPs have a relation only if one NP is a named entity (NE) of type person, organization, or location according to the Stanford NE recognizer TREF and the other NP is a common noun.\n",
      "TREF used Gibbs sampling, a sim\u0002ple Monte Carlo method used to perform approxi\u0002mate inference in factored probabilistic models.\n",
      "Semantic features: we use the Stanford NER tag_x005f_x0002_ger TREF to determine if the targeted NP is a named entity, and we use the Sundance parser REF to assign seman\u0002tic class labels to each NP’s head noun.\n",
      "To create d we extract all NEs from the text using the StanfordNE Recognizer TREF and represent each NE by its Wikipedia URI.\n",
      "We use the Stanford Named Entity Recognizer TREF for this purpose.\n",
      "Due to the large size of the corpora, we uniformly sampled a subset of doc\u0002uments for each corpus and ran the Stanford NER tagger TREF, which tagged named en\u0002tities mentions as person, location, and organization.\n",
      "The Figure also shows the results of the Stanford NER tagger for English TREF (we used the MUC-7 classifier).\n",
      "Following REF, we filter out noisy doc\u0002uments and use natural language packages to anno\u0002tate the documents, including NER tagging TREF and dependency parsing REF.\n",
      "Run Stanford CoreNLP with POS tagging and named entity recognition TREF\n",
      "Mainly, pre\u0002determined word-level dependencies were repre\u0002sented as links in the underlying graphical model (GTREF).\n",
      "Accordingly, we extract all the named entities in a sentence using Stanford’s Name Entity Recognizer (TREF).\n",
      "Similarly, the semantic phrase table (SPT) has been extracted from the same corpora annotated with the Stanford NE tagger (GTREF).\n",
      "We adopted the feature set investigatedin REF for article error correction. We use the Stanford coreNLP toolkit1 (GTREF) to extract the features.\n",
      "It includes: cleaning up and normalization of the input using reg\u0002ular expressions, sentence segmentation, tokeniza\u0002tion and lemmatization using GATE REF, syntactical parsing and dependency parsing (collapsed) using the Stanford Parser REF, and Named Entity Recogni\u0002tion using Stanford NER TREF.\n",
      "For example, the average F1 of the Stanford NER TREF drops from 90.8% REF to 45.8% on tweets, while Liu et al. (2010) report that the F1 score of a state-of\u0002the-art SRL system REF falls to 42.5% on tweets as apposed to 75.5% on news.\n",
      "BIUTEE provides state-of-the-art pre-processing utilities: Easy-First parser REF, Stanford named-entity-recognizer TREF and ArkRef coreference resolver REF, as well as utilities for sentence\u0002splitting and numerical-normalizations.\n",
      "The Stanford CRF-based NER tagger was used as the monolingual component in our models TREF.\n",
      "We utilize the Stanford tools (GTREF).\n",
      "Most well-known is the Stanford named entity recog\u0002nition (NER) tagger TREF which assigns coarse-grained types like person, organization, lo\u0002cation, and other to noun phrases that are likely to denote entities.\n",
      "When training the Stanford NER system TREF on just the tokens from the Freire data set and the parameters from en\u0002glish.all.3class.distsim.prop (included in the Stan\u0002ford NER release, see also REF), our F-scores come very close to those reported by REF, but mostly with a higher recall and lower precision.\n",
      "To identify entities, we use a CRF-based named entity tagger TREF and a Chinese word breaker REF for English and Chinese corpora, respectively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We validate the hypothesis that using linguistic features, e.g., part-of-speech tags REF, named-entity tags TREF, and dependency trees REF, helps improve the quality of our approach, called Joint.\n",
      "Similarly, distributional features support generalization in Named Entity Recognition TREF.\n",
      "We used 500,000 Wikipedia articles (2,000,000 sentences) for generating training data for the NED compo\u0002nent. We used Open NLP POS tagger, Standford NER TREF and MaltParser REF to label/tag sentences.\n",
      "State-of-the-art tools for named entity recognition such as the Stanford NER Tagger TREF compute semantic tags only for a small set of coarse-grained types: Person, Location, and Organization (plus tags for non-entity phrases of type time, money, percent, and date).\n",
      "The first is named entity (NE) tags (PERSON, ORGANI\u0002ZATION and LOCATION) returned by the Stanford NE recognition tool TREF.\n",
      "Corry has only participated in the “open” setting, as it has already a number of preprocessing mod\u0002ules integrated into the system: the Stanford NLP toolkit for parsing REF and NE-tagging TREF, Wordnet for se\u0002mantic classes and the U.S. census data for assigning gender values to person names.\n",
      "Named Entities using the Stanford named en\u0002tity recognizer recognizer TREF\n",
      "Initially all sentences are pre-processed by the CoreNLP (GTREF) suite of tools, a process that includes named entity recognition, normalization, part of speech tag\u0002ging, lemmatization and stemming.\n",
      "For capturing and normalizing the above mentioned expressions, we make use of the Stan\u0002ford NER Toolkit TREF.\n",
      "Named entity recognition The Stanford Named Entity Recogniser TREF was run over the BNC and any nouns that occur in the corpus with both named entity and non-named entity tags are ex\u0002tracted to form the list Stanford.\n",
      "Initially all sentences are pre-processed by the CoreNLP (GTREF) suite of tools, a process that includes named entity recognition, normalization, part of speech tag\u0002ging, lemmatization and stemming.\n",
      "We analysed the text in the metadata, performing lemmatization, PoS tagging, named entity recognition and classification (NERC) and date detection using Stanford CoreNLP (GTREF).\n",
      "In our experiments, we performed named entity recognition with the Stanford NER tool using the standard English model TREF.\n",
      "We used the Stanford Named Entity Recognizer by TREF, with the 7 class model trained for MUC: Time, Location, Organization, Person, Money, Percent, Date.\n",
      "Most previous work with CRFs containing non\u0002local dependencies used approximate probabilis\u0002tic inference techniques, including TRP REF and Gibbs sampling TREF.\n",
      "There is a significant volume of work explor\u0002ing the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (GTREF).\n",
      "One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer TREF.\n",
      "The results we obtained on the CoNLL03 test set were consistent with what was reported in TREF.\n",
      "Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (GTREF) and dependency parsing REF with a great deal of success.\n",
      "The texts we use in our experiments are the develop\u0002ment set of the RTE-5 challenge REF, and we preprocess the data using the Stan\u0002ford named-entity recognizer TREF.\n",
      "REF com￾bines a linear-chain CRF and two SVM models to enhance the recall. TREF used Gibbs Sampling to add non-local dependencies into linear-chain CRF model for information ex\u0002traction.\n",
      "We use SYNERGY REF, an ensemble NER system that combines the UIUC NER TREF and Stanford NER TREF systems, to produce GNM and ONM from G and O by selecting named mentions.\n",
      "We used the Stanford NER tagger TREF with its default configuration as our full monolingual model for each language.\n",
      "The Stanford Named Entity Recognizer (NER) is based on the machine learning algorithm Condi\u0002tional Random Fields TREF and has been used extensively for identifying named enti\u0002ties in news text.\n",
      "These include several off-the-shelf sta\u0002tisical NLP tools such as the Stanford POS tagger REF, the Stanford named-entity recognizer (NER) TREF and the Stanford Parser REF.\n",
      "For example, TREF note that in the CoNLL dataset, the same term can be used for a location and for the name of a sports team.\n",
      "For EMD we used the Stanford named entity recognizer TREF.\n",
      "Stanford NER system first detects sentences in the data then labels four classes of named entities: PER\u0002SON, ORGANIZATION, LOCATION, and MIS_x005f\u0002CELLANEOUS TREF.\n",
      "The merged news file is acted upon by a log-linear part of speech tagger we obtained from the Stanford NLP webpage TREF\n",
      "We use the Stanford MaxentTagger REF for part\u0002of-speech tagging, and the Stanford Named En_x0002_tity Recognizer TREF for annotat\u0002ing named entities.\n",
      "NP chunks are computed from the parse trees provided in the CoNLL distribution, Named entities are ex\u0002tracted with the Stanford NER tool TREF.\n",
      "We ran the Stan\u0002ford Named Entity Recognition system TREF to obtain a set of 25,000 candidate men\u0002tions which the system judged to be names of people.\n",
      "For the learning of patterns we used the top 64 documents retrieved by Google and to recog\u0002nize the named entities in the pattern we apply several strategies, namely: 1) the Stanford’s Con\u0002ditional Random-Field-based named entity recog\u0002nizer TREF to detect entities of type HUMAN; 2) regular expressions to detect NUMERIC and DATE type entities; 3) gazetteers to detect enti\u0002ties of type LOCATION.\n",
      "We use Stanford named entity recognizer to extract named entities from the texts TREF.\n",
      "For the annotation pipeline we use the TnT POS tagger REF, WordNet REF, the YamCha chun_x005f_x0002_ker REF, the Stanford NERC TREF, and an in-house tem\u0002poral expressions recogniser.\n",
      "Both the blogs and the Wikipedia articles were tagged using the Stanford Named Entity Recog_x0002_nizer TREF, which labels the en\u0002tities according to these types: Time, Location, Organization, Person, Money, Percent, Date, and Miscellaneous.\n",
      "We use the Stanford Named Entity Recognizer TREF that tags named entities as peo\u0002ple, locations, organizations, and miscellaneous.\n",
      "One successful and freely available named entity recognizer is the Stanford NER system TREF, which provides an implementation of linear chain CRF sequence models, coupled with well-engineered feature extractors for NER, and trained with newswire documents.\n",
      "The Stanford tools perform part of speech tagging REF, constituent and dependency parsing REF, named entity recognition TREF, and coreference resolution REF.\n",
      "We used the Stanford NER Tagger REF to detect words that belong to one of four groups: Person, Location, Organization and Misc.\n",
      "Our first experiment utilizes WN, VN, and the Stanford Parser REF and Named Entity Recognizer TREF.\n",
      "Given the preponderance of terms on the web, using a named entity recognizer (e.g., TREF) for preprocessing may also provide benefits.\n",
      "We use Stanford NER TREF for named entity recognition.\n",
      "We extract further information indi\u0002cating whether a named entity, as identified by the Stanford NE Recognizer TREF begins at wi.\n",
      "In the first more common TREF schema, all PNs are divided into four MUC categories, i.e. per\u0002son, organization, location and other.\n",
      "We retrained the Stanford named entity recog\u0002nizer TREF on the OntoNotes data.\n",
      "External tools we used include WordNet REF for word base form and noun cate\u0002gory generation, Morphg REF to generate inflections of nouns and verbs, mate_x0002_tools REF for SRL, Stanford-ner TREF for name entity extraction and Longman online dictionary for generation  of noun countability and verb transitivity.\n",
      "To ex\u0002tract the extended targets, we capture named enti\u0002ties (NE) from the Wikipedia page of the debate topic (fetched using jsoup java library) using the Stanford Named Entity Recognizer TREF and sort them based on their page occur\u0002rence count.\n",
      "We utilized an off\u0002the-shelf system, Stanford Named Entity Recog_x0002_nizer TREF for detecting entity mentions on the English sentences.\n",
      "We first extract named entities from scattered opinions DT using Stanford Named Entity Recognizer TREF.\n",
      "First, all documents are parsed and processed with standard tools for named en\u0002tity recognition TREF and corefer\u0002ence resolution.\n",
      "the annotations from the Stanford Named Entity Recognizer TREF that labels PERSON, ORGANIZATION, and LOCATION entities.\n",
      "First, if NPi is an NE, we create a feature whose value is the NE label of NPi, as determined by the Stanford CRF-based NE recognizer TREF.\n",
      "A large body of literature in joint learning has demonstrated that such an approach can suffer from cascaded errors at testing, and does not ben\u0002efit from the potential for joint learning TREF.\n",
      "Some prior work REF demonstrated the effec\u0002tiveness of using semantic relations to improve entity coreference resolution; while (GTREF) experimented with in\u0002formation fusion of relations across multiple  documents.\n",
      "To compute the features which we extract in the next section, all instances in our data sets were part-of-speech tagged by the MXPOST tag_x0002_ger REF, parsed with the Malt\u0002Parser, and named entity tagged with the Stan_x0002_ford NE tagger TREF.\n",
      "Named Entity Recognizer tagging (NER): We in\u0002tegrated Stanford’s NER tagger TREF.\n",
      "For example, non-local features such as “same phrases in a document do not have different entity classes” were shown to be use\u0002ful in named entity recognition (GTREF).\n",
      "The use of annealing to obtain a maximum a posteri\u0002ori (MAP) configuration from sampling-based in\u0002ference is common (e.g., GTREF).\n",
      "REF use dis\u0002course trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. TREF and REF incorporate global information by en\u0002forcing event role or label consistency over a doc\u0002ument or across related documents.\n",
      "However, due to the lack of a fine grained NER tool at hand, we employ the Stanford NER package TREF which identifies only four types of named entities.\n",
      "Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer TREF.\n",
      "As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (GTREF)\n",
      "On the MUC6-TEST dataset, our system outperforms both REF (an un\u0002supervised Markov Logic Network system which uses explicit constraints) and TREF (a supervised system which uses ILP in\u0002ference to reconcile the predictions of a pairwise classifier) on all comparable measures\n",
      "We used the features generated by the CRF pack_x0002_age TREF. These features include the word string feature, the case feature for the current word, the context words for the current word and their cases, the presence in dictionaries for the current word, the position of the current word in the sentence, prefix and suffix of the current word as well as the case information of the multiple occurrences of the current word.\n",
      "For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named en\u0002tity recognizer (NER) TREF.\n",
      "We preprocess our textual data as follows: We first use the Stanford named entity recog\u0002nizer TREF to find entity mentions in the corpus. The NER tagger segments each docu\u0002ment into sentences and classifies each token into four categories: PERSON, ORGANIZATION, LO\u0002CATION and NONE.\n",
      "Then, we use the Stanford Named Entity Recognizer TREF to identify named entities, which we replace with a unique token (‘NE’).  Next, we replace all nouns with their POS tag; we use the Stanford POS Tagger REF.\n",
      "We use the Stanford NER Tagger TREF to discover these and segment the text accordingly.\n",
      "We used as candidates all strings labeled in the annotated data as well as all named entities found by the Stanford NER tagger for CoNLL TREF.\n",
      "Next we recognize named entities TREF by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags.\n",
      "Compared with the state-of-the-art news\u0002trained Stanford Named Entity Recognizer TREF, T-SEG obtains a 52% increase in F1 score. In addition there has been been work on Skip\u0002Chain CRFs (GTREF) which enforce consistency when classifying multi\u0002ple occurrences of an entity within a document.\n",
      "Various modifications to CRF have recently been introduced to take into account of non-local dependencies REF or broader context beyond training data TREF.\n",
      "The sampling distri\u0002butions are annealed, as a search technique to find the best configuration of assignments TREF.\n",
      "They used the Stanford named entity recog\u0002nizer TREF to find entity mentions in text and constructed relation mentions only between entity mentions in the same sentence.\n",
      "To identify potential speakers they used the Stanford NER tagger TREF and a method out\u0002lined in REF that allowed them to find nominal character references.\n",
      "We use Stanford Named Entity Rec\u0002ognizer TREF to collect named entities which are not in the Wikipedia list.\n",
      "We use the Stanford Core NLP suite (GTREF) to annotate each document with POS and NER tags, parse trees, and coreference chains.\n",
      "While earlier studies focus on sentence-level extraction REF, later ones turn to employ high-level information, such as document (GTREF), cross-event REF and cross\u0002entity REF information.\n",
      "For parser adaptation, self-training REF, using weakly annotated data from the tar\u0002get domain REF, ensemble learning REF, hierarchical bayesian models TREF and co-training REF achieve substantial performance gains. For a recent survey see REF.\n",
      "After post-processing (tokeniza￾tion, sentence-splitting, and part-of-speech tagging), named entities were automatically recognized and labeled with PER, ORG, LOC, and MISC TREF.\n",
      "Starting with about 752,450 pages, we ran the Stanford NER TREF to mark per\u0002son spans.\n",
      "Extracting Named Entities. EXEMPLAR employs the Stanford NER TREF to recognize named entities.\n",
      "The QA-SYS performs Part of Speech tagging using Stanford POS tagger REF, and Named Entity Recog\u0002nition using Stanford NER TREF, and then builds a Lucene index over the set of input documents.\n",
      "For training the CRF model, we used a compre\u0002hensive set of features from TREF that gives state-of-the-art results on this task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities were found using the Stanford named entity tagger TREF, and were matched to their name in Freebase.\n",
      "NER is a fairly researched field (GTREF) and is also used in several commer\u0002cial applications such as Zemanta, OpenCalais and AlchemyAPI , which are able to automatically in\u0002sert links for a NE pointing to a knowledge base such as Wikipedia or IMDB.\n",
      "A common approach is to utilize a Named Entity Recognition (NER) system like Stan\u0002ford NER TREF, which recognizes the names of things (e.g., person and product names) from texts.\n",
      "To assign types to arguments, we apply Stanford Named Entity Recognizer TREF, and also look up the argument in WordNet 2.1 and record the first three senses if they map to our target se\u0002mantic types.\n",
      "We identified these mentions of persons using Stanford NER TREF and treated each person mention as a single token.\n",
      "Stanford NER (or in short Stanford) associ\u0002ated to the following model provided by the tool and which was trained on different news corpora (CoNLL, MUC6, MUC7 and ACE): ner-eng-ie.crf-3-all2008-distsim.ser.gz TREF\n",
      "We then proceed to split the data into smaller sentences and tag them using Ratnaparkhi’s Maximum Entropy Tag_x0002_ger REF. We parse the data us\u0002ing the Collins Parser REF, and then tag person, location and organization names us\u0002ing the Stanford Named Entity Recognizer TREF.\n",
      "Some stem from work on graphical mod\u0002els, including loopy belief propagation REF, Gibbs sampling TREF, sequential Monte Carlo methods such as particle filtering REF, and variational inference REF.\n",
      "In all the experiments, our source side language is English, and the Stanford Named Entity Recognizer TREF was used to ex\u0002tract NEs from the source side article.\n",
      "We also use the Stanford NER tagger TREF to identify Named Entities within the NP.\n",
      "Named-entity information is provided by Stanford Tagger TREF.\n",
      "Named Entity The replaced token should not be part of a named entity. For this purpose, we applied the Stanford NER TREF.\n",
      "In the first approach, heuristic rules are used to find the dependencies REF or penalties for label inconsistency are required to handset ad-hoc TREF.\n",
      "We have used the freely available Stanford Named Entity Recognizer (GTREF) in our engine.\n",
      "From the above corpora, we first extracted all the NEs from the English side, using the Stanford NER tool TREF.\n",
      "Furthermore, as our tasks can be regarded as a sequential labeling problem (to label if a word is an opinion word, a target, or an ordinary word), we experimented with the conditional random fields (CRF) technique REF for extraction, which is a popular information extraction method and has been success\u0002fully used in labeling tasks such as POS tagging REF and Named Entity Recognition (GTREF).\n",
      "Then, we use the Stanford Named Entity Recognizer (GTREF) to identify named entities, which we replace with a unique token (‘NE’).\n",
      "First, we test the NE alignment performance with the same Chinese NE recognizer (Wu’s system, adopted earlier) but with different English NE recognizers that include the Mallet toolkit (used before), the Stanford NE recognizer (GTREF), and Minor Third REF.\n",
      "Such techniques include Gibbs sampling TREF, a general-purpose Monte Carlo method, and integer linear program\u0002ming (ILP), REF, a general-purpose exact framework for NP-complete problems. For named entity recognition, a phrase that appears multiple times should tend to get the same label each time TREF. In seminar announcements, a given field (speaker, start time, etc.) should appear with at most one value in each announcement, al\u0002though the field and value may be repeated TREF.\n",
      "Undirected graphical models such as Conditional Random Fields (CRFs) REF have shown great success for problems involving struc\u0002tured output variables (e.g. REF, TREF).\n",
      "Others have attempted to train global scoring functions using Gibbs sampling TREF, message propagation, REF, and integer linear programming REF.\n",
      "More distantly related, REF and TREF propose graphical models for combining information about a given entity from multiple mentions.\n",
      "Recent applications of statistical co\u0002reference models are beginning to show promise (GTREF).\n",
      "For the named entity features, we used a fairly standard feature set, similar to those described in TREF.\n",
      "Our features were based on those in TREF.\n",
      "We focus on training using Gibbs sampling REF, because it has been popularly applied in the natural language literature, e.g., (GTREF).\n",
      "C’s nameEntitySet represents the set of all named entities in C’s article labeled by Stan\u0002ford NER TREF.\n",
      "One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling TREF, another is (loopy) sum-product belief propagation REF.\n",
      "Named\u0002entity information was obtained by the Stanford tag_x0002_ger TREF.\n",
      "Global Constraints Previous work demonstrated the benefits of applying declarative constraints in in\u0002formation extraction (GTREF).\n",
      "Global Constraints Previous work demonstrated the benefits of applying declarative constraints in in\u0002formation extraction (GTREF).\n",
      "The Stanford named entity recognition (NER) software TREF is an implementation of linear chain Conditional Random Field (CRF) se\u0002quence models, which includes a three class (per\u0002son, organization, location and other) named entity recognizer for English.\n",
      "For identification of named entities, we use Stanford NER TREF.\n",
      "It uses Named Entity Recognition (NER) (GTREF) as a sub\u0002routine to identify named person entities, though we are also interested in unnamed persons such as “my teacher” and pronouns.\n",
      "More specifically, we seek a posterior distribution over latent variables that partition words in a sentence into flow and in\u0002ert groups; we estimate this posterior using Gibbs sampling TREF.\n",
      "English features were taken from TREF.\n",
      "We used Stanford NER TREF for English named entity recognition (NER), KNP for Japanese NER, and BaseNER REF for Chinese NER.\n",
      "In part-of-speech tagging, the accuracy of the Stanford tagger REF falls from 97% on Wall Street Journal text to 85% accuracy on Twitter REF. In named entity recognition, the CoNLL-trained Stan\u0002ford recognizer achieves 44% F-measure REF, down from 86% on the CoNLL test set TREF.\n",
      "For syntactic analysis we use the Stanford Parser TREF.\n",
      "In NER tasks, however, informa￾tion about a distant entity is often more useful than information about the previous state TREF.\n",
      "Global information is known to be use\u0002ful in other NLP tasks, especially in the named en\u0002tity recognition task, and several studies success\u0002fully used global features TREF\n",
      "TREF hand-set penalties for inconsistency in entity labeling at different occurrences in the text, based on some statistics from training data.\n",
      "An additional consistent edge of a linear-chain conditional random field (CRF) ex\u0002plicitly models the dependencies between distant occurrences of similar words (GTREF).\n",
      "We employ Gibbs sampling, previously used in NLP by TREF and REF, among others.\n",
      "We added named entity (NE) tags to the data using the tagger of TREF.\n",
      "Starting out with a chunking pipeline, which uses a classical combination of tagger and chun\u0002ker, with the Stanford POS tagger REF, the YamCha chunker REF and the Stanford Named Entity Rec_x0002_ognizer TREF, the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and John\u0002son’s reranking parser REF to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors.\n",
      "We perform named entity tagging using the Stanford four-class named entity tagger TREF.\n",
      "To implement this method, we first use the Stanford Named Entity Recognizer TREF to identify the set of person and organisation entities, E, from each article in the corpus.\n",
      "We processed each novel with the Stanford NER tagger TREF and extracted noun phrases that were categorized as persons or organizations.\n",
      "We run the Stanford Named Entity Recognizer TREF and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs.\n",
      "That is, a non-animate noun can hardly  constitute an experience. In order to make a dis\u0002tinction, we use the dependency parser and a named-entity recognizer TREF that can recognize person pronouns and person names.\n",
      "NE OpenNLP REF Recognizer Stanford TREF\n",
      "To reproduce the system by REF, we substitute the cast and crew list employed by them (see Section 3.2), with a NER component TREF.\n",
      "Proposed solutions to NER fall into three categories: 1) The rule-based REF; 2) the machine learn\u0002ing based (TREF) ; and 3) hybrid methods REF.\n",
      "Since knowledge extraction from web\u0002based encyclopedia is typically noisy REF, we use YAGO to determine whether two NPs have a relation only if one NP is a named entity (NE) of type person, organization, or location according to the Stanford NE recognizer TREF and the other NP is a common noun.\n",
      "TREF used Gibbs sampling, a sim\u0002ple Monte Carlo method used to perform approxi\u0002mate inference in factored probabilistic models.\n",
      "Semantic features: we use the Stanford NER tag_x0002_ger TREF to determine if the targeted NP is a named entity, and we use the Sundance parser REF to assign seman\u0002tic class labels to each NP’s head noun.\n",
      "To created we extract all NEs from the text using the Stanford NE Recognizer TREF and represent each NE by its Wikipedia URI.\n",
      "We use the Stanford Named Entity Recognizer TREF for this purpose.\n",
      "Due to the large size of the corpora, we uniformly sampled a subset of doc\u0002uments for each corpus and ran the Stanford NER tagger TREF, which tagged named en\u0002tities mentions as person, location, and organization.\n",
      "The Figure also shows the results of the Stanford NER tagger for English TREF (we used the MUC-7 classifier).\n",
      "Following REF, we filter out noisy doc\u0002uments and use natural language packages to anno\u0002tate the documents, including NER tagging TREF and dependency parsing REF.\n",
      "Run Stanford CoreNLP with POS tagging and named entity recognition TREF\n",
      "Mainly, pre\u0002determined word-level dependencies were repre\u0002sented as links in the underlying graphical model (GTREF).\n",
      "Accordingly, we extract all the named entities in a sentence using Stanford’s Name Entity Recognizer TREF.\n",
      "Similarly, the semantic phrase table (SPT) has been ex-tracted from the same corpora annotated with the Stanford NE tagger (GTREF).\n",
      "We use the Stanford coreNLP toolkit1 (GTREF) to extract  the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It includes: cleaning up and normalization of the input using reg\u0002ular expressions, sentence segmentation, tokeniza\u0002tion and lemmatization using GATE REF, syntactical parsing and dependency parsing (collapsed) using the Stanford Parser REF, and Named Entity Recogni\u0002tion using Stanford NER TREF.\n",
      "For example, the average F1 of the Stanford NER TREF drops from 90.8% REF to 45.8% on tweets, while Liu et al. (2010) report that the F1 score of a state-of\u0002the-art SRL system REF falls to 42.5% on tweets as apposed to 75.5% on news.\n",
      "BIUTEE provides state-of-the-art pre-processing utilities: Easy-First parser REF, Stanford named-entity-recognizer TREF and ArkRef coreference resolver REF, as well as utilities for sentence\u0002splitting and numerical-normalizations.\n",
      "The Stanford CRF-based NER tagger was used as the monolingual component in our models TREF.\n",
      "We utilize the Stanford tools (GTREF).\n",
      "Most well-known is the Stanford named entity recog\u0002nition (NER) tagger TREF which assigns coarse-grained types like person, organization, lo\u0002cation, and other to noun  phrases that are likely to denote entities.\n",
      "When training the Stanford NER system TREF on just the tokens from the Freire data set and the parameters from en\u0002glish.all.3class.distsim.prop (included in the Stan\u0002ford NER release, see also REF), our F-scores come very close to those reported by REF, but mostly with a higher recall and lower precision.\n",
      "To identify entities, we use a CRF-based named entity tagger TREF and a Chinese word breaker REF for English and Chinese corpora, respectively.\n",
      "We validate the hypothesis that using linguistic features, e.g., part-of-speech tags REF, named-entity tags TREF, and dependency trees REF,helps improve the quality of our approach, called Joint.\n",
      "Similarly, distributional features support generalization in Named Entity Recognition TREF.\n",
      "We used Open NLP POS tagger, Standford NER TREF and MaltParser REF to label/tag sentences.\n",
      "State-of-the-art tools for named entity recognition such as the Stanford NER Tagger TREF compute semantic tags only for a small set of coarse-grained types: Person, Location, and Organization (plus tags for non-entity phrases of type time, money, percent, and date).\n",
      "The first is named entity (NE) tags (PERSON, ORGANI\u0002ZATION and LOCATION) returned by the Stanford NE recognition tool TREF.\n",
      "Corry has only participated in the “open” setting, as it has already a number of preprocessing mod\u0002ules integrated into the system: the Stanford NLP toolkit for parsing REF and NE-tagging TREF, Wordnet for se\u0002mantic classes and the U.S. census data for assigning gender values to person names.\n",
      "Named Entities using the Stanford named en\u0002tity recognizer recognizer TREF\n",
      "The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly out\u0002perform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (GTREF).\n",
      "For capturing and normalizing the above mentioned expressions, we make use of the Stan\u0002ford NER Toolkit TREF.\n",
      "Named entity recognition The Stanford Named Entity Recogniser TREF was run over the BNC and any nouns that occur in the corpus with both named entity and non-named entity tags are ex\u0002tracted to form the list Stanford.\n",
      "The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (GTREF).\n",
      "We analysed the text in the metadata, performing lemmatization, PoS tagging, named entity recognition and classification (NERC) and date detection using Stanford CoreNLP (GTREF).\n",
      "In our experiments, we performed named entity recognition with the Stanford NER tool using the standard English model TREF.\n",
      "We used the Stanford Named Entity Recognizer by TREF, with the 7 class model trained for MUC: Time, Location, Organization, Person, Money, Percent, Date.\n",
      "Most previous work with CRFs containing non\u0002local dependencies used approximate probabilis\u0002tic inference techniques, including TRP REF and Gibbs sampling TREF.\n",
      "There is a significant volume of work explor\u0002ing the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (GTREF).\n",
      "One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer TREF.\n",
      "We have chosen to com\u0002pare against the Stanford tagger because to the best of our knowledge, it is the best publicly available system which is trained on the same data. We have downloaded the Stanford NER tagger and used the strongest provided model trained on the CoNLL03 data with distributional similarity features. The re\u0002sults we obtained on the CoNLL03 test set were consistent with what was reported in TREF.\n",
      "We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (GTREF) and dependency parsing REF with a great deal of success.\n",
      "The texts we use in our experiments are the develop\u0002ment set of the RTE-5 challenge REF, and we preprocess the data using the Stan\u0002ford named-entity recognizer TREF.\n",
      "TREF used Gibbs Sampling to add non-local dependencies into linear-chain CRF model for information extraction.\n",
      "In comparison, there are nu￾merous Named Entity recognition (NER) sys￾tems, both general-purpose and specialized, and many of them achieve scores better than F1 = 0.95 (GTREF)\n",
      "We used the Stanford NER tagger TREF with its default configuration as our full monolingual model for each language.\n",
      "The Stanford Named Entity Recognizer (NER) is based on the machine learning algorithm Conditional Random Fields TREF and has been used extensively for identifying named enti\u0002ties in news text.\n",
      "These include several off-the-shelf sta\u0002tisical NLP tools such as the Stanford POS tagger \n",
      "REF, the Stanford named-entity recognizer (NER) TREF and the Stanford Parser REF.\n",
      "The Stanford Tagger is based on Conditional Ran\u0002dom Fields TREF.\n",
      "For EMD we used the Stanford named entity recognizer TREF.\n",
      "Next, the spelling corrected dataset is run through the Stanford Named Entity Recognizer (NER). Stanford NER system first detects sentences in the data then labels four classes of named entities: PERSON, ORGANIZATION, LOCATION, and MIS\u0002CELLANEOUS TREF.\n",
      "The preprocessor merges all the files into one but defines start/end delimiters for each file in the merged file, to enable bulk processing. The merged news file is acted upon by a log-linear part of speech tagger we obtained from the Stanford NLP webpage TREF.\n",
      "We use the Stan\u0002ford MaxentTagger REF for part\u0002of-speech tagging, and the Stanford Named Entity Recognizer TREF for annotat\u0002ing named entities.\n",
      "First, we create a list of candidate mentions by merging basic NP chunks with named entities. NP chunks are computed from the parse trees provided in the CoNLL distribution, Named entities are ex\u0002tracted with the Stanford NER tool TREF.\n",
      "Evaluation was performed on a corpus of blogs describing United States politics in 2008 REF. We ran the Stan\u0002ford Named Entity Recognition system TREF to obtain a set of 25,000 candidate men\u0002tions which the system judged to be names of people.\n",
      "For the learning of patterns we used the top 64 documents retrieved by Google and to recognize the named entities in the pattern we apply several strategies, namely: 1) the Stanford’s Con\u0002ditional Random-Field-based named entity recog_x0002_nizer TREF to detect entities of type HUMAN; 2) regular expressions to detect NUMERIC and DATE type entities; 3) gazetteers to detect enti\u0002ties of type LOCATION.\n",
      "If more named entities co-occur in two documents, they are very likely to talk about the same event or subject and thus should be more comparable. We use Stanford named entity recognizer to extract named entities from the texts TREF\n",
      "For the annotation pipeline we use the TnT POS tagger REF, WordNet REF, the YamCha chun_x0002_ker REF, the Stanford NERC TREF, and an in-house tem\u0002poral expressions recogniser.\n",
      "Both the blogs and the Wikipedia articles were tagged using the Stanford Named Entity Recog\u0002nizer TREF, which labels the en\u0002tities according to these types: Time, Location, Organization, Person, Money, Percent, Date, and Miscellaneous.\n",
      "We use the Stanford Named Entity Recognizer TREF that tags named entities as peo\u0002ple, locations, organizations, and miscellaneous.\n",
      "One successful and freely available named entity recognizer is the Stanford NER system TREF, which provides an implementation of linear chain CRF sequence models, coupled with well-engineered feature extractors for NER, and trained with newswire documents.\n",
      "Each dialog was processed using the Stanford Core NLP tools. The Stanford tools perform part of speech tagging REF, constituent and dependency parsing REF, named entity recognition TREF, and coreference resolution REF\n",
      "We used the Stanford NER Tagger TREF to detect words that belong to one of four groups: Person, Lo\u0002cation, Organization and Misc. Each group is repre\u0002sented by a binary feature.\n",
      "Our first experiment utilizes WN, VN, and the Stanford Parser REF and Named Entity Recognizer TREF.\n",
      "In general, language models could be used for more context-sensitive spelling correction. Given the preponderance of terms on the web, using a named entity recognizer (e.g., TREF) for preprocessing may also provide benefits.\n",
      "We use Stanford NER TREF for named entity recognition. Figure 1 shows a sentence and its corre\u0002sponding semantic pattern.\n",
      "We extract further information indi\u0002cating whether a named entity, as identified by the Stanford NE Recognizer TREF begins at wi.\n",
      "We have evaluated two schemas with a limited number of the NE categories. In the first more common TREF schema, all PNs are divided into four MUC categories, i.e. per\u0002son, organization, location and other\n",
      "We retrained the Stanford named entity recognizer TREF on the OntoNotes data.\n",
      "External tools we used include WordNet REF for word base form and noun cate\u0002gory generation, Morphg TREF to generate inflections of nouns and verbs, mate tools REF for SRL, Stanford-ner TREF for name entity extraction and Longman online dictionary for generation of noun countability and verb transitivity\n",
      "To ex\u0002tract the extended targets, we capture named enti\u0002ties (NE) from the Wikipedia page of the debate topic (fetched using jsoup java library) using the Stanford Named Entity Recognizer TREF and sort them based on their page occur\u0002rence count. Out of top-k (k = 20) NEs, some can belong to both of the debate topics\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "X_input_ids, X_attn_masks = generate_training_data(df, X_input_ids, X_attn_masks, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8478d4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101.,  1967.,  1195., ...,     0.,     0.,     0.],\n",
       "       [  101., 14895.,  5208., ...,     0.,     0.,     0.],\n",
       "       [  101.,  2994., 22559., ...,     0.,     0.,     0.],\n",
       "       ...,\n",
       "       [  101.,  1284.,  1231., ...,     0.,     0.,     0.],\n",
       "       [  101., 22841.,  5537., ...,     0.,     0.,     0.],\n",
       "       [  101.,  1706., 16143., ...,     0.,     0.,     0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccb16e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_attn_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b0417f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(617, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.zeros((len(df), 4))\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9455af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[np.arange(len(df)), df['Follow-up'].values] = 1 # one-hot encoded target tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "354a5012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb61c3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(4,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n",
    "dataset.take(1) # one sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f64c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CitationDatasetMapFunction(input_ids, attn_masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attn_masks\n",
    "    }, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59775ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(CitationDatasetMapFunction) # converting to required format for tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d58e6a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(4,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f40c89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(10000).batch(16, drop_remainder=True) # batch size, drop any left out tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6eb5a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(16, 256), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(16, 256), dtype=tf.float64, name=None)}, TensorSpec(shape=(16, 4), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfddb11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.8\n",
    "train_size = int((len(df)/16)*p) # for each 4 batch of data we will have len(df)//16 samples, take 80% of that for train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c1cc751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "617\n"
     ]
    }
   ],
   "source": [
    "print(train_size)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2bff21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0b294c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9675b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3ca0b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained('bert-base-cased') # bert base model with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dd501fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 256,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " intermediate_layer (Dense)     (None, 512)          393728      ['tf_bert_model[0][1]']          \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 4)            2052        ['intermediate_layer[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,706,052\n",
      "Trainable params: 108,706,052\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\n",
    "attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n",
    "\n",
    "bert_embds = model(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\n",
    "intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\n",
    "output_layer = tf.keras.layers.Dense(4, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n",
    "\n",
    "citation_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
    "citation_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ced44347",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0161274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fe15f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_model.compile(optimizer=optim, loss=loss_func, metrics=[tf.keras.metrics.CategoricalAccuracy('accuracy'),\n",
    "                              tf.keras.metrics.Precision(),\n",
    "                              tf.keras.metrics.Recall(),\n",
    "                              tfa.metrics.F1Score(num_classes=4,\n",
    "                                                  average='micro',\n",
    "                                                  threshold=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb90c946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 1907s 63s/step - loss: 1.1250 - accuracy: 0.5500 - precision: 0.7685 - recall: 0.1729 - f1_score: 0.2823 - val_loss: 0.9563 - val_accuracy: 0.6094 - val_precision: 0.7471 - val_recall: 0.5078 - val_f1_score: 0.6047\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 2070s 69s/step - loss: 0.8553 - accuracy: 0.6812 - precision: 0.7197 - recall: 0.6313 - f1_score: 0.6726 - val_loss: 0.5892 - val_accuracy: 0.8047 - val_precision: 0.8462 - val_recall: 0.7734 - val_f1_score: 0.8082\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 2289s 77s/step - loss: 0.7446 - accuracy: 0.7375 - precision: 0.7878 - recall: 0.6729 - f1_score: 0.7258 - val_loss: 0.7880 - val_accuracy: 0.7109 - val_precision: 0.7607 - val_recall: 0.6953 - val_f1_score: 0.7265\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 1934s 65s/step - loss: 0.6453 - accuracy: 0.7708 - precision: 0.8392 - recall: 0.7396 - f1_score: 0.7863 - val_loss: 0.5017 - val_accuracy: 0.8125 - val_precision: 0.8571 - val_recall: 0.7969 - val_f1_score: 0.8259\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 1742s 58s/step - loss: 0.5516 - accuracy: 0.8042 - precision: 0.8602 - recall: 0.7563 - f1_score: 0.8049 - val_loss: 0.5124 - val_accuracy: 0.8047 - val_precision: 0.8644 - val_recall: 0.7969 - val_f1_score: 0.8293\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 5340s 182s/step - loss: 0.4487 - accuracy: 0.8604 - precision: 0.8867 - recall: 0.8313 - f1_score: 0.8581 - val_loss: 0.4012 - val_accuracy: 0.8516 - val_precision: 0.8710 - val_recall: 0.8438 - val_f1_score: 0.8571\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 3471s 118s/step - loss: 0.4079 - accuracy: 0.8667 - precision: 0.8933 - recall: 0.8375 - f1_score: 0.8645 - val_loss: 0.3408 - val_accuracy: 0.8828 - val_precision: 0.9024 - val_recall: 0.8672 - val_f1_score: 0.8845\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 1976s 66s/step - loss: 0.4094 - accuracy: 0.8729 - precision: 0.8971 - recall: 0.8354 - f1_score: 0.8652 - val_loss: 0.3149 - val_accuracy: 0.8984 - val_precision: 0.8976 - val_recall: 0.8906 - val_f1_score: 0.8941\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 2032s 68s/step - loss: 0.3506 - accuracy: 0.8750 - precision: 0.9124 - recall: 0.8458 - f1_score: 0.8778 - val_loss: 0.2486 - val_accuracy: 0.9141 - val_precision: 0.9504 - val_recall: 0.8984 - val_f1_score: 0.9237\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 1775s 59s/step - loss: 0.3213 - accuracy: 0.8958 - precision: 0.9129 - recall: 0.8729 - f1_score: 0.8924 - val_loss: 0.2790 - val_accuracy: 0.8984 - val_precision: 0.9008 - val_recall: 0.8516 - val_f1_score: 0.8755\n"
     ]
    }
   ],
   "source": [
    "hist = citation_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    batch_size=8,\n",
    "    verbose=1,                    \n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de44fc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bert_model_tagged_sentences/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bert_model_tagged_sentences/assets\n"
     ]
    }
   ],
   "source": [
    "citation_model.save('bert_model_tagged_sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f139b0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f84135a86d0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracy')\n",
    "\n",
    "xaxis = np.arange(len(hist.history['accuracy']))\n",
    "plt.plot(xaxis, hist.history['accuracy'], label='Train set')\n",
    "plt.plot(xaxis, hist.history['val_accuracy'], label='Validation set')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acb33241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.94%\n"
     ]
    }
   ],
   "source": [
    "scores = citation_model.evaluate(val_dataset, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95690826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
