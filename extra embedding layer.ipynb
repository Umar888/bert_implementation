{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f5e5fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 10:33:08.040814: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annotator</th>\n",
       "      <th>Paper</th>\n",
       "      <th>Cited-by</th>\n",
       "      <th>Follow-up</th>\n",
       "      <th>Citing Sentence</th>\n",
       "      <th>Tagged Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>A00-1043</td>\n",
       "      <td>C00-2140</td>\n",
       "      <td>0</td>\n",
       "      <td>Since we only use shallow methods for textual ...</td>\n",
       "      <td>Since we only use shallow methods for textual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>A00-1043</td>\n",
       "      <td>P02-1057</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentence simplification systems (Chandrasekar ...</td>\n",
       "      <td>Sentence simplification systems (GTREF) are ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>W09-1118</td>\n",
       "      <td>1</td>\n",
       "      <td>Each token is represented using a fairly stand...</td>\n",
       "      <td>Each token is represented using a fairly stand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>A00-2017</td>\n",
       "      <td>1</td>\n",
       "      <td>The training and the test data were processed ...</td>\n",
       "      <td>The training and the test data were processed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>C00-2099</td>\n",
       "      <td>0</td>\n",
       "      <td>The only other high-\f",
       "delity computational rend...</td>\n",
       "      <td>The only other high-_x005f\f",
       "delity computationa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Annotator     Paper  Cited-by  Follow-up  \\\n",
       "0         A  A00-1043  C00-2140          0   \n",
       "1         A  A00-1043  P02-1057          0   \n",
       "2         A  A97-1011  W09-1118          1   \n",
       "3         A  A97-1011  A00-2017          1   \n",
       "4         A  A97-1011  C00-2099          0   \n",
       "\n",
       "                                     Citing Sentence  \\\n",
       "0  Since we only use shallow methods for textual ...   \n",
       "1  Sentence simplification systems (Chandrasekar ...   \n",
       "2  Each token is represented using a fairly stand...   \n",
       "3  The training and the test data were processed ...   \n",
       "4  The only other high-\n",
       "delity computational rend...   \n",
       "\n",
       "                                     Tagged Sentence  \n",
       "0  Since we only use shallow methods for textual ...  \n",
       "1  Sentence simplification systems (GTREF) are ca...  \n",
       "2  Each token is represented using a fairly stand...  \n",
       "3  The training and the test data were processed ...  \n",
       "4  The only other high-_x005f\n",
       "delity computationa...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "df = pd.read_csv(\"/Users/revglue/study/main_work/my_working/my_data_set/my_data_set.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e2269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Citation Type']=df['Follow-up'].apply(lambda x: 'Related work' if x==0 else ('Comparison' if x==1 else ('Using the work' if x==2 else 'Extending the work')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22b71fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def softmax(z):\n",
    "    z_exp = [math.exp(i) for i in z]\n",
    "    sum_z_exp = sum(z_exp)\n",
    "    return [i / sum_z_exp for i in z_exp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ff06691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annotator</th>\n",
       "      <th>Paper</th>\n",
       "      <th>Cited-by</th>\n",
       "      <th>Follow-up</th>\n",
       "      <th>Citing Sentence</th>\n",
       "      <th>Tagged Sentence</th>\n",
       "      <th>Citation Type</th>\n",
       "      <th>Coarse Label</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>A00-1043</td>\n",
       "      <td>C00-2140</td>\n",
       "      <td>0</td>\n",
       "      <td>Since we only use shallow methods for textual ...</td>\n",
       "      <td>Since we only use shallow methods for textual ...</td>\n",
       "      <td>Related work</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>A00-1043</td>\n",
       "      <td>P02-1057</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentence simplification systems (Chandrasekar ...</td>\n",
       "      <td>Sentence simplification systems (GTREF) are ca...</td>\n",
       "      <td>Related work</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>W09-1118</td>\n",
       "      <td>1</td>\n",
       "      <td>Each token is represented using a fairly stand...</td>\n",
       "      <td>Each token is represented using a fairly stand...</td>\n",
       "      <td>Comparison</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.001380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>A00-2017</td>\n",
       "      <td>1</td>\n",
       "      <td>The training and the test data were processed ...</td>\n",
       "      <td>The training and the test data were processed ...</td>\n",
       "      <td>Comparison</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.001380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>A97-1011</td>\n",
       "      <td>C00-2099</td>\n",
       "      <td>0</td>\n",
       "      <td>The only other high-\f",
       "delity computational rend...</td>\n",
       "      <td>The only other high-_x005f\f",
       "delity computationa...</td>\n",
       "      <td>Related work</td>\n",
       "      <td>Incidental</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Annotator     Paper  Cited-by  Follow-up  \\\n",
       "0         A  A00-1043  C00-2140          0   \n",
       "1         A  A00-1043  P02-1057          0   \n",
       "2         A  A97-1011  W09-1118          1   \n",
       "3         A  A97-1011  A00-2017          1   \n",
       "4         A  A97-1011  C00-2099          0   \n",
       "\n",
       "                                     Citing Sentence  \\\n",
       "0  Since we only use shallow methods for textual ...   \n",
       "1  Sentence simplification systems (Chandrasekar ...   \n",
       "2  Each token is represented using a fairly stand...   \n",
       "3  The training and the test data were processed ...   \n",
       "4  The only other high-\n",
       "delity computational rend...   \n",
       "\n",
       "                                     Tagged Sentence Citation Type  \\\n",
       "0  Since we only use shallow methods for textual ...  Related work   \n",
       "1  Sentence simplification systems (GTREF) are ca...  Related work   \n",
       "2  Each token is represented using a fairly stand...    Comparison   \n",
       "3  The training and the test data were processed ...    Comparison   \n",
       "4  The only other high-_x005f\n",
       "delity computationa...  Related work   \n",
       "\n",
       "  Coarse Label  normalized  \n",
       "0   Incidental    0.000508  \n",
       "1   Incidental    0.000508  \n",
       "2   Incidental    0.001380  \n",
       "3   Incidental    0.001380  \n",
       "4   Incidental    0.000508  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Coarse Label']=df['Follow-up'].apply(lambda x: 'Incidental' if x==0 or x==1 else 'Important')\n",
    "df['normalized']=softmax(df['Follow-up'])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d86e9557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = TFBertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d43a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(df, ids, masks, tokenizer):\n",
    "    for i, text in tqdm(enumerate(df['Tagged Sentence'])):\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            text.lower(),\n",
    "            max_length=512, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            add_special_tokens=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        ids[i, :] = tokenized_text.input_ids\n",
    "        masks[i, :] = tokenized_text.attention_mask\n",
    "    return ids, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f6e05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_input_ids = np.zeros((len(df), 512))\n",
    "X_attn_masks = np.zeros((len(df), 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56c0c056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de28f72513964100b38e62fb03b9a298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "X_input_ids, X_attn_masks = generate_training_data(df, X_input_ids, X_attn_masks, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1041a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros((len(df), 4))\n",
    "labels[np.arange(len(df)), df['Follow-up'].values] = 1\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7cb1cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CitationDatasetMapFunction(input_ids, attn_masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attn_masks\n",
    "    }, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3654b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(CitationDatasetMapFunction)\n",
    "dataset = dataset.shuffle(10000).batch(16, drop_remainder=True)\n",
    "p = 0.8\n",
    "train_size = int((len(df)/16)*p)\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca490ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
    "attn_masks = tf.keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32')\n",
    "citation_type_ids = tf.keras.layers.Input(shape=(1,), name='citation_type_ids', dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a7524b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_type_embedding = tf.keras.layers.Embedding(4, 30)(citation_type_ids)\n",
    "citation_type_embedding = tf.keras.backend.squeeze(citation_type_embedding, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96da22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embds = model(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D) \n",
    "citation_type_and_text = tf.keras.layers.concatenate([bert_embds, citation_type_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ce2cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_layer = tf.keras.layers.Dense(1024, activation='relu', name='intermediate_layer')(citation_type_and_text)\n",
    "output_layer = tf.keras.layers.Dense(4, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a926eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " citation_type_ids (InputLayer)  [(None, 1)]         0           []                               \n",
      "                                                                                                  \n",
      " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1, 30)        120         ['citation_type_ids[0][0]']      \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze (TFOpLamb  (None, 30)          0           ['embedding[0][0]']              \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 798)          0           ['tf_bert_model[0][1]',          \n",
      "                                                                  'tf.compat.v1.squeeze[0][0]']   \n",
      "                                                                                                  \n",
      " intermediate_layer (Dense)     (None, 1024)         818176      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 4)            4100        ['intermediate_layer[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,132,668\n",
      "Trainable params: 109,132,668\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "citation_model = tf.keras.Model(inputs=[input_ids, attn_masks, citation_type_ids], outputs=output_layer)\n",
    "citation_model.summary() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
