Annotator,Paper,Cited-by,Follow-up,Citing Sentence,Tagged Sentence
A,A00-1043,C00-2140,0,"Since we only use shallow methods for textual analysis that do not generate a dependency structure, we cannot use complex methods for text reduction as described, e.g., in (Jing, 2000).","Since we only use shallow methods for textual analysis that do not generate a dependency structure, we cannot use complex methods for text reduction as described, e.g., in TREF."
A,A00-1043,P02-1057,0,"Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phrases.",Sentence simplification systems (GTREF) are capable of compressing long sentences by deleting unimportant words and phrases.
A,A97-1011,W09-1118,1,"Each token is represented using a fairly standard menagerie of features, including such stemming from the surface appearance of the token (e.g., Contains dollar? Length in characters), calculated based on linguistic pre-processing made with the English Functional Dependency Grammar (Tapanainen and J¨arvinen, 1997) (e.g., Case, Part-of-speech), fetched from precompiled lists of information (e.g., Is first name?), and features based on predictions concerning the context of the token (e.g, Class of previous token).","Each token is represented using a fairly standard menagerie of features, including such stemming from the surface appearance of the token (e.g., Contains dollar? Length in characters), calculated based on linguistic pre-processing made with the English Functional Dependency Grammar TREF (e.g., Case, Part-of-speech), fetched from precompiled lists of information (e.g., Is first name?), and features based on predictions concerning the context of the token (e.g, Class of previous token)."
A,A97-1011,A00-2017,1,"The training and the test data were processed by the FDG parser (Tapanainen and Jrvinen, 1997).",The training and the test data were processed by the FDG parser TREF.
A,A97-1011,C00-2099,0,"The only other high-delity computational rendering of Tesniere's dependency syntax that we are aware of is that of (Tapanainen and Jarvinen, 1997), which is neither generative nor statistical.","The only other high-_x005fdelity computational rendering of Tesniere's dependency syntax that we are aware of is that of TREF, which is neither generative nor statistical."
A,A97-1011,W04-1505,0,"The Statistical Dependency Model Most successful deep-linguistic Dependency Parsers (Lin, 1998; Tapanainen and Jarvinen, 1997) do not have a statistical base.",The Statistical Dependency Model Most successful deep-linguistic Dependency Parsers (GTREF) do not have a statistical base.
A,A97-1011,P99-1033,0,"J~irvinen and Tapananinen have demonstrated an efficient wide-coverage dependency parser for English (Tapanainen and J~irvinen, 1997; J£rvinen and Tapanainen, 1998).",J~irvinen and Tapananinen have demonstrated an efficient wide-coverage dependency parser for English (GTREF).
A,A97-1011,W06-0202,3,"Generating Dependency Patterns Three dependency parsers were used for these experiments: MINIPAR3 (Lin, 1999), the Machinese Syntax4 parser from Connexor Oy (Tapanainen and J¨arvinen, 1997) and the Stanford5 parser (Klein and Manning, 2003).","Generating Dependency Patterns Three dependency parsers were used for these experiments: MINIPAR3 REF, the Machinese Syntax4 parser from Connexor Oy TREF and the Stanford5 parser REF."
A,A97-1011,P01-1006,3,"Pre-processing tools Parser The current version of the evaluation workbench employs one of the high performance ”super-taggers” for English - Conexor’s FDG Parser (Tapanainen and Jarvinen, 1997).",Pre-processing tools Parser The current version of the evaluation workbench employs one of the high performance ”super-taggers” for English - Conexor’s FDG Parser TREF.
A,A97-1011,E12-1072,0,"The corpus was parsed by Connexor’s Machinese Syntax (Connexor Oy, 2006), which returns lexical and morphological information as well as the dependency relations between words by employing a functional dependency grammar (Tapanainen and Jarvinen, 1997).","The corpus was parsed by Connexor’s Machinese Syntax REF, which returns lexical and morphological information as well as the dependency relations between words by employing a functional dependency grammar TREF."
A,C00-1072,C02-1130,2,"A topic signature, as described in (Lin and Hovy, 2000), is a list of terms that can be used to signal the membership of a text in the relevant topic or category. We implemented the algorithm described in (Lin and Hovy, 2000) with the addition of a cutoff, such that the topic signatures for a term are only included if the p1/p2 for that term is greater than the mean p1/p2 over all terms.","A topic signature, as described in TREF, is a list of terms that can be used to signal the membership of a text in the relevant topic or category. We implemented the algorithm described in TREF with the addition of a cutoff, such that the topic signatures for a term are only included if the p1/p2 for that term is greater than the mean p1/p2 over all terms."
A,C00-1072,C04-1077,1,"This is an extension of Lin’s method (Lin and Hovy, 2000).",This is an extension of Lin’s method TREF.
A,C00-1072,C08-1021,1,"Topic Signatures Topic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy, 2000).",Topic Signatures Topic Signatures (TS) are word vectors related to a particular topic TREF.
A,C00-1072,C08-1124,1,"Recently, content features were also well studied, including centroid (Radev et al., 2004), signature terms (Lin and Hovy, 2000) and high frequency words (Nenkova e t al., 2006). Lin and Hovy (2000) identified signature terms that were strongly associated with documents based on statistics measures.","Recently, content features were also well studied, including centroid REF, signature terms TREF and high frequency words REF. REF identified signature terms that were strongly associated with documents based on statistics measures."
A,C00-1072,D08-1080,0,"More advanced methods for query expansion use “topic signatures” – words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as relevant or not (Lin & Hovy, 2000; Harabagiu, 2004).",More advanced methods for query expansion use “topic signatures” – words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as relevant or not (GTREF).
A,C00-1072,D09-1032,1,"Topic signatures are words highly descriptive of the input, as determined by the application of loglikelihood test (Lin and Hovy, 2000).","Topic signatures are words highly descriptive of the input, as determined by the application of loglikelihood test TREF."
A,C00-1072,E09-1062,0,"Log-likelihood ratio for words in the input Number of topic signature words (Lin and Hovy, 2000; Conroy et al., 2006) and percentage of signature words in the vocabulary.",Log-likelihood ratio for words in the input Number of topic signature words (GTREF) and percentage of signature words in the vocabulary.
A,C00-1072,I08-1016,0,"NP-rewrite enhanced frequency summarizer Frequency and frequency-related measures of importance have been traditionally used in text summarization as indicators of importance (Luhn, 1958; Lin and Hovy, 2000; Conroy et al., 2006).",NP-rewrite enhanced frequency summarizer Frequency and frequency-related measures of importance have been traditionally used in text summarization as indicators of importance (GTREF ).
A,C00-1072,J10-1003,0,"Comparing the Machine-Made Summaries and the Manually Created Extracts Measuring sentence co-selection between extractive summaries created by humans and those created by automatic summarizers has a long tradition in the text summarization community (Lin and Hovy 2000; Marcu 2000), but this family of measures has a number of well-known shortcomings.","Comparing the Machine-Made Summaries and the Manually Created Extracts Measuring sentence co-selection between extractive summaries created by humans and those created by automatic summarizers has a long tradition in the text summarization community (GTREF), but this family of measures has a number of well-known shortcomings."
A,C00-1072,J11-1001,0,ROUGE (Lin and Hovy 2000) compares any summary to any other (typically human-generated) summary using a recall-oriented approach.,ROUGE TREF compares any summary to any other (typically human-generated) summary using a recall-oriented approach.
A,C00-1072,P02-1058,2,This is similar to the idea of topic signature introduced in (Lin and Hovy 2000).,This is similar to the idea of topic signature introduced in TREF.
A,C00-1072,P05-1026,0,"The notion of topic signatures was first introduced in (Lin and Hovy, 2000).",The notion of topic signatures was first introduced in TREF.
A,C00-1072,P06-1015,0,"To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al. 2005), and word similarity lists (Hindle 1990).","To date, researchers have harvested, with varying success, several resources, including concept lists REF, topic signatures TREF, facts REF, and word similarity lists REF."
A,C00-1072,P06-4007,0,"Under this approach, topic representations like those introduced in (Lin and Hovy, 2000) and (Harabagiu, 2004) are used to identify a set of text passages that are relevant to a user’s domain of interest.","Under this approach, topic representations like those introduced in TREF and REF are used to identify a set of text passages that are relevant to a user’s domain of interest."
A,C00-1072,P07-1070,0,"The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (Hovy and Lin, 1997; Lin and Hovy, 2000), etc.","The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (GTREF), etc."
A,C00-1072,P08-1090,0,"In summarization, topic signatures are a set of terms indicative of a topic (Lin and Hovy, 2000).","In summarization, topic signatures are a set of terms indicative of a topic TREF."
A,C00-1072,P08-1092,0,"A similar approach is explored in Biryukov et al. (2005), which uses Topic Signatures (Lin and Hovy, 2000) constructed around the target individual’s name to identify sentences to be included in the biography.","A similar approach is explored in REF, which uses Topic Signatures TREF constructed around the target individual’s name to identify sentences to be included in the biography."
A,C00-1072,P09-1023,0,"In addition, infobox could be considered as topic signature (Lin and Hovy, 2000) or keywords about the topic.","In addition, infobox could be considered as topic signature TREF or keywords about the topic."
A,C00-1072,P09-2030,0,"Among them, query relevance, centroid (Radev et al., 2004) and signature term (Lin and Hovy, 2000) are most remarkable.","Among them, query relevance, centroid REF and signature term TREF are most remarkable."
A,C00-1072,P10-1094,0,"For single document summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (Luhn 1969; Lin and Hovy, 2000).","For single document summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (GTREF)."
A,C00-1072,P11-1155,0,"In the task of single document summarization, various features have been investigated for ranking sentences in a document, including term frequency, sentence position, cue words, stigma words, and topic signature (Luhn 1969; Lin and Hovy, 2000).","In the task of single document summarization, various features have been investigated for ranking sentences in a document, including term frequency, sentence position, cue words, stigma words, and topic signature (GTREF)."
A,C02-1025,C10-2104,0,"In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.","In such cases, neither global features TREF nor aggregated contexts REF can help."
A,C02-1025,P03-1028,3,"More recently, machine learning approaches have been used for IE from semi-structured texts (Califf and Mooney, 1999; Soderland, 1999; Roth and Yih, 2001; Ciravegna, 2001; Chieu and Ng, 2002a), named entity extraction (Chieu and Ng, 2002b), template element extraction, and template relation extraction (Miller et al., 1998).","More recently, machine learning approaches have been used for IE from semi-structured texts (GREF), named entity extraction TREF, template element extraction, and template relation extraction REF."
A,C02-1025,P05-1045,0,"Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.","TREF propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document."
A,C02-1025,P05-1051,0,"(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.",REF made a second tagging pass which uses information on token sequences tagged in the first pass; TREF used as features information about features assigned to other instances of the same token.
A,C02-1025,P06-1089,0,"Global information is known to be useful in other NLP tasks, especially in the named entity recognition task, and several studies successfully used global features (Chieu and Ng, 2002; Finkel et al., 2005). Chieu and Ng (2002) conducted named entity recognition using global features as well as local features.","Global information is known to be useful in other NLP tasks, especially in the named entity recognition task, and several studies successfully used global features (GTREF). TREF conducted named entity recognition using global features as well as local features."
A,C02-1025,P06-1141,1,"Most work has looked to model non-local dependencies only within a document (Finkel 1125 et al., 2005; Chieu and Ng, 2002; Sutton and McCallum, 2004; Bunescu and Mooney, 2004). Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features based on known information, taken from other occurrences of the same token in the document.","Most work has looked to model non-local dependencies only within a document ( GTREF). TREF propose a solution to this problem: for each token, they define additional features based on known information, taken from other occurrences of the same token in the document."
A,H05-1079,C08-1028,0,Impressive results have been achieved culminating in the state-of-the-art parser of Clark and Curran (2004) which has been used as the parser for the Pascal Rich Textual Entailment Challenge entry of Bos and Markert (2005).,Impressive results have been achieved culminating in the state-of-the-art parser of REF which has been used as the parser for the Pascal Rich Textual Entailment Challenge entry of TREF.
A,H05-1079,C08-2001,0,"Metaphor in RTE Challenges Even though annotators aimed to filter out metaphorical uses of language from the RTE datasets (Zaenen et al., 2005), some metaphorical texts have eluded the annotators’ selection policies (Bos and Markert, 2006). Although the sample of metaphor pairs is fairly small, table 1 shows that there is a trend for the accuracy to be significantly lower when metaphor is involved than for the overall results (which agrees with Bos and Markert’s (2006) diagnostic).","Metaphor in RTE Challenges Even though annotators aimed to filter out metaphorical uses of language from the RTE datasets REF, some metaphorical texts have eluded the annotators’ selection policies TREF. Although the sample of metaphor pairs is fairly small, table 1 shows that there is a trend for the accuracy to be significantly lower when metaphor is involved than for the overall results (which agrees with TREF diagnostic)."
A,H05-1079,D10-1074,0,"Different approaches have been developed, for example, based on logic proving (Tatu and Moldovan, 2005; Bos and Markert, 2005; Raina et al., 2005) and graph match (Haghighi et al., 2005; de Salvo Braz et al., 2005; MacCartney et al., 2006).","Different approaches have been developed, for example, based on logic proving (GTREF) and graph match REF."
A,H05-1079,J07-3004,0,"Furthermore, Bos et al. (2004) and Bos (2005) have demonstrated that the output of CCGbank parsers can be successfully translated into Kamp and Reyle’s (1993) Discourse Representation Theory structures, to support question answering and the textual entailment task (Bos and Markert 2005).","Furthermore, REF and REF have demonstrated that the output of CCGbank parsers can be successfully translated into REF Discourse Representation Theory structures, to support question answering and the textual entailment task TREF."
A,H05-1079,N06-1005,2,"The RTE problem as presented in the PASCAL RTE dataset is particularly attractive in that it is a reasonably simple task for human annotators with high inter-annotator agreement (95.1% in one independent labeling (Bos and Markert, 2005)), but an extremely challenging task for automated systems. For example, two high-accuracy systems are those described in (Tatu and Moldovan, 2005), achieving 60.4% accuracy with no task-specific information, and (Bos and Markert, 2005), which achieves 61.2% task-dependent accuracy, i.e. when able to use the specific task labels as input. Many previous approaches have used a logical form representation of the text and hypothesis sentences, focusing on deriving a proof by which one can infer the hypothesis logical form from the text logical form (Bayer et al., 2005; Bos and Markert, 2005; Raina et al., 2005; Tatu and Moldovan, 2005). Attempts have been made to remedy this deficit through various techniques, including modelbuilding (Bos and Markert, 2005) and the addition of semantic axioms (Tatu and Moldovan, 2005).","The RTE problem as presented in the PASCAL RTE dataset is particularly attractive in that it is a reasonably simple task for human annotators with high inter-annotator agreement (95.1% in one independent labeling TREF), but an extremely challenging task for automated systems. For example, two high-accuracy systems are those described in REF, achieving 60.4% accuracy with no task-specific information, and TREF, which achieves 61.2% task-dependent accuracy, i.e. when able to use the specific task labels as input. Many previous approaches have used a logical form representation of the text and hypothesis sentences, focusing on deriving a proof by which one can infer the hypothesis logical form from the text logical form (GTREF). Attempts have been made to remedy this deficit through various techniques, including modelbuilding TREF and the addition of semantic axioms REF."
A,H05-1079,N06-1006,1,"Finally, a few efforts (Akhmatova, 2005; Fowler et al., 2005; Bos and Markert, 2005) have tried to translate sentences into formulas of first-order logic, in order to test logical entailment with a theorem prover. We show comparable results from recent systems based on lexical similarity (Jijkoun and de Rijke, 2005), graph alignment (Haghighi et al., 2005), weighted abduction (Raina et al., 2005), and a mixed system including theorem proving (Bos and Markert, 2005).","Finally, a few efforts (GTREF) have tried to translate sentences into formulas of first-order logic, in order to test logical entailment with a theorem prover. We show comparable results from recent systems based on lexical similarity REF, graph alignment REF, weighted abduction REF, and a mixed system including theorem proving TREF."
A,H05-1079,N10-1146,0,"Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions.","Early deep semantic models (e.g., REF) as well as more recent ones (e.g., (GTREF)) rely on specific world knowledge encoded in rules for drawing decisions."
A,H05-1079,P06-1051,0,"These transformations are logical rules in (Bos and Markert, 2005) or sequences of allowed rewrite rules in (de Salvo Braz et al., 2005).",These transformations are logical rules in TREF or sequences of allowed rewrite rules in REF.
A,H05-1079,P06-2105,0,"(Bos and Markert, 2005) represents and into a first-order logic translation of the DRS language used in Discourse Representation Theory (Kamp and Reyle, 1993) and uses a theorem prover and a model builder with some generic, lexical and geographical background knowledge to prove the entailment between the two texts.","TREF represents and into a first-order logic translation of the DRS language used in Discourse Representation Theory REF and uses a theorem prover and a model builder with some generic, lexical and geographical background knowledge to prove the entailment between the two texts."
A,H05-1079,P11-1059,0,"Some NLP applications deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005).","Some NLP applications deal indirectly with negation, e.g., machine translation REF, text classification REF and recognizing entailments TREF."
A,H05-1079,P12-3013,0,"NutCracker3 (Bos and Markert, 2005) is a system based on logical representation and automatic theorem proving, but utilizes only WordNet (Fellbaum, 1998) as a lexical knowledge resource.","NutCracker3 TREF is a system based on logical representation and automatic theorem proving, but utilizes only WordNet REF as a lexical knowledge resource."
A,H05-1079,Q13-1015,0,"Conversely, computational models of formal semantics have shown low recall on practical applications, stemming from their reliance on ontologies such as WordNet (Miller, 1995) to model the meanings of content words (Bobrow et al., 2007; Bos and Markert, 2005)","Conversely, computational models of formal semantics have shown low recall on practical applications, stemming from their reliance on ontologies such as WordNet REF to model the meanings of content words (GTREF)"
A,H05-1079,S12-1082,0,"Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005)","Related methods incorporate measurements of similarity at various levels: lexical REF, syntactic REF, and semantic (GTREF)"
A,H05-1079,S12-1108,0,"Logic-based approach is to map the language expressions to logical meaning representations, and then rely on logical entailment checks, possibly by invoking theorem provers (Rinaldi et al., 2003; Bos & Markert, 2005; Tatu & Moldovan, 2005, 2007).","Logic-based approach is to map the language expressions to logical meaning representations, and then rely on logical entailment checks, possibly by invoking theorem provers ( GTREF)."
A,H05-1079,S13-1002,3,"Of existing RTE approaches, the closest to ours is by Bos and Markert (2005), who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover. This approach can be viewed as a bridge between Bos and Markert (2005)’s purely logical approach, which relies purely on hard logical rules and theorem proving, and distributional approaches, which support graded similarity between concepts but have no notion of logical operators or entailment. Task 1: Recognizing Textual Entailment 4.1 Dataset In order to compare directly to the logic-based system of Bos and Markert (2005), we focus on the RTE-1 dataset (Dagan et al., 2005), which includes 567 Text-Hypothesis (T-H) pairs in the development set and 800 pairs in the test set. For the RTE task, systems were evaluated using both accuracy and confidence-weighted score (cws) as used by Bos and Markert (2005) and the RTE- 1 challenge (Dagan et al., 2005). In particular, they perform worse than strict entailment from Bos and Markert (2005), a system that uses only logic.","Of existing RTE approaches, the closest to ours is by TREF, who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover. This approach can be viewed as a bridge between TREF’s purely logical approach, which relies purely on hard logical rules and theorem proving, and distributional approaches, which support graded similarity between concepts but have no notion of logical operators or entailment. Task 1: Recognizing Textual Entailment 4.1 Dataset In order to compare directly to the logic-based system of TREF, we focus on the RTE-1 dataset REF, which includes 567 Text-Hypothesis (T-H) pairs in the development set and 800 pairs in the test set. For the RTE task, systems were evaluated using both accuracy and confidence-weighted score (cws) as used by TREF and the RTE- 1 challenge REF. In particular, they perform worse than strict entailment from TREF, a system that uses only logic."
A,H05-1079,S13-1014,0,"Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005).","Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical REF, syntactic REF, and semantic ( GTREF)."
A,H05-1079,W06-1621,0,"Interestingly, a number of works (e.g. (Bos and Markert, 2005; Corley and Mihalcea, 2005; Jijkoun and de Rijke, 2005; Glickman et al., 2006)) applied or utilized lexical based word overlap measures.","Interestingly, a number of works (e.g. (GTREF)) applied or utilized lexical based word overlap measures."
A,H05-1079,W08-2004,0,"Several models, ranging from the simple lexical similarity between T and H to advanced Logic Form Representations, have been proposed (Corley and Mihalcea, 2005; Glickman and Dagan, 2004; de Salvo Braz et al., 2005; Bos and Markert, 2005).","Several models, ranging from the simple lexical similarity between T and H to advanced Logic Form Representations, have been proposed (GREF)."
A,H05-1079,W11-0112,0,"They, therefore, tend to have high precision at the cost of low recall (Bos and Markert, 2005). Bos and Markert (2005) describe a system for Recognizing Textual Entailment (RTE) that uses Boxer to convert both the premise and hypothesis of an RTE pair into first-order logical semantic representations and then uses a theorem prover to check for logical entailment. Note that when running an example in the theorem prover, weights are not possible, so any rule that would be weighted in an MLN is simply treated as a “hard clause” following Bos and Markert (2005).","They, therefore, tend to have high precision at the cost of low recall TREF. TREF describe a system for Recognizing Textual Entailment (RTE) that uses Boxer to convert both the premise and hypothesis of an RTE pair into first-order logical semantic representations and then uses a theorem prover to check for logical entailment. Note that when running an example in the theorem prover, weights are not possible, so any rule that would be weighted in an MLN is simply treated as a “hard clause” following TREF."
A,I05-2038,C10-1076,1,"For preprocessing, all the sentences in the Bioscope corpus are tokenized and then parsed using the Berkeley parser2 (Petrov and Klein, 2007) trained on the GENIA TreeBank (GTB) 1.0 (Tateisi et al., 2005)3 , which is a bracketed corpus in (almost) PTB style.","For preprocessing, all the sentences in the Bioscope corpus are tokenized and then parsed using the Berkeley parser2 REF trained on the GENIA TreeBank (GTB) 1.0 TREF3 , which is a bracketed corpus in (almost) PTB style."
A,I05-2038,C10-1088,1,"The BioNLP’09 shared task involved documents contained also in the GENIA treebank (Tateisi et al., 2005), creating an opportunity for direct study of intrinsic and task-oriented evaluation results. GENIA treebank processing For comparison and evaluation, the texts in the GENIA treebank (Tateisi et al., 2005) are converted to the various formats as follows.","The BioNLP’09 shared task involved documents contained also in the GENIA treebank TREF, creating an opportunity for direct study of intrinsic and task-oriented evaluation results. GENIA treebank processing For comparison and evaluation, the texts in the GENIA treebank TREF are converted to the various formats as follows."
A,I05-2038,D09-1157,1,"We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al., 2005) for biomedical text;","We used the Stanford parser REF, and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank TREF for biomedical text;"
A,I05-2038,N10-1004,1,"Our biomedical data comes from the GENIA treebank8 (Tateisi et al., 2005), a corpus of abstracts from the Medline database.9","Our biomedical data comes from the GENIA treebank8 TREF, a corpus of abstracts from the Medline database.9"
A,I05-2038,P06-1128,1,"Because our target is biomedical texts, we re-trained a parser (Hara et al., 2005) with the GENIA treebank (Tateisi et al., 2005), and also applied a bidirectional part-ofspeech tagger (Tsuruoka and Tsujii, 2005) trained with the GENIA treebank as a preprocessor.","Because our target is biomedical texts, we re-trained a parser REF with the GENIA treebank TREF, and also applied a bidirectional part-ofspeech tagger REF trained with the GENIA treebank as a preprocessor."
A,I05-2038,P06-4005,2,"The GENIA treebank (Tateisi et al., 2005) consists of 500 abstracts (4,446 sentences) extracted from MEDLINE. Automatically Annotated Corpus First, we applied a POS analyzer and then Enju. The POS analyzer and HPSG parser are trained by using the GENIA corpus (Tsuruoka et al., 2005; Hara et al., 2005), which comprises around 2,000 MEDLINE abstracts annotated with POS and Penn Treebank style syntactic parse trees (Tateisi et al., 2005).","The GENIA treebank TREF consists of 500 abstracts (4,446 sentences) extracted from MEDLINE. Automatically Annotated Corpus First, we applied a POS analyzer and then Enju. The POS analyzer and HPSG parser are trained by using the GENIA corpus (GREF), which comprises around 2,000 MEDLINE abstracts annotated with POS and Penn Treebank style syntactic parse trees TREF."
A,J02-3001,C04-1018,0,Automatic identification of sources has also been addressed indirectly by Gildea and Jurafsky’s (2002) work on semantic role identification in that finding sources often corresponds to finding the filler of the agent role for verbs.,Automatic identification of sources has also been addressed indirectly by TREF work on semantic role identification in that finding sources often corresponds to finding the filler of the agent role for verbs.
A,J02-3001,D08-1094,0,"Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002).","Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation REF, word sense disambiguation REF and semantic role labeling TREF."
A,J02-3001,E03-1040,0,"Because verbs play a central role in the syntactic and semantic interpretation of a sentence, much research has focused on automatically learning properties of verbs from text corpora, such as their subcategorization (Brent, 1993; Briscoe and Carroll, 1997), argument roles (Riloff and Schmelzenbach, 1998; Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (Dorr and Jones, 1996; Lapata and Brew, 1999; Schulte im Walde, 2000; Merlo and Stevenson, 2001).","Because verbs play a central role in the syntactic and semantic interpretation of a sentence, much research has focused on automatically learning properties of verbs from text corpora, such as their subcategorization REF, argument roles ( GTREF), selectional preferences REF, and lexical semantic classification REF."
A,J02-3001,P04-1043,1,"Several machine learning approaches for argument identification and classification have been developed (Gildea and Jurasfky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003). These standard features, firstly proposed in (Gildea and Jurasfky, 2002), refer to a flat information derived from parse trees, i.e. Phrase Type, Predicate Word, Head Word, Governing Category, Position and Voice. This latter choice allows us to compare the results with previous literature works, e.g. (Gildea and Jurasfky, 2002; Surdeanu et al., 2003; Hacioglu et al., 2003).","Several machine learning approaches for argument identification and classification have been developed (GTREF). These standard features, firstly proposed in TREF, refer to a flat information derived from parse trees, i.e. Phrase Type, Predicate Word, Head Word, Governing Category, Position and Voice. This latter choice allows us to compare the results with previous literature works, e.g. (GTREF)."
A,J02-3001,P07-1071,2,"Many current solutions are complicated, consist of several stages and handbuilt features, and are too slow to be applied as part of real applications that require such semantic labels, partly because of their use of a syntactic parser (Pradhan et al., 2004; Gildea and Jurafsky, 2002). In (Gildea and Jurafsky, 2002) the authors presented a statistical approach to learning (for FrameNet), with some success. Argument Classification Accuracy So far we have not used the same accuracy measures as in previous work (Gildea and Jurafsky, 2002; Pradhan et al., 2004). For example, simply adding whether each word is part of a noun or verb phrase using the handannotated parse tree (the so-called “GOV” feature from (Gildea and Jurafsky, 2002)) improves the performance of our system from 83.95% to 85.8%.","Many current solutions are complicated, consist of several stages and handbuilt features, and are too slow to be applied as part of real applications that require such semantic labels, partly because of their use of a syntactic parser ( GTREF). In TREF the authors presented a statistical approach to learning (for FrameNet), with some success. Argument Classification Accuracy So far we have not used the same accuracy measures as in previous work (GTREF). For example, simply adding whether each word is part of a noun or verb phrase using the handannotated parse tree (the so-called “GOV” feature from TREF) improves the performance of our system from 83.95% to 85.8%."
A,J96-1002,A00-1019,0,"Techniques for weakening the independence assumptions made by the IBM models 1 and 2 have been proposed in recent work (Brown et al., 1993; Berger et al., 1996; Och and Weber, 98; Wang and Waibel, 98; Wu and Wong, 98).",Techniques for weakening the independence assumptions made by the IBM models 1 and 2 have been proposed in recent work (GTREF).
A,J96-1002,A00-2026,2,"The only trainable approaches (known to the author) to surface generation are the purely statistical machine translation (MT) systems such as (Berger et al., 1996) and the corpus-based generation system described in (Langkilde and Knight, 1998). The MT systems of (Berger et al., 1996) learn to generate text in the target language straight from the source language, without the aid of an explicit semantic representation. The form of the maximum entropy probability model is identical to the one used in (Berger et al., 1996; Ratnaparkhi, 1998): where wi ranges over V t3 .stop. and {wi-l,wi-2,attri} is the history, where wi denotes the ith word in the phrase, and attri denotes the attributes that remain to be generated at position i in the phrase. e features used in NLG2 are described in the next section, and the feature weights aj, obtained from the Improved Iterative Scaling algorithm (Berger et al., 1996), are set to maximize the likelihood of the training data. Our approach differs from the corpus-based surface generation approaches of (Langkilde and Knight, 1998) and (Berger et al., 1996). (Berger et ai., 1996) describes a statistical machine translation approach that generates text in the target language directly from the source text.","The only trainable approaches (known to the author) to surface generation are the purely statistical machine translation (MT) systems such as TREF and the corpus-based generation system described in REF. The MT systems of TREF learn to generate text in the target language straight from the source language, without the aid of an explicit semantic representation. The form of the maximum entropy probability model is identical to the one used in (GTREF): where wi ranges over V t3 .stop. and {wi-l,wi-2,attri} is the history, where wi denotes the ith word in the phrase, and attri denotes the attributes that remain to be generated at position i in the phrase. e features used in NLG2 are described in the next section, and the feature weights aj, obtained from the Improved Iterative Scaling algorithm TREF, are set to maximize the likelihood of the training data. Our approach differs from the corpus-based surface generation approaches of REF and TREF. TREF describes a statistical machine translation approach that generates text in the target language directly from the source text."
A,J96-1002,A00-2031,0,"This is concordant with the usage in the maximum entropy literature (Berger et al., 1996).",This is concordant with the usage in the maximum entropy literature TREF.
A,J96-1002,A97-1056,0,"Because their joint distributions have such closed-form expressions, the parameters can be estimated directly from the training data without the need for an iterative fitting procedure (as is required, for example, to estimate the parameters of maximum entropy models; (Berger et al., 1996)). Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., (Berger et al., 1996)), but within this framework no systematic study of interactions has been proposed.","Because their joint distributions have such closed-form expressions, the parameters can be estimated directly from the training data without the need for an iterative fitting procedure (as is required, for example, to estimate the parameters of maximum entropy models; TREF. Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., TREF), but within this framework no systematic study of interactions has been proposed."
A,J96-1002,C00-1060,0,"We report that our parsing framework achieved high accuracy (88.6%) in dependency analysis of Japanese with a combination of an underspecified HPSG-based Japanese grammar, SLUNG (Mitsuishi et al., 1998) and the maximum entropy method (Berger et al., 1996).","We report that our parsing framework achieved high accuracy (88.6%) in dependency analysis of Japanese with a combination of an underspecified HPSG-based Japanese grammar, SLUNG REF and the maximum entropy method TREF."
A,J96-1002,C00-1061,0,"(Tae-il Kim, 2000) use up to ve phonemes in feature function(Berger et al., 1996).",REF use up to _x005fve phonemes in feature function TREF.
A,J96-1002,C00-1064,1,"Thus, a lot of alignment techniques have been suggested at the sentence (Gale et al., 1993), phrase (Shin et al., 1996), noun phrase (Kupiec, 1993), word (Brown et al., 1993; Berger et al., 1996; Melamed, 1997), collocation (Smadja et al., 1996) and terminology level. Recently, many approaches based on the maximum entropy model have been applied to natural language processing (Berger et al., 1994; Berger et al., 1996; Pietra et al., 1997). We referred to the studies of (Berger et al., 1996; Pietra et al., 1997).","Thus, a lot of alignment techniques have been suggested at the sentence REF, phrase REF, noun phrase REF, word (GTREF), collocation REF and terminology level. Recently, many approaches based on the maximum entropy model have been applied to natural language processing (GTREF). We referred to the studies of (GTREF)."
A,J96-1002,C00-2124,0,"For every class the weights of the active features are combined and the best scoring class is chosen (Berger et al., 1996).",For every class the weights of the active features are combined and the best scoring class is chosen TREF.
A,J96-1002,C02-1064,1,"We implemented these models within an maximum entropy framework (Berger et al., 1996; Ristad, 1997; Ristad, 1998).",We implemented these models within an maximum entropy framework (GTREF).
A,J96-1002,D07-1019,1,"Specifically we use the maximum entropy model (Berger et al., 1996) for this task: = exp?, =1 exp( (𝑐, )?=1 𝑐 ) (2) where exp( (𝑐, 𝑞) =1 𝑐 ) is the normalization factor; , is a feature function defined over query q and correction candidate c , while is the corresponding feature weight.","Specifically we use the maximum entropy model TREF for this task: = exp?, =1 exp( (𝑐, )?=1 𝑐 ) (2) where exp( (𝑐, 𝑞) =1 𝑐 ) is the normalization factor; , is a feature function defined over query q and correction candidate c , while is the corresponding feature weight."
A,J96-1002,D07-1051,1,"Classifier and Features For our AL framework we decided to employ a Maximum Entropy (ME) classifier (Berger et al., 1996).",Classifier and Features For our AL framework we decided to employ a Maximum Entropy (ME) classifier TREF.
A,J96-1002,C10-3004,0,"A maximum entropy model (Berger et al., 1996) is adopted here.",A maximum entropy model TREF is adopted here.
A,J96-1002,P05-1020,0,"Learning algorithms. We consider three learning algorithms, namely, the C4.5 decision tree induction system (Quinlan, 1993), the RIPPER rule learning algorithm (Cohen, 1995), and maximum entropy classification (Berger et al., 1996).","Learning algorithms. We consider three learning algorithms, namely, the C4.5 decision tree induction system REF, the RIPPER rule learning algorithm REF, and maximum entropy classification TREF."
A,J96-1002,P10-1142,0,"In recent years, statistical learners such as maximum entropy models (Berger et al., 1996), voted perceptrons (Freund and Schapire, 1999),and support vector machines (Joachims, 1999) have been increasingly used, in part due to their ability to provide a confidence value (e.g., in the form of a probability) associated with a classification, and in part due to the fact that they can be easily adapted to train recently proposed rankingbased coreference models (see Section 3.3).","In recent years, statistical learners such as maximum entropy models TREF, voted perceptrons REF,and support vector machines REF have been increasingly used, in part due to their ability to provide a confidence value (e.g., in the form of a probability) associated with a classification, and in part due to the fact that they can be easily adapted to train recently proposed rankingbased coreference models (see Section 3.3)."
A,J96-1002,P98-2214,2,"As a model learning method, we adopt the maximum entropy model learning method (Della  Pietra et al., 1997; Berger et al., 1996).","As a model learning method, we adopt the maximum entropy model learning method (GTREF)."
A,J96-1002,P99-1069,0,"Berger et al. (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument.","TREF and REF make this same point and arrive at the same estimator, albeit through a maximum entropy argument."
A,J96-1002,P02-1002,0,"Conditional Maximum Entropy models have been used for a variety of natural language tasks, including Language Modeling (Rosenfeld, 1994), partof-speech tagging, prepositional phrase attachment, and parsing (Ratnaparkhi, 1998), word selection for machine translation (Berger et al., 1996), and finding sentence boundaries (Reynar and Ratnaparkhi, 1997).","Conditional Maximum Entropy models have been used for a variety of natural language tasks, including Language Modeling REF, partof-speech tagging, prepositional phrase attachment, and parsing REF, word selection for machine translation TREF, and finding sentence boundaries REF."
A,J96-1002,P05-1066,0,"A number of other researchers (Berger et al., 1996; Niessen and Ney, 2004; Xia and McCord, 2004) have described previous work on preprocessing methods. (Berger et al., 1996) describe an approach that targets translation of French phrases of the form NOUN de NOUN (e.g., conflit d’interet ).","A number of other researchers (GTREF) have described previous work on preprocessing methods. (TREF) describe an approach that targets translation of French phrases of the form NOUN de NOUN (e.g., conflit d’interet )."
A,J96-1002,W00-0707,1,"In previous work (Foster, 2000), I described a Maximum Entropy/Minimum Divergence (MEMD) model (Berger et al., 1996) for p(w[hi, s) which incorporates a trigram language model and a translation component which is an analog of the well-known IBM translation model 1 (Brown et al., 1993). For a given choice of q and f, the IIS algorithm (Berger et al., 1996) can be used to find maximum likelihood values for the parameters ~.","In previous work REF, I described a Maximum Entropy/Minimum Divergence (MEMD) model TREF for p(w[hi, s) which incorporates a trigram language model and a translation component which is an analog of the well-known IBM translation model 1 REF. For a given choice of q and f, the IIS algorithm TREF can be used to find maximum likelihood values for the parameters ~."
A,J96-1002,W02-0813,1,"Under the maximum entropy framework (Berger et al., 1996), evidence from different features can be combined with no assumptions of feature independence.","Under the maximum entropy framework TREF, evidence from different features can be combined with no assumptions of feature independence."
A,J96-1002,W96-0213,0,"Previous uses of this model include language modeling(Lau et al., 1993), machine translation(Berger et al., 1996), prepositional phrase attachment(Ratnaparkhi et al., 1994), and word morphology(Della Pietra et al., 1995).","Previous uses of this model include language modeling REF, machine translation TREF, prepositional phrase attachment REF, and word morphology REF."
A,N03-1028,C04-1081,0,"However, our implementation uses a quasi-Newton gradient-climber BFGS for optimization, which has been shown to converge much faster (Malouf, 2002; Sha and Pereira, 2003).","However, our implementation uses a quasi-Newton gradient-climber BFGS for optimization, which has been shown to converge much faster (GTREF)."
A,N03-1028,C08-1094,1,"We trained log linear models with the perceptron algorithm (Collins, 2002) using features similar to those used for NP chunking in Sha and Pereira (2003), including surrounding POStags (provided by a separately trained log linear POS-tagger) and surrounding words, up to 2 before and 2 after the current word position.","We trained log linear models with the perceptron algorithm REF using features similar to those used for NP chunking in TREF, including surrounding POStags (provided by a separately trained log linear POS-tagger) and surrounding words, up to 2 before and 2 after the current word position."
A,N03-1028,C08-1106,2,"As the representative problem in shallow parsing, noun phrase chunking has received much attention, with the development of standard evaluation datasets and with extensive comparisons among methods (McDonald 2005; Sha & Pereira 2003; Kudo & Matsumoto 2001). As for the task of shallow parsing, CRFs also outperform many other state-of-the-art models (Sha & Pereira 2003; McDonald et al. 2005).  We employ similar predicate sets defined in Sha & Pereira (2003). Since the CRF model is one of the successful models in sequential labeling tasks (Lafferty et al. 2001; Sha & Pereira 2003; McDonald et al. 2005), in this section, we also compare LDCRFs with CRFs. Nevertheless, since testing the significance of shallow parsers’ F-measures is tricky, individual labeling accuracy provides a more convenient basis for statistical significance tests (Sha & Pereira 2003). We observe that the L-BFGS optimizer is slightly faster than CG on LDCRFs (see Figure 3), which echoes the comparison between the L-BFGS and the CG optimizing technique on the CRF model (Sha & Pereira 2003).","As the representative problem in shallow parsing, noun phrase chunking has received much attention, with the development of standard evaluation datasets and with extensive comparisons among methods (GTREF). As for the task of shallow parsing, CRFs also outperform many other state-of-the-art models (GTREF).  We employ similar predicate sets defined in TREF. Since the CRF model is one of the successful models in sequential labeling tasks (GTREF), in this section, we also compare LDCRFs with CRFs. Nevertheless, since testing the significance of shallow parsers’ F-measures is tricky, individual labeling accuracy provides a more convenient basis for statistical significance tests TREF. We observe that the L-BFGS optimizer is slightly faster than CG on LDCRFs (see Figure 3), which echoes the comparison between the L-BFGS and the CG optimizing technique on the CRF model TREF."
A,N03-1028,C08-1113,0,"Although this non-concavity prevents efficient global maximization of equation (3), it still allows us to incorporate incomplete annotations using gradient ascent iterations (Sha and Pereira, 2003).","Although this non-concavity prevents efficient global maximization of equation (3), it still allows us to incorporate incomplete annotations using gradient ascent iterations TREF."
A,N03-1028,C10-1082,0,"CRFs have been successfully applied to many sequence labeling tasks (Sha and Pereira, 2003; McCallum and Li, 2003).",CRFs have been successfully applied to many sequence labeling tasks (GTREF).
A,N03-1028,P03-1064,0,"Many machine learning techniques have been successfully applied to chunking tasks, such as Regularized Winnow (Zhang et al., 2001), SVMs (Kudo and Matsumoto, 2001), CRFs (Sha and Pereira, 2003), Maximum Entropy Model (Collins, 2002), Memory Based Learning (Sang, 2002) and SNoW (Munoz ˜ et al., 1999).","Many machine learning techniques have been successfully applied to chunking tasks, such as Regularized Winnow REF, SVMs REF, CRFs (GTREF), Maximum Entropy Model REF, Memory Based Learning REF and SNoW REF."
A,N03-1028,P10-1038,1,"To address this difficulty, we use the forward-backward algorithm (Sha and Pereira, 2003; Culotta and McCallum, 2004) to estimate separately for each position the probability of a hyphen at that position.","To address this difficulty, we use the forward-backward algorithm (GTREF) to estimate separately for each position the probability of a hyphen at that position."
A,N03-1028,W05-0622,0,"CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003; Cohn et al., 2005), part-of-speech (PoS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of table data (Pinto et al., 2003), among other tasks.","CRFs have been applied with impressive empirical results to the tasks of named entity recognition REF, part-of-speech (PoS) tagging REF, noun phrase chunking TREF and extraction of table data REF, among other tasks."
A,N03-1028,W06-1655,0,"Relation to Previous Work There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (Finkel et al., 2005; Culotta et al., 2005; Sha and Pereira, 2003).","Relation to Previous Work There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (GTREF)."
A,N03-1028,W10-2923,0,"SVMHMMs and CRFs have been successfully applied to a range of sequential tagging tasks such as syllabification (Bartlett et al., 2009), chunk parsing (Sha and Pereira, 2003) and word segmentation (Zhao et al., 2006).","SVMHMMs and CRFs have been successfully applied to a range of sequential tagging tasks such as syllabification REF, chunk parsing TREF and word segmentation REF."
A,N04-4028,C04-1081,1,"Segment confidence is estimated using constrained forward-backward (Culotta and McCallum, 2004).",Segment confidence is estimated using constrained forward-backward TREF.
A,N04-4028,D07-1068,0,"It is calculated by constrained forwardbackward algorithm (Culotta and McCallum, 2004), and confident segments are added to the dictionary in order to improve segmentation accuracy.","It is calculated by constrained forwardbackward algorithm TREF, and confident segments are added to the dictionary in order to improve segmentation accuracy."
A,N04-4028,D08-1070,0,A similar approach was used by Culotta et al. in (2004) in order to associate confidence values with sequences of contiguous tokens identified by a CRF model as fields in an information extraction task.,A similar approach was used by TREF in order to associate confidence values with sequences of contiguous tokens identified by a CRF model as fields in an information extraction task.
A,N04-4028,D10-1095,0,"Related Work: Most previous work has focused on confidence estimation for an entire example or some fields of an entry (Culotta and McCallum, 2004) using CRFs.",Related Work: Most previous work has focused on confidence estimation for an entire example or some fields of an entry TREF using CRFs.
A,N04-4028,N06-1038,2,Culotta and McCallum (2004) describe the constrained forward-backward algorithm to efficiently estimate the conditional probability that a segment of text is correctly extracted by a CRF.,TREF describe the constrained forward-backward algorithm to efficiently estimate the conditional probability that a segment of text is correctly extracted by a CRF.
A,N04-4028,P08-1004,1,"To obtain the probability at each position of a linear-chain CRF, the constrained forward-backward technique described in (Culotta and McCallum, 2004) is used.","To obtain the probability at each position of a linear-chain CRF, the constrained forward-backward technique described in TREF is used."
A,N04-4028,P09-1015,0,"This is analogous to the task of estimating record confi- dence using field confidence scores in information extraction (Culotta and McCallum, 2004).",This is analogous to the task of estimating record confi- dence using field confidence scores in information extraction TREF.
A,N04-4028,P10-1038,1,"To address this difficulty, we use the forward-backward algorithm (Sha and Pereira, 2003; Culotta and McCallum, 2004) to estimate separately for each position the probability of a hyphen at that position.","To address this difficulty, we use the forward-backward algorithm (GTREF) to estimate separately for each position the probability of a hyphen at that position."
A,N04-4028,P11-1149,0,"Quality assessment of a learned model output was explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008).","Quality assessment of a learned model output was explored by many previous works (see REF for a survey), and applied to several NL processing tasks such as syntactic parsing REF, machine translation REF, speech REF, relation extraction REF, IE TREF, QA REF and dialog systems REF."
A,N06-1040,C08-5001,0,"Applications of this algorithm include k-best parsing (McDonald et al., 2005; Mohri and Roark, 2006) and machine translation (Chiang, 2007).",Applications of this algorithm include k-best parsing (GTREF) and machine translation REF.
A,N06-1040,C10-1007,0,"By focusing on weighted loss as opposed to arc frequency, the classifier discovers structural zeros (Mohri and Roark, 2006), events which could have been observed, but were not.","By focusing on weighted loss as opposed to arc frequency, the classifier discovers structural zeros TREF, events which could have been observed, but were not."
A,N06-1040,D08-1091,0,"In the present work, we exploit L1-regularization, though other techniques such as structural zeros (Mohri and Roark, 2006) could also potentially be used.","In the present work, we exploit L1-regularization, though other techniques such as structural zeros TREF could also potentially be used."
A,N06-1040,N07-1051,0,It is interesting to note that these grammars capture many of the “structural zeros” described by Mohri and Roark (2006) and pruning rules with probability below e −10 reduces the grammar size drastically without influencing parsing performance.,It is interesting to note that these grammars capture many of the “structural zeros” described by TREF and pruning rules with probability below e −10 reduces the grammar size drastically without influencing parsing performance.
A,P01-1048,N03-2018,1,"Although (Ang et al., 2002; Litman et al., 2001; Batliner et al., 2000) have hand-labeled naturally-occurring utterances in a variety of corpora for various emotions, then extracted acoustic, prosodic and lexical features and used machine-learning techniques to develop predictive models, little work to date has addressed emotion detection in computer-based educational settings.","Although (GTREF) have hand-labeled naturally-occurring utterances in a variety of corpora for various emotions, then extracted acoustic, prosodic and lexical features and used machine-learning techniques to develop predictive models, little work to date has addressed emotion detection in computer-based educational settings."
A,P01-1048,N04-1026,2,"As a result of this mismatch, recent work motivated by spoken dialogue applications has started to use naturally-occurring speech to train emotion predictors (Litman et al., 2001; Lee et al., 2001; Ang et al., 2002; Lee et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003), but often predicts emotions using only acoustic-prosodic features that would be automatically available to a dialogue system in real-time. Lexical information has been shown to improve speech-based emotion prediction in other domains (Litman et al., 2001; Lee et al., 2002; Ang et al., 2002; Batliner et al., 2003; Devillers et al., 2003; Shafran et al., 2003), so our first non-acoustic-prosodic feature represents the transcription3 of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn). The number of words and syllables in a turn provide alternative ways to quantify turn duration (Litman et al., 2001). Adding Context-Level Features Research in other domains (Litman et al., 2001; Batliner et al., 2003) has shown that features representing the di- alogue context can sometimes improve the accuracy of predicting negative user states, compared to the use of features computed from only the turn to be predicted.","As a result of this mismatch, recent work motivated by spoken dialogue applications has started to use naturally-occurring speech to train emotion predictors (GTREF), but often predicts emotions using only acoustic-prosodic features that would be automatically available to a dialogue system in real-time. Lexical information has been shown to improve speech-based emotion prediction in other domains (GTREF), so our first non-acoustic-prosodic feature represents the transcription3 of each student turn as a word occurrence vector (indicating the lexical items that are present in the turn). The number of words and syllables in a turn provide alternative ways to quantify turn duration TREF. Adding Context-Level Features Research in other domains (GTREF) has shown that features representing the di- alogue context can sometimes improve the accuracy of predicting negative user states, compared to the use of features computed from only the turn to be predicted."
A,P01-1048,W01-1610,1,"In other papers (Swerts et al., 2000; Hirschberg et al., 2001; Litman et al., 2001), we have already given some descriptive statistics on corrections and aware sites and we have been looking at methods to automatically predict these two utterance categories. These experiments show that corrections and aware sites can be classi ed as such automatically, with a considerable degree of accuracy (Litman et al., 2001; Hirschberg et al., 2001).","In other papers (GTREF), we have already given some descriptive statistics on corrections and aware sites and we have been looking at methods to automatically predict these two utterance categories. These experiments show that corrections and aware sites can be classi ed as such automatically, with a considerable degree of accuracy (GTREF)."
A,P01-1048,W03-0205,0,"Moreover, speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states (Ang et al., 2002; Batliner et al., 2000) and user responses to system errors (Litman et al., 2001) that are useful for triggering system adaptation.","Moreover, speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states GTREF and user responses to system errors TREF that are useful for triggering system adaptation."
A,P01-1048,W03-0409,0,"For more information on our tasks and features, see (Litman et al., 2000; Hirschberg et al., 2001; Litman et al., 2001).","For more information on our tasks and features, see (GTREF)."
A,P02-1031,P03-1002,3,"One such augmented parser, trained on data available from the PropBank project has been recently presented in (Gildea and Palmer, 2002). In this paper we describe a domain-independent IE paradigm that is based on predicate-argument structures identified automatically by two different methods: (1) the statistical method reported in (Gildea and Palmer, 2002); and (2) a new method based on inductive learning which obtains 17% higher Fscore over the first method when tested on the same data. Section 2 reports on the parser that produces predicate-argument labels and compares it against the parser introduced in (Gildea and Palmer, 2002). The Model In previous work using the PropBank corpus, (Gildea and Palmer, 2002) proposed a model predicting argument roles using the same statistical method as the one employed by (Gildea and Jurafsky, 2002) for predicting semantic roles based on the FrameNet corpus (Baker et al., 1998). To achieve high accuracy and resolve the data sparsity problem the method reported in (Gildea and Palmer, 2002; Gildea and Jurafsky, 2002) employed a backoff solution based on a lattice that combines the model features. For example, the backoff lattice in (Gildea and Palmer, 2002) consists of eight connected nodes for a five-feature set. Our model considers two sets of features: Feature Set 1 (FS1): features used in the work reported in (Gildea and Palmer, 2002) and (Gildea and Jurafsky, 2002) ; and Feature Set 2 (FS2): a novel set of features introduced in this paper. We also were interested in comparing the results of the decision-tree-based method against the results obtained by the statistical approach reported in (Gildea and Palmer, 2002). (Gildea and Palmer, 2002) report the results listed on the first line of Table 2. Because predicate lexical information is used for less than 5% of the branching decisions, the generated classifier scales better than the statistical method from (Gildea and Palmer, 2002) to unknown predicates.","One such augmented parser, trained on data available from the PropBank project has been recently presented in TREF. In this paper we describe a domain-independent IE paradigm that is based on predicate-argument structures identified automatically by two different methods: (1) the statistical method reported in TREF; and (2) a new method based on inductive learning which obtains 17% higher Fscore over the first method when tested on the same data. Section 2 reports on the parser that produces predicate-argument labels and compares it against the parser introduced in TREF. The Model In previous work using the PropBank corpus, TREF proposed a model predicting argument roles using the same statistical method as the one employed by REF for predicting semantic roles based on the FrameNet corpus REF. To achieve high accuracy and resolve the data sparsity problem the method reported in (GTREF) employed a backoff solution based on a lattice that combines the model features. For example, the backoff lattice in TREF consists of eight connected nodes for a five-feature set. Our model considers two sets of features: Feature Set 1 (FS1): features used in the work reported in TREF and REF ; and Feature Set 2 (FS2): a novel set of features introduced in this paper. We also were interested in comparing the results of the decision-tree-based method against the results obtained by the statistical approach reported in TREF. TREF report the results listed on the first line of Table 2. Because predicate lexical information is used for less than 5% of the branching decisions, the generated classifier scales better than the statistical method from TREF  to unknown predicates."
A,P02-1046,W04-2405,1,"In recent work, (Abney, 2002) shows that the independence assumption can be relaxed, and co-training is still effective under a weaker independence assumption. However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al., 2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations.","In recent work, TREF shows that the independence assumption can be relaxed, and co-training is still effective under a weaker independence assumption. However, as theoretically shown in TREF, and then empirically in REF, co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations."
A,P02-1053,P06-2079,1,"Polarity Classification There is a large body of work on classifying the polarity of a document (e.g., Pang et al. (2002), Turney (2002)), a sentence (e.g., Liu et al. (2003), Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Gamon et al. (2005)), a phrase (e.g., Wilson et al. (2005)), and a specific object (such as a product) mentioned in a document (e.g., Morinaga et al. (2002), Yi et al. (2003), Popescu and Etzioni (2005)).  Much work has been performed on learning to identify and classify polarity terms (i.e., terms expressing a positive sentiment (e.g., happy) or a negative sentiment (e.g., terrible)) and exploiting them to do polarity classification (e.g., Hatzivassiloglou and McKeown (1997), Turney (2002), Kim and Hovy (2004), Whitelaw et al. (2005), Esuli and Sebastiani (2005)). For instance, instead of representing the polarity of a term using a binary value, Mullen and Collier (2004) use Turney’s (2002) method to assign a real value to represent term polarity and introduce a variety of numerical features that are aggregate measures of the polarity values of terms selected from the document under consideration.","Polarity Classification There is a large body of work on classifying the polarity of a document (e.g., GTREF), a sentence (e.g., REF), a phrase (e.g., REF), and a specific object (such as a product) mentioned in a document (e.g., REF).  Much work has been performed on learning to identify and classify polarity terms (i.e., terms expressing a positive sentiment (e.g., happy) or a negative sentiment (e.g., terrible)) and exploiting them to do polarity classification (e.g., GTREF). For instance, instead of representing the polarity of a term using a binary value, REF use TREF method to assign a real value to represent term polarity and introduce a variety of numerical features that are aggregate measures of the polarity values of terms selected from the document under consideration."
A,P02-1053,P07-1056,0,"Introduction Sentiment detection and classification has received considerable attention recently (Pang et al., 2002; Turney, 2002; Goldberg and Zhu, 2004).",Introduction Sentiment detection and classification has received considerable attention recently (GTREF ).
A,P02-1057,P13-1151,0,"Similarly for summarization, systems that have employed language models trained only on unsummarized text (Banko et al., 2000; Daume and Marcu, 2002).","Similarly for summarization, systems that have employed language models trained only on unsummarized text (GTREF)."
A,P03-1002,P07-1075,0,Surdeanu et al. (2003) applied semantic parsing to capture the predicate-argument sentence structure.,TREF applied semantic parsing to capture the predicate-argument sentence structure.
A,P03-1002,P13-1058,0,"This basic kind of verb knowledge has been shown to be useful in many NLP tasks such as information extraction (Surdeanu et al., 2003; Venturi1 et al., 2009), parsing (Carroll et al., 1998; Carroll and Fang, 2004) and word sense disambiguation (Kohomban and Lee, 2005; McCarthy et al., 2007).","This basic kind of verb knowledge has been shown to be useful in many NLP tasks such as information extraction (GTREF), parsing REF and word sense disambiguation REF."
A,P03-1002,N04-1030,3,"Abstract In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others. In recent work, a number of researchers have cast this problem as a tagging problem and have applied various supervised machine learning techniques to it (Gildea and Jurafsky (2000, 2002); Blaheta and Charniak (2000); Gildea and Palmer (2002); Surdeanu et al. (2003); Gildea and Hockenmaier (2003); Chen and Rambow (2003); Fleischman and Hovy (2003); Hacioglu and Ward (2003); Thompson et al. (2003); Pradhan et al. (2003)). Named Entities in Constituents – Following Surdeanu et al. (2003), we tagged 7 named entities (PERSON, ORGANIZATION, LOCATION, PERCENT, MONEY, TIME, DATE) using IdentiFinder (Bikel et al., 1999) and added them as 7 binary features. Head Word POS – Surdeanu et al. (2003) showed that using the part of speech (POS) of the head word gave a significant performance boost to their system. The Surdeanu et al. System. Surdeanu et al. (2003) report results on two systems using a decision tree classifier. G&P system estimates the posterior probabilities using several different feature sets and interpolate the estimates, while Surdeanu et al. (2003) use a decision tree classifier.","Abstract In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of REF, TREF and others. In recent work, a number of researchers have cast this problem as a tagging problem and have applied various supervised machine learning techniques to it (GTREF). Named Entities in Constituents – Following TREF, we tagged 7 named entities (PERSON, ORGANIZATION, LOCATION, PERCENT, MONEY, TIME, DATE) using IdentiFinder REF and added them as 7 binary features. Head Word POS – TREF showed that using the part of speech (POS) of the head word gave a significant performance boost to their system. The Surdeanu et al. System. TREF report results on two systems using a decision tree classifier. G&P system estimates the posterior probabilities using several different feature sets and interpolate the estimates, while TREF use a decision tree classifier."
A,P03-1054,C04-1097,1,"However, the parsing results of Klein & Manning (2003) involving unlexicalized grammars suggest that gains may be limited.","However, the parsing results of TREF involving unlexicalized grammars suggest that gains may be limited."
A,P03-1054,C04-1097,0,"However, the parsing results of Klein & Manning (2003) involving unlexicalized grammars suggest that gains may be limited.","However, the parsing results of TREF involving unlexicalized grammars suggest that gains may be limited."
A,P03-1069,C04-1108,0,"Lapata (Lapata, 2003) proposes another approach to information ordering based on a probabilistic model that assumes the probability of any given sentence is determined by its adjacent sentence and learns constraints on sentence order from a corpus of domain specific texts.",Lapata TREF proposes another approach to information ordering based on a probabilistic model that assumes the probability of any given sentence is determined by its adjacent sentence and learns constraints on sentence order from a corpus of domain specific texts.
A,P03-1069,C10-2105,0,"Features proposed to create the appropriate order include publication date of document (Barzilay et al., 2002), content words (Lapata, 2003; Althaus et al., 2004), and syntactic role of words (Barzilay and Lapata, 2005).","Features proposed to create the appropriate order include publication date of document REF, content words (GTREF), and syntactic role of words REF."
A,P03-1069,C10-2170,0,"A simple ordering criterion is the chronological order of the events represented in the sentences, which is often augmented with other ordering criteria such as lexical overlap (Conroy et al., 2006), lexical cohesion (Barzilay et al., 2002) or syntactic features (Lapata 2003).","A simple ordering criterion is the chronological order of the events represented in the sentences, which is often augmented with other ordering criteria such as lexical overlap REF, lexical cohesion REF or syntactic features TREF."
A,P03-1069,D07-1009,2,"Most of the existing algorithms represent text structure as a linear sequence and are driven by local coherence constraints (Lapata, 2003; Karamanis et al., 2004; Okazaki et al., 2004; Barzilay and Lapata, 2005; Bollegala et al., 2006; Elsner and Charniak, 2007). To identify the exact location of the sentence within the chosen paragraph, local ordering methods such as (Lapata, 2003) could be used.  Features Features used in our experiments are inspired by previous work on corpus-based approaches for discourse analysis (Marcu and Echihabi, 2002; Lapata, 2003; Elsner et al., 2007). This measure was first introduced in the context of sentence ordering by Lapata (2003). Relative to other corpora used in text structuring research (Barzilay and Lee, 2004; Lapata, 2003; Karamanis et al., 2004), texts in our collection are long: an average article has 32.9 sentences, organized in 3.61 sections and 10.9 paragraphs. The degree of variability observed in this experiment is consistent with human performance on other text structuring tasks such as sentence ordering (Barzilay et al., 2002; Lapata, 2003). To make sentencelevel insertion decisions, we use a local model in line with previous sentence-ordering work (Lapata, 2003; Bollegala et al., 2006).","Most of the existing algorithms represent text structure as a linear sequence and are driven by local coherence constraints (GTREF).  To identify the exact location of the sentence within the chosen paragraph, local ordering methods such as TREF could be used. Features Features used in our experiments are inspired by previous work on corpus-based approaches for discourse analysis (GTREF). This measure was first introduced in the context of sentence ordering by TREF. Relative to other corpora used in text structuring research (GTREF), texts in our collection are long: an average article has 32.9 sentences, organized in 3.61 sections and 10.9 paragraphs. The degree of variability observed in this experiment is consistent with human performance on other text structuring tasks such as sentence ordering (GTREF). To make sentencelevel insertion decisions, we use a local model in line with previous sentence-ordering work (GTREF)."
A,P03-1069,P04-1050,0,"In accordance with recent work in the emerging field of text-to-text generation (Barzilay et al., 2002; Lapata, 2003), we assume that the input to text structuring is a set of clauses.","In accordance with recent work in the emerging field of text-to-text generation (GTREF), we assume that the input to text structuring is a set of clauses."
A,P03-1069,P13-2016,2,"It is typically applicable in the text generation field, both for concept-to-text generation and text-totext generation (Lapata, 2003), such as multiple document summarization (MDS), question answering and so on. Related Work For works taking no use of source document, Lapata (2003) proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts. The probability model originates from (Lapata, 2003), and we implement the model with four features of lemmatized noun, verb, adjective or adverb, and verb and noun related dependency.","It is typically applicable in the text generation field, both for concept-to-text generation and text-totext generation TREF, such as multiple document summarization (MDS), question answering and so on. Related Work For works taking no use of source document, TREF proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts. The probability model originates from TREF, and we implement the model with four features of lemmatized noun, verb, adjective or adverb, and verb and noun related dependency."
A,P04-1015,C08-2012,0,"As to analysis of NPs, there have been a lot of work on statistical techniques for lexical dependency parsing of sentences (Collins and Roark, 2004; McDonald et al., 2005), and these techniques potentially can be used for analysis of NPs if appropriate resources for NPs are available.","As to analysis of NPs, there have been a lot of work on statistical techniques for lexical dependency parsing of sentences (GTREF), and these techniques potentially can be used for analysis of NPs if appropriate resources for NPs are available."
A,P04-1015,D07-1009,0,"We also view this as one of the key ideas of the incremental perceptron algorithm of (Collins and Roark, 2004), which searches through a complex decision space step-by-step and is immediately updated at the first wrong move.","We also view this as one of the key ideas of the incremental perceptron algorithm of TREF, which searches through a complex decision space step-by-step and is immediately updated at the first wrong move."
A,P04-1015,E06-1011,0,"Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment (Moore, 2005), sequence analysis (Daum´e and Marcu, 2005; McDonald et al., 2005a) and phrase-structure parsing (Collins and Roark, 2004).","Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment REF, sequence analysis REF and phrase-structure parsing TREF."
A,P04-1015,P05-1023,2,"In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004). For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq≥20) and several other statistical parsers (Collins, 1999; Collins and Duffy, 2002; Collins and Roark, 2004; Henderson, 2003; Charniak, 2000; Collins, 2000; Shen and Joshi, 2004; Shen et al., 2003; Henderson, 2004; Bod, 2003). When compared to other kernel methods, our approach performs better than those based on the Tree kernel (Collins and Duffy, 2002; Collins and Roark, 2004), and is only 0.2% worse than the best results achieved by a kernel method for parsing (Shen et al., 2003; Shen and Joshi, 2004).","In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (GTREF). For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq≥20) and several other statistical parsers (GTREF). When compared to other kernel methods, our approach performs better than those based on the Tree kernel (GTREF), and is only 0.2% worse than the best results achieved by a kernel method for parsing REF."
A,P04-1015,P06-1096,0,"Perceptron-based training To tune the parameters w of the model, we use the averaged perceptron algorithm (Collins, 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark, 2004; Roark et al., 2004).","Perceptron-based training To tune the parameters w of the model, we use the averaged perceptron algorithm TREF because of its efficiency and past success on various NLP tasks (GTREF)."
A,P04-1015,P11-1069,1,"Following recent work applying global discriminative models to large-scale structured prediction problems (Collins and Roark, 2004; Miyao and Tsujii, 2005; Clark and Curran, 2007; Finkel et al., 2008), we build our shift-reduce parser using a global linear model, and compare it with the chartbased C&C parser. Following Collins and Roark (2004), we apply the “early update” strategy to perceptron training: at any step during decoding, if neither the candidate output nor any item in the agenda is correct, decoding is stopped and the parameters are updated using the current highest scored item in the agenda or the candidate output, whichever has the higher score.","Following recent work applying global discriminative models to large-scale structured prediction problems (GTREF), we build our shift-reduce parser using a global linear model, and compare it with the chartbased C&C parser. Following TREF, we apply the “early update” strategy to perceptron training: at any step during decoding, if neither the candidate output nor any item in the agenda is correct, decoding is stopped and the parameters are updated using the current highest scored item in the agenda or the candidate output, whichever has the higher score."
A,P04-1015,J07-4004,0,"Another alternative for future work is to compare the dynamic programming approach taken here with the beam-search approach of Collins and Roark (2004), which allows more “global” features.","Another alternative for future work is to compare the dynamic programming approach taken here with the beam-search approach of TREF, which allows more “global” features."
A,P04-1054,C08-1088,1,"Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) to current research (Zhang et al., 2006; Zhou et al., 2007), kernel methods have been showing more and more potential in relation extraction. While kernel methods using the dependency tree (Culotta and Sorensen, 2004) and the shortest dependency path (Bunescu and Mooney, 2005) suffer from low recall performance, convolution tree kernels (Zhang et al., 2006; Zhou et al., 2007) over syntactic parse trees achieve comparable or even better performance than feature-based methods. Culotta and Sorensen (2004) proposed a slightly generalized version of this kernel between dependency trees, in which a successful match of two relation instances requires the nodes to be at the same layer and in the identical path starting from the roots to the current nodes. Similar to Culotta and Sorensen (2004), this method also suffers from high precision but low recall.","Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (GTREF) to current research REF, kernel methods have been showing more and more potential in relation extraction. While kernel methods using the dependency tree TREF and the shortest dependency path REF suffer from low recall performance, convolution tree kernels REF over syntactic parse trees achieve comparable or even better performance than feature-based methods. TREF proposed a slightly generalized version of this kernel between dependency trees, in which a successful match of two relation instances requires the nodes to be at the same layer and in the identical path starting from the roots to the current nodes. Similar to TREF, this method also suffers from high precision but low recall."
A,P04-1054,P06-1104,2,"In relation extraction, typical work on kernel methods includes: Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005). Related Work Many techniques on relation extraction, such as rule-based (MUC, 1987-1998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature. Culotta and Sorensen (2004) generalized it to estimate similarity between dependency trees. Therefore, although this kernel shows performance improvement over the previous one (Culotta and Sorensen, 2004), the constraint makes the two dependency kernels share the similar behavior: good precision but much lower recall on the ACE corpus. This may be due to two reasons: 1) the dependency tree (Culotta and Sorensen, 2004) and the shortest path (Bunescu and Mooney, 2005) lack the internal hierarchical phrase structure information, so their corresponding kernels can only carry out node-matching directly over the nodes with word tokens; 2) the parse tree kernel has less constraints. Compared with Previous Kernels: Since our method only counts the occurrence of each sub-tree without considering the layer and the ancestors of the root node of the sub-tree, our method is not limited by the constraints (identical layer and ancestors for the matchable nodes, as discussed in Section 2) in Culotta and Sorensen (2004).","In relation extraction, typical work on kernel methods includes: REF, TREF and REF. Related Work Many techniques on relation extraction, such as rule-based REF, feature-based REF and kernel-based (GTREF), have been proposed in the literature. TREF generalized it to estimate similarity between dependency trees. Therefore, although this kernel shows performance improvement over the previous one TREF, the constraint makes the two dependency kernels share the similar behavior: good precision but much lower recall on the ACE corpus. This may be due to two reasons: 1) the dependency tree TREF and the shortest path REF lack the internal hierarchical phrase structure information, so their corresponding kernels can only carry out node-matching directly over the nodes with word tokens; 2) the parse tree kernel has less constraints. Compared with Previous Kernels: Since our method only counts the occurrence of each sub-tree without considering the layer and the ancestors of the root node of the sub-tree, our method is not limited by the constraints (identical layer and ancestors for the matchable nodes, as discussed in Section 2) in TREF."
A,P04-1054,P08-2023,0,Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees.,TREF extended this work to estimate similarity between augmented dependency trees.
A,P06-1114,S12-1065,0,"During the last years, many authors have focused on resolving TE detection, as solutions to this problem have proved to be useful in many natural language processing tasks, such as question answering (Harabagiu and Hickl, 2006) or machine translation (MT) (Mirkin et al., 2009; Pado et al., 2009).","During the last years, many authors have focused on resolving TE detection, as solutions to this problem have proved to be useful in many natural language processing tasks, such as question answering TREF or machine translation (MT) REF."
A,P06-1114,C10-1087,0,"TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al., 2006) and question answering (Harabagiu and Hickl, 2006).","TE has been successfully applied to a variety of natural language processing applications, including information extraction REF and question answering TREF."
A,P06-1114,P08-1081,0,"Extensive research has been done in questionanswering, e.g. (Berger et al., 2000; Jeon et al., 2005; Cui et al., 2005; Harabagiu and Hickl, 2006; Dang et al., 2007).","Extensive research has been done in questionanswering, e.g. (GTREF)."
A,P07-1033,C08-1003,1,"In order to build models that perform well in new (target) domains we usually find two settings (Daume III, 2007). In the supervised setting, a recent paper by Daume III (2007) shows that, using a very simple ´ feature augmentation method coupled with Support Vector Machines, he is able to effectively use both labeled target and source data to provide the best results in a number of NLP tasks.","In order to build models that perform well in new (target) domains we usually find two settings TREF. In the supervised setting, a recent paper by TREF shows that, using a very simple ´ feature augmentation method coupled with Support Vector Machines, he is able to effectively use both labeled target and source data to provide the best results in a number of NLP tasks."
A,P07-1033,C10-1145,1,"To benefit from Wikipedia data, we introduce a domain adaption approach (Daumé III, 2007) which is suitable for this work since we have enough “target” domain data.","To benefit from Wikipedia data, we introduce a domain adaption approach TREF which is suitable for this work since we have enough “target” domain data."
A,P07-1033,J11-3002,1,"Inspired by recent work on domain adaptation, we tested whether the performance of the out-of-domain models can be improved when training includes a small amount of data from the target domain, by applying the method of Daume (2007).","Inspired by recent work on domain adaptation, we tested whether the performance of the out-of-domain models can be improved when training includes a small amount of data from the target domain, by applying the method of TREF."
A,P07-1033,P10-2056,3,"In this paper we investigate a recently proposed Bayesian adaptation approach (Daume III, 2007; Finkel and Manning, 2009) for adapting a conditional maximum entropy (ME) LM (Rosenfeld, 1996) to a new domain, given a large corpus of out-of-domain training data and a small corpus of in-domain data. Domain Adaptation of Maximum Entropy Models Recently, a hierarchical Bayesian adaptation method was proposed that can be applied to a large family of discriminative learning tasks (such as ME models, SVMs) (Daume III, 2007; Finkel and Manning, 2009). Discussion In this paper we have tested a hierarchical adaptation method (Daume III, 2007; Finkel and Manning, 2009) on building style-adapted LMs for speech recognition.","In this paper we investigate a recently proposed Bayesian adaptation approach (GTREF) for adapting a conditional maximum entropy (ME) LM REF to a new domain, given a large corpus of out-of-domain training data and a small corpus of in-domain data. Domain Adaptation of Maximum Entropy Models Recently, a hierarchical Bayesian adaptation method was proposed that can be applied to a large family of discriminative learning tasks (such as ME models, SVMs) (GTREF). Discussion In this paper we have tested a hierarchical adaptation method (GTREF) on building style-adapted LMs for speech recognition."
A,P07-1033,P13-2012,1,"For extra training data, we pool material from different datasets and use the multi-domain split feature space approach to learn dataset specific behaviors (Daume, 2007).","For extra training data, we pool material from different datasets and use the multi-domain split feature space approach to learn dataset specific behaviors TREF."
A,P07-1056,C08-1135,0,Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers using structural correspondence learning,TREF investigate domain adaptation for sentiment classifiers using structural correspondence learning
A,P07-1056,P08-2059,2,"We selected four binary NLP datasets for evaluation: 20 Newsgroups1 and Reuters (Lewis et al., 2004) (used by Tong and Koller) and sentiment classification (Blitzer et al., 2007) and spam (Bickel, 2006). For each dataset we extracted binary unigram features and sentiment was prepared according to Blitzer et al. (2007).",We selected four binary NLP datasets for evaluation: 20 Newsgroups1 and Reuters REF (used by Tong and Koller) and sentiment classification TREF and spam REF. For each dataset we extracted binary unigram features and sentiment was prepared according to TREF.
A,P07-1056,P08-2065,2,"It is worth noting that Blitzer et al. (2007) deal with the domain adaptation problem for sentiment classification where labeled data from one domain is used to train a classifier for classifying data from a different domain. As the training data from DVDs is much more similar to books than that from kitchen (Blitzer et al., 2007), we should give the data from DVDs a higher weight. Although the size of our training data are smaller than that reported in Blitzer et al.(2007) (70% vs. 80%), the classification performance is comparative to theirs.","It is worth noting that TREF deal with the domain adaptation problem for sentiment classification where labeled data from one domain is used to train a classifier for classifying data from a different domain. As the training data from DVDs is much more similar to books than that from kitchen TREF, we should give the data from DVDs a higher weight. Although the size of our training data are smaller than that reported in TREF (70% vs. 80%), the classification performance is comparative to theirs."
A,P07-1056,P11-1014,3,"Work in cross-domain sentiment classification (Blitzer et al., 2007) focuses on the challenge of training a classifier from one or more domains (source domains) and applying the trained classi- fier in a different domain (target domain).  Previous work on sentiment classification has shown that both unigrams and bigrams are useful for training a sentiment classifier (Blitzer et al., 2007). Experiments 5.1 Dataset To evaluate our method we use the cross-domain sentiment classification dataset prepared by Blitzer et al. (2007).  Effect of Feature Expansion To study the effect of feature expansion at train time compared to test time, we used Amazon reviews for two further domains, music and video, which were also collected by Blitzer et al. (2007) but are not part of the benchmark dataset. Blitzer et al. (2007) apply the structural correspondence learning (SCL) algorithm to train a crossdomain sentiment classifier.","Work in cross-domain sentiment classification TREF focuses on the challenge of training a classifier from one or more domains (source domains) and applying the trained classi- fier in a different domain (target domain).  Previous work on sentiment classification has shown that both unigrams and bigrams are useful for training a sentiment classifier TREF. Experiments 5.1 Dataset To evaluate our method we use the cross-domain sentiment classification dataset prepared by TREF. Effect of Feature Expansion To study the effect of feature expansion at train time compared to test time, we used Amazon reviews for two further domains, music and video, which were also collected by TREF but are not part of the benchmark dataset. TREF apply the structural correspondence learning (SCL) algorithm to train a crossdomain sentiment classifier."
A,P08-2026,P11-2049,0,"In addition, such an approach relies heavily on the availability of accurately parsed sentences, which could be problematic for domains such as biomedical texts (Clegg and Shepherd, 2007; McClosky and Charniak, 2008).","In addition, such an approach relies heavily on the availability of accurately parsed sentences, which could be problematic for domains such as biomedical texts (GTREF)."
A,P09-1113,D10-1099,2,"Here in place of annotated text, only an existing knowledge base (KB) is needed to train a relation extractor (Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). This room is not as large as in previous work (Mintz et al., 2009) where target text and training KB are closely related. First we follow Mintz et al. (2009), use Freebase as source of distant supervision, and employ Wikipedia as source of unlabelled text—we will call this an in-domain setting. We will follow (Mintz et al., 2009) and call the term R (c1, . . . cn) with c ∈ R a relation instance. Following previous work (Mintz et al., 2009; Zelenko et al., 2003; Culotta and Sorensen, 2004) we make one more simplifying assumption: every candidate tuple can be member of at most one relation. On the other end, we have methods that take relation mentions from several documents and use these as input features (Mintz et al., 2009; Bunescu and Mooney, 2007).  Mintz et al. (2009) refer to this as the distant supervision assumption. On inspection, we find that these preferences are often not satis- fied in a baseline distant supervision system akin to Mintz et al. (2009). The feature functions of this template are taken from Mintz et al. (2009). Our work is inspired by Mintz et al. (2009) who also use Freebase as distant supervision source. We also heuristically align our knowledge base to text by making the distant supervision assumption (Bunescu and Mooney, 2007; Mintz et al., 2009). Experimental Setup We follow Mintz et al. (2009) and perform two types of evaluation: held-out and manual. For each candidate tuple c with arity 2 and each of its mention tuples i we extract a set of features Xi c similar to those used in (Mintz et al., 2009): lexical, Part-Of-Speech (POS), named entity and syntactic features, i.e. features obtained from the dependency parsing tree of a sentence. As our baseline, and roughly equivalent to previous work (Mintz et al., 2009), we pick the templates TBias and TMen. This is similar to the setting of Mintz et al. (2009).","Here in place of annotated text, only an existing knowledge base (KB) is needed to train a relation extractor (GTREF). This room is not as large as in previous work TREF where target text and training KB are closely related. First we follow TREF, use Freebase as source of distant supervision, and employ Wikipedia as source of unlabelled text—we will call this an in-domain setting. We will follow TREF and call the term R (c1, . . . cn) with c ∈ R a relation instance. Following previous work (GTREF) we make one more simplifying assumption: every candidate tuple can be member of at most one relation. On the other end, we have methods that take relation mentions from several documents and use these as input features (GTREF).  TREF refer to this as the distant supervision assumption. On inspection, we find that these preferences are often not satis- fied in a baseline distant supervision system akin to TREF. The feature functions of this template are taken from TREF. Our work is inspired by TREF who also use Freebase as distant supervision source. We also heuristically align our knowledge base to text by making the distant supervision assumption (GTREF). Experimental Setup We follow TREF and perform two types of evaluation: held-out and manual. For each candidate tuple c with arity 2 and each of its mention tuples i we extract a set of features Xi c similar to those used in TREF: lexical, Part-Of-Speech (POS), named entity and syntactic features, i.e. features obtained from the dependency parsing tree of a sentence. As our baseline, and roughly equivalent to previous work TREF, we pick the templates TBias and TMen. This is similar to the setting of TREF."
A,P09-1113,P12-1076,3,"A particularly attractive approach, called distant supervision (DS), creates labeled data by heuristically aligning entities in text with those in a knowledge base, such as Freebase (Mintz et al., 2009). We applied our method to Wikipedia articles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS (Mintz et al., 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multiinstance learning system for relation extraction (see Section 7). Our work was inspired by Mintz et al. (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia. Dataset Following Mintz et al. (2009), we carried out our experiments using Wikipedia as the target corpus and Freebase (September, 2009, (Google, 2009)) as the knowledge base. Following Mintz et al. (2009), we performed an automatic held-out evaluation and a manual evaluation. Configuration of Classifiers Following Mintz et al. (2009), we used a multiclass logistic classifier optimized using L-BFGS with Gaussian regularization to classify entity pairs to the predefined 24 relations and NONE. We used syntactic features (i.e., features obtained from the dependency parse tree of a sentence) and lexical features, and entity types, which essentially correspond to the ones developed by Mintz et al. (2009).","A particularly attractive approach, called distant supervision (DS), creates labeled data by heuristically aligning entities in text with those in a knowledge base, such as Freebase TREF. We applied our method to Wikipedia articles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS TREF and MultiR REF, which is a state-of-the-art multiinstance learning system for relation extraction (see Section 7). Our work was inspired by TREF who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia. Dataset Following TREF, we carried out our experiments using Wikipedia as the target corpus and Freebase (September, 2009, REF) as the knowledge base. Following TREF, we performed an automatic held-out evaluation and a manual evaluation. Configuration of Classifiers Following TREF, we used a multiclass logistic classifier optimized using L-BFGS with Gaussian regularization to classify entity pairs to the predefined 24 relations and NONE. We used syntactic features (i.e., features obtained from the dependency parse tree of a sentence) and lexical features, and entity types, which essentially correspond to the ones developed by TREF."
A,P09-1113,N13-1008,2,"Here one aligns existing database records with the sentences in which these records have been “rendered”––effectively labeling the text—and from this labeling we can train a machine learning system as before (Craven and Kumlien, 1999; Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). Distant Supervision In Distant Supervision (DS) a set of facts from pre-existing structured sources is aligned with surface patterns mentioned in text (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), and this alignment is then used to train a relation extractor. Our first baseline is MI09, a distantly supervised classifier based on the work of Mintz et al. (2009).","Here one aligns existing database records with the sentences in which these records have been “rendered”––effectively labeling the text—and from this labeling we can train a machine learning system as before (GTREF). Distant Supervision In Distant Supervision (DS) a set of facts from pre-existing structured sources is aligned with surface patterns mentioned in text (GTREF), and this alignment is then used to train a relation extractor. Our first baseline is MI09, a distantly supervised classifier based on the work of TREF."
A,P11-2071,D12-1003,1,"Some of the previous methods tried to alleviate the problem of the limited seed lexicon size (Koehn and Knight, 2002; Morin and Prochasson, 2011; Hazem et al., 2011), while others did not require any seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daume III and Jagarlamudi, 2011).","Some of the previous methods tried to alleviate the problem of the limited seed lexicon size REF, while others did not require any seed lexicon (GTREF)."
A,P11-2071,D12-1025,0,"Some researchers have attempted to do this by adding a domain specific dictionary (Wu et al., 2008), or mining unseen words (Daume and Jagarlamudi, 2011) using one of several translation lexicon induction techniques (Haghighi et al., 2008; Koehn and Knight, 2002; Rapp, 1995).","Some researchers have attempted to do this by adding a domain specific dictionary REF, or mining unseen words TREF using one of several translation lexicon induction tech_x0002_niques REF."
A,P11-2071,D13-1109,2,Daume III and Jagarlamudi (2011) mine translations for high frequency OOV words in NEWdomain text in order to do domain adaptation.,TREF mine translations for high frequency OOV words in NEWdomain text in order to do domain adaptation.
A,P11-2071,P13-2072,0,"When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (Daume III and Jagarlamudi, 2011; Clark et al., 2012).","When a linear model does not fit well, researchers are careful to manually add important feature conjunctions, as for example, (GTREF)."
A,P94-1013,W95-0104,3,"Yarowsky [1994] has exploited this complementarity by combining the two methods using 
decision lists. The idea is to pool the evidence provided by the component methods, and to then 
solve a target problem by applying the single strongest piece of evidence, whatever type it happens 
to be.","TREF has exploited this complementarity by combining the two methods using 
decision lists. The idea is to pool the evidence provided by the component methods, and to then 
solve a target problem by applying the single strongest piece of evidence, whatever type it happens 
to be."
A,P95-1026,W99-0613,3,"The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision. The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98). (Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance. The approach builds from an initial seed set for a category, and is quite similar to the decision list approach described in (Yarowsky 95). Unsupervised Algorithms based on Decision Lists 3.1 Supervised Decision List Learning The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95). (Yarowsky 95) describes the use of more sophisticated smoothing methods. The Algorithm in (Yarowsky 95) We can now compare this algorithm to that of (Yarowsky 95). It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.","The first method uses a similar algorithm to that of TREF. Recent results (e.g., (GTREF) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision. The first method builds on results from TREF and REF. TREF describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance. The approach builds from an initial seed set for a category, and is quite similar to the decision list approach described in TREF. Unsupervised Algorithms based on Decision Lists 3.1 Supervised Decision List Learning The first unsupervised algorithm we describe is based on the decision list method from TREF. TREF describes the use of more sophisticated smoothing methods. The Algorithm in TREF We can now compare this algorithm to that of TREF. It was motivated by the observation that the TREF algorithm added a very large number of rules in the first few iterations."
A,P95-1026,P02-1046,3,"The plenitude of unlabeled natural language data, and the paucity of labeled data, have made bootstrapping a topic of interest in computational linguistics. Current work has been spurred by two papers, (Yarowsky, 1995) and (Blum and Mitchell, 1998).","The plenitude of unlabeled natural language data, and the paucity of labeled data, have made bootstrapping a topic of interest in computational linguistics. Current work has been spurred by two papers, TREF and REF."
A,P95-1026,J04-3004,3,The Yarowsky (1995) algorithm was one of the first bootstrapping algorithms to become widely known in computational linguistics.,The TREF algorithm was one of the first bootstrapping algorithms to become widely known in computational linguistics.
A,P95-1026,P01-1008,0,"This method of co-training has been previously applied to a variety of natural language tasks, such as word sense disambiguation (Yarowsky, 1995), lexicon construction for information extraction (Riloff and Jones, 1999), and named entity classification (Collins and Singer, 1999).","This method of co-training has been previously applied to a variety of natural language tasks, such as word sense disambiguation TREF, lexicon construction for information ex_x005f_x005f_x0002_traction REF, and named en_x0002_tity classification REF."
A,P95-1026,C08-1135,0,"Yarowsky (1995) describes a 'semi-unsupervised' approach to the problem of sense disambiguation of words, also using a set of initial seeds, in this case a few high quality sense annotations.","TREF describes a 'semi-unsupervised' approach to the problem of sense disambiguation of words, also using a set of initial seeds, in this case a few high quality sense annotations."
A,P95-1026,P07-1125,0,Self-training is an alternative single-view algorithm in which a labelled pool is incrementally enlarged with unlabelled samples for which the learner is most confident. Early work by Yarowsky (1995) falls within this framework.,Self-training is an alternative single-view algorithm in which a labelled pool is incrementally enlarged with unlabelled samples for which the learner is most confident. Early work by TREF falls within this framework.
A,P96-1025,A00-2015,1,"Previous works on statistical dependency analysis include Fujio and Matsumoto (1998) and Haruno et al. (1998) in Japanese analysis as well as Lafferty et al. (1992), Eisner (1996), and Collins (1996) in English analysis. TOur modeling is slightly different from those of other standard approaches to statistical dependency analysis (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998) which simply distinguish the two cases: the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold. The model (b) corresponds to a model in which standard approaches to statistical dependency analysis (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998) are applied to our task of deciding dependency between two subordinate clauses. Fujio and Matsumoto (1998)'s lexicalized dependency analyzer is similar to that of Collins (1996), where various features were evaluated through performance test and an optimal feature set was manually selected.","Previous works on statistical dependency analysis include REF and REF in Japanese analysis as well as REF, REF, and TREF in English analysis. TOur modeling is slightly different from those of other standard approaches to statistical dependency analysis (GTREF) which simply distinguish the two cases: the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold. The model (b) corresponds to a model in which standard approaches to statistical dependency analysis (GTREF) are applied to our task of deciding dependency between two subordinate clauses. REF's lexicalized dependency analyzer is similar to that of TREF, where various features were evaluated through performance test and an optimal feature set was manually selected."
A,P96-1025,A97-1004,0,"Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g. (Brill, 1994; Collins, 1996)).","Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g. (GTREF)."
A,P96-1025,C00-1017,0,"Our investigation of these models was motivated rather by our desire to obtain a generalizable result for these simple and well-understood models, since obtaining similar results for more sophisticated models (e.g. (Collins, 1996; Ratnaparkhi, 1997)) might have been attributed to special properties of these models.","Our investigation of these models was motivated rather by our desire to obtain a generalizable result for these simple and well-understood models, since obtaining similar results for more sophisticated models (e.g. (GTREF) might have been attributed to special properties of these models."
A,P96-1025,C00-1029,0,"Further work will look at how to integrate probabilities such as p(cjv; r) into a model of dependency structure, similar to that of Collins (1996) and Collins (1997), which can be used for parse selection.","Further work will look at how to integrate probabilities such as p(cjv; r) into a model of dependency structure, similar to that of TREF and REF, which can be used for parse selection."
A,P96-1025,C00-1043,1,"Semantic and Logic Transformations Semantic Transformations Instead of producing only a phrasal parse for the question and answer, we make use of one of the new statistical parsers of large real-world text coverage (Collins, 1996). The label propagation rules are identical to the rules for mapping from trees to dependency structures used my Michael Collins (cf. (Collins, 1996)).","Semantic and Logic Transformations Semantic Transformations Instead of producing only a phrasal parse for the question and answer, we make use of one of the new statistical parsers of large real-world text coverage TREF. The label propagation rules are identical to the rules for mapping from trees to dependency structures used my Michael Collins (cf. TREF)."
A,P96-1025,C00-1051,1,"Note that, while we restrict our discussion to analysis of Japanese sentences in this paper, what we present below should also be straightforwardly applicable to more wideranged tasks such as English dependency analysis just like the problem setting considered by Collins (1996). Estimation of DPs Some of the state-of-the-art probabilistic language models such as the bottomup models P (Rjs) proposed by Collins (1996) and Fujio et al. (1998) directly estimate DPs for a given input, whereas other models such as PCFGbased topdown generation models P (R; s) do not (Charniak, 1997; Collins, 1997; Shirai et al., 1998). CHAGAKE (Fujio et al., 1998): an extension of the bottom-up model proposed by Collins (Collins, 1996).","Note that, while we restrict our discussion to analysis of Japanese sentences in this paper, what we present below should also be straightforwardly applicable to more wideranged tasks such as English dependency analysis just like the problem setting considered by TREF. Estimation of DPs Some of the state-of-the-art probabilistic language models such as the bottomup models P (Rjs) proposed by TREF and REF directly estimate DPs for a given input, whereas other models such as PCFGbased topdown generation models P (R; s) do not (GTREF). CHAGAKE REF: an extension of the bottom-up model proposed by Collins TREF."
A,P96-1025,C00-2141,0,"As the development of corpus linguistics, many statistics-based parsers were proposed, such as Magerman(1995)’s statistical decision tree parser, Collins(1996)’s bigram dependency model parser, Ratnaparkhi(1997)’s maximum entropy model parser.","As the development of corpus linguistics, many statistics-based parsers were proposed, such as REF’s statistical decision tree parser, TREF’s bigram dependency model parser, REF’s maximum entropy model parser."
A,P96-1025,C00-2146,0,"(Charniak, 1995; Collins, 1996) use the lexical informationand (Magerman and Marcus, 1991; Magerman and Weir, 1992) use the contextual information for structural disambiguation.",(GTREF) use the lexical informationand REF use the contextual information for structural disambiguation.
A,P96-1025,C02-1112,1,"The features are extracted using a statistical parser (Collins, 1996), and consist of the head and modifiers of each phrase.","The features are extracted using a statistical parser TREF, and consist of the head and modifiers of each phrase."
A,P96-1025,E03-1051,0,"A typical current parser (e.g., statistical parsers such as (Collins, 1996; Ratnaparkhi, 1997; Charniak, 2000)) interleaves PP attachment with all its other disambiguation tasks.","A typical current parser (e.g., statistical parsers such as (GTREF) interleaves PP attachment with all its other disambiguation tasks."
A,P96-1025,E06-1012,2,"With the emergence of the important role of word-to-word relations in parsing (Charniak, 2000; Collins, 1996), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, Kudo and Matsumoto (2000; 2002), Sekine et al. (2000) for Japanese, Chung and Rim (2004) for Korean, Nivre et al. (2004) for Swedish, Nivre and Nilsson (2005) for Czech, among others. Collins (1996) employs this distance ∆i,H(i) in the computation of word-toword dependency probabilities P(dep (wi ,wH(i) )| S) ≈ (2) P(link(wi ,wH(i) )| ∆i,H(i) ) suggesting that distance is a crucial variable when deciding whether two words are related, along with other features such as intervening punctuation. This is a modified version of the backed-off smoothing used by Collins (1996) to alleviate sparse data problems.","With the emergence of the important role of word-to-word relations in parsing (GTREF), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, GTREF, REF for Japanese, REF for Korean, REF for Swedish, REF for Czech, among others. TREF employs this distance ∆i,H(i) in the computation of word-toword dependency probabilities P(dep (wi ,wH(i) )| S) ≈ (2) P(link(wi ,wH(i) )| ∆i,H(i) ) suggesting that distance is a crucial variable when deciding whether two words are related, along with other features such as intervening punctuation. This is a modified version of the backed-off smoothing used by TREF to alleviate sparse data problems."
A,P96-1025,E09-1097,2,"To calculate the edge weights, we adapt the definition of Collins (1996) to use direction rather than relation type (represented in the original as triples of non-terminals). We adopt the same smoothing strategy as Collins (1996), which backs off to PoS for unseen dependency events.","To calculate the edge weights, we adapt the definition of TREF to use direction rather than relation type (represented in the original as triples of non-terminals). We adopt the same smoothing strategy as TREF, which backs off to PoS for unseen dependency events."
A,P96-1025,E99-1025,0,"There have been two main robust parsing paradigms: Finite State Grammar-based approaches (such as Abney (1990), Grishman (1995), and Hobbs et al. (1997)) and Statistical Parsing (such as Charniak (1996), Magerman (1995), and Collins (1996)).","There have been two main robust parsing paradigms: Finite State Grammar-based approaches (such as REF, REF, and REF) and Statistical Parsing (such as REF, REF, and TREF)."
A,P96-1025,J06-2003,1,"In such cases, the program splits a sentence into coordinated clauses (or coordinated verb phrases) by using a parser (Collins 1996) to distinguish when a coordinating conjunction (and, but, whereas) is conjoining two main clauses or two parts of a complex verb phrase.","In such cases, the program splits a sentence into coordinated clauses (or coordinated verb phrases) by using a parser TREF to distinguish when a coordinating conjunction (and, but, whereas) is conjoining two main clauses or two parts of a complex verb phrase."
A,P96-1025,J08-3003,2,"A conditional probability model to score the analyses (Collins 1996). Maximum likelihood estimation to make inferences about the underlying probability models (Collins 1996; Chung and Rim 2004). For the probability model,we adopt the approach by Chung and Rim (2004),which itself is a modified version of the statistical model used in Collins (1996). The statistical model in Collins (1996) is actually used in a phrase-structure-based parsing approach,but it uses the same idea of computing probabilities between dependents and head units. We use a strategy similar to Collins (1996) and we interpolate with estimates based on less context: P(x|y) ≈ λ · P1(x|y) + (1 − λ) · P2(x) (3) where λ = δ/(δ + 1) and δ is the count of the x occurrences.","A conditional probability model to score the analyses TREF. Maximum likelihood estimation to make inferences about the underlying probability models (GTREF). For the probability model,we adopt the approach by REF,which itself is a modified version of the statistical model used in TREF. The statistical model in TREF is actually used in a phrase-structure-based parsing approach,but it uses the same idea of computing probabilities between dependents and head units. We use a strategy similar to TREF and we interpolate with estimates based on less context: P(x|y) ≈ λ · P1(x|y) + (1 − λ) · P2(x) (3) where λ = δ/(δ + 1) and δ is the count of the x occurrences."
A,P96-1025,J11-3004,3,"Collins (1996) One of the most straightforward projective dependency parsing strategies was introduced by Collins (1996), and is based on the CYK bottom–up parsing strategy (Kasami 1965; Younger 1967). COMBINER steps follow the same mechanism as those in the algorithm of Eisner (1996), and LINK steps work analogously to those of Collins (1996), so this schema can be seen as being intermediate between those two algorithms. By generalizing the linking steps in Eisner and Satta’s parser so that the head of each item can be in any position, we obtain an O(n5) parser which can be filtered into the parser of Collins (1996) by eliminating the COMBINER steps.","TREF One of the most straightforward projective dependency parsing strategies was introduced by REF, and is based on the CYK bottom–up parsing strategy REF. COMBINER steps follow the same mechanism as those in the algorithm of REF, and LINK steps work analogously to those of TREF, so this schema can be seen as being intermediate between those two algorithms. By generalizing the linking steps in Eisner and Satta’s parser so that the head of each item can be in any position, we obtain an O(n5) parser which can be filtered into the parser of TREF by eliminating the COMBINER steps."
A,P96-1025,M98-1009,1,"Statistical Model In SIFT’s statistical model, augmented parse trees are generated according to a process similar to that described in Collins (1996, 1997).","Statistical Model In SIFT’s statistical model, augmented parse trees are generated according to a process similar to that described in (GTREF)."
A,P96-1025,P10-1002,2,"However, in order to utilize some syntax information between the pair of words, we adopt the syntactic distance representation of (Collins, 1996), named Collins distance for convenience.","However, in order to utilize some syntax information between the pair of words, we adopt the syntactic distance representation of TREF, named Collins distance for convenience."
A,P96-1025,P11-2123,0,"This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003), where related words, like scissors and knife cannot be generalized.","This was done trying to overcome the limitations of lexicalized approaches to parsing (GTREF), where related words, like scissors and knife cannot be generalized."
A,P97-1003,P98-1091,0,"Examples of formalisms using this approach include the work of Magerman (1995), Charniak (1997), Collins (1997), and Goodman (1997).","Examples of formalisms using this approach include the work of REF, REF, TREF, and REF."
A,P97-1003,P98-1106,1,"Recently, it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words (see, e.g., (Collins, 1997)), which is what dependency grammars model explicitly do, but context-free phrasestructure grammars do not.","Recently, it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words (see, e.g., TREF), which is what dependency grammars model explicitly do, but context-free phrasestructure grammars do not."
A,P97-1003,P98-2148,0,"Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models (Jelinek et al., 1994; Collins, 1997).",Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models (GTREF).
A,P97-1003,P98-2234,0,"The 75.4% results may seen low compared to parsing results like the 88% precision and recall in (Collins, 1997), but those parsing results include many easier-to-parse constructs.","The 75.4% results may seen low compared to parsing results like the 88% precision and recall in TREF, but those parsing results include many easier-to-parse constructs."
A,P97-1003,P99-1059,1,"proved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). three models in (Collins, 1997) are susceptible to the O(n 3) method (cf. Collins's O(nh)).",proved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars (GTREF). three models in TREF are susceptible to the O(n 3) method (cf. Collins's O(nh)).
A,P97-1003,P99-1065,3,"We describe our experience in building on the parsing model of (Collins 97). This paper first describes a baseline approach, based on the parsing model of (Collins 97), which recovers dependencies with 72% accuracy.  Sketch of the Parsing Model The parsing model builds on Model 1 of (Collins 97); this section briefly describes the model. In (Collins 97), P(/~lo~) is defined as a product of terms, by assuming that the right-hand-side of the rule is generated in three steps: (Collins 97) describes a series of refinements to this basic model: the addition of ""distance"" (a conditioning feature indicating whether or not a modifier is adjacent to the head); the addition of subcategorization parameters (Model 2), and parameters that model wh-movement (Model 3); estimation techniques that smooth various levels of back-off (in particular using POS tags as word-classes, allowing the model to learn generalizations about POS classes of words).  Parsing the Czech PDT Many statistical parsing methods developed for English use lexicalized trees as a representation (e.g., (Jelinek et al. 94; Magerman 95; Ratnaparkhi 97; Charniak 97; Collins 96; Collins 97)); several (e.g., (Eisner 96; Collins 96; Collins 97; Charniak 97)) emphasize the use of parameters associated with dependencies between pairs of words.  Preferences for dependencies that do not cross verbs The model of (Collins 97) had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs. The parsers of (Collins 96,97) encoded this as a hard constraint. (Collins 99) describes results of 91% accuracy in recovering dependencies on section 0 of the Penn Wall Street Journal Treebank, using Model 2 of (Collins 97).","We describe our experience in building on the parsing model of TREF. This paper first describes a baseline approach, based on the parsing model of TREF, which recovers dependencies with 72% accuracy. A Sketch of the Parsing Model The parsing model builds on Model 1 of TREF; this section briefly describes the model. In TREF, P(/~lo~) is defined as a product of terms, by assuming that the right-hand-side of the rule is generated in three steps: TREF describes a series of refinements to this basic model: the addition of ""distance"" (a conditioning feature indicating whether or not a modifier is adjacent to the head); the addition of subcategorization parameters (Model 2), and parameters that model wh-movement (Model 3); estimation techniques that smooth various levels of back-off (in particular using POS tags as word-classes, allowing the model to learn generalizations about POS classes of words). Parsing the Czech PDT Many statistical parsing methods developed for English use lexicalized trees as a representation (e.g., (GTREF)); several (e.g., (GREF) emphasize the use of parameters associated with dependencies between pairs of words. Preferences for dependencies that do not cross verbs The model of TREF had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs. The parsers of (GTREF) encoded this as a hard constraint. REF describes results of 91% accuracy in recovering dependencies on section 0 of the Penn Wall Street Journal Treebank, using Model 2 of TREF."
A,P97-1003,P00-1058,2,"Inducing a stochastic grammar from the Treebank 4.1 Reconstructing derivations We want to extract from the Penn Treebank an LTAG whose derivations mirror the dependency analysis implicit in the head-percolation rules of (Magerman, 1995; Collins, 1997). Following (Collins, 1997), words occurring fewer than four times in training were replaced with the symbol *UNKNOWN* and tagged with the output of the part-of-speech tagger described in (Ratnaparkhi, 1996).","Inducing a stochastic grammar from the Treebank 4.1 Reconstructing derivations We want to extract from the Penn Treebank an LTAG whose derivations mirror the dependency analysis implicit in the head-percolation rules of (GTREF). Following TREF, words occurring fewer than four times in training were replaced with the symbol *UNKNOWN* and tagged with the output of the part-of-speech tagger described in REF."
A,P97-1003,P00-1060,0,"Introduction In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1991] [Magerman, 1992] [Magerman, 1995] [Eisner, 1996]. Many probabilistic evaluation models have been published inspired by one or more of these feature types [Black, 1992] [Briscoe, 1993] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996], but discrepancies between training sets, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively.","Introduction In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types REF REF REF REF REF TREF REF REF REF REF. Many probabilistic evaluation models have been published inspired by one or more of these feature types REF REF REF REF TREF REF REF, but discrepancies between training sets, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively."
A,P97-1003,P00-1061,0,"Such word-based lexicalizations of probability models are used successfully in the statistical parsing models of, e.g., Collins (1997), Charniak (1997), or Ratnaparkhi (1997).","Such word-based lexicalizations of probability models are used successfully in the statistical parsing models of, e.g., TREF, REF, or REF."
A,P97-1003,P00-1065,1,"The system is trained by rst using the Collins parser (Collins, 1997) to parse the 36,995 training sentences, matching annotated frame elements to parse constituents, and extracting various features from the string of words and the parse tree.","The system is trained by _x005f_x000c_rst using the Collins parser TREF to parse the 36,995 training sentences, matching annotated frame elements to parse constituents, and extracting various features from the string of words and the parse tree."
A,P97-1003,P02-1018,0,"(Collins (1997) discusses the recovery of one kind of empty node, viz., WH-traces).","TREF discusses the recovery of one kind of empty node, viz., WH-traces)."
A,P97-1003,P02-1031,1,"In this paper, we examine how the information provided by modern statistical parsers such as Collins (1997) and Charniak (1997) contributes to solving this problem. The Experiments In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997).","In this paper, we examine how the information provided by modern statistical parsers such as TREF and REF contributes to solving this problem. The Experiments In previous work using the FrameNet corpus, REF developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of TREF."
A,P97-1003,P02-1043,2,"Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly. Distance measures for CCG Our distance measures are related to those proposed by Goodman (1997), which are appropriate for binary trees (unlike those of Collins (1997)). Adding lexical information Gildea (2001) shows that removing the lexical dependencies in Model 1 of Collins (1997) (that is, not conditioning on wh when generating ws) decreases labeled precision and recall by only 0.5%.","Like the models of REF, the additional features in our model are generated probabilistically, whereas in the parser of TREF distance measures are assumed to be a function of the already generated structure and are not generated explicitly. Distance measures for CCG Our distance measures are related to those proposed by REF, which are appropriate for binary trees (unlike those of TREF). Adding lexical information REF shows that removing the lexical dependencies in Model 1 of TREF (that is, not conditioning on wh when generating ws) decreases labeled precision and recall by only 0.5%."
A,P97-1003,P03-1002,1,"This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in (Collins, 1997). At last, the dependency parser presented in (Collins, 1997) is used to generate the full parse.","This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in TREF. At last, the dependency parser presented in TREF is used to generate the full parse."
A,P97-1003,P03-1013,3,"Introduction Treebank-based probabilistic parsing has been the subject of intensive research over the past few years, resulting in parsing models that achieve both broad coverage and high parsing accuracy (e.g., Collins 1997; Charniak 2000). Lexicalization can increase parsing performance dramatically for English (Carroll and Rooth, 1998; Charniak, 1997, 2000; Collins, 1997), and the lexicalized model proposed by Collins (1997) has been successfully applied to Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000). Section 3 describes two standard lexicalized models (Carroll and Rooth, 1998; Collins, 1997), as well as an unlexicalized baseline model. Section 5 presents an error analysis for Collins’s (1997) lexicalized model, which shows that the head-head dependencies used in this model fail to cope well with the flat structures in Negra. Table 1: Results for the Collins (1997) model for various languages (dependency precision for Czech). Probabilistic Parsing Models 3.1 Probabilistic Context-Free Grammars Lexicalization has been shown to improve parsing performance for the Penn Treebank (e.g., Carroll and Rooth 1998; Charniak 1997, 2000; Collins 1997). Collins’s Head-Lexicalized Model In contrast to Carroll and Rooth’s (1998) approach, the model proposed by Collins (1997) does not compute rule probabilities directly. The lexicalized model proposed by Collins (1997) (henceforth Collins model) was re-implemented by one of the authors. The reader is referred to Schmid (2000) and Collins (1997) for details. Table 4: Linguistic features in the current model compared to the models of Carroll and Rooth (1998), Collins (1997), and Charniak (2000). In a second series of experiments, we investigated a more general way of dealing with the flatness of Negra, based on Collins’s (1997) model for nonrecursive NPs in the Penn Treebank (which are also flat). For non-recursive NPs, Collins (1997) does not use the probability function in (5), but instead substitutes Pr (and, by analogy, Pl) by: Pr(Ri (8) ,t(Ri),l(Ri)|P,Ri−1,t(Ri−1),l(Ri−1),d(i)). Table 4 shows the linguistic features of the resulting model compared to the models of Carroll and Rooth (1998), Collins (1997), and Charniak (2000). The Collins (1997) model does not use context-free rules, but generates the next category using zeroth order Markov chains (see Section 3.3), hence no information about the previous sisters is included. We first added sister-head dependencies for NPs (following Collins’s (1997) original proposal) and then for PPs, which are flat in Negra, and thus similar in structure to NPs (see Section 2.2). The progression in the probabilistic parsing literature has been to start with lexical head-head dependencies (Collins, 1997) and then add non-lexical sister information (Charniak, 2000), as illustrated in Table 4. The work by Collins et al. (1999) and Bikel and Chiang (2000) has demonstrated the applicability of the Collins (1997) model for Czech and Chinese. However, the learning curve for Negra (see Figure 1) indicates that the performance of the Collins (1997) model is stable, even for small training sets. In Experiment 1, we applied three standard parsing models from the literature to Negra: an unlexicalized PCFG model (the baseline), Carroll and Rooth’s (1998) head-lexicalized model, and Collins’s (1997) model based on head-head dependencies.","Introduction Treebank-based probabilistic parsing has been the subject of intensive research over the past few years, resulting in parsing models that achieve both broad coverage and high parsing accuracy (e.g., GTREF). Lexicalization can increase parsing performance dramatically for English (GTREF), and the lexicalized model proposed by TREF has been successfully applied to Czech REF and Chinese REF. Section 3 describes two standard lexicalized models (GTREF), as well as an unlexicalized baseline model. Section 5 presents an error analysis for TREF lexicalized model, which shows that the head-head dependencies used in this model fail to cope well with the flat structures in Negra. Table 1: Results for the TREF model for various languages (dependency precision for Czech).  Probabilistic Parsing Models 3.1 Probabilistic Context-Free Grammars Lexicalization has been shown to improve parsing performance for the Penn Treebank (e.g., TREF). Collins’s Head-Lexicalized Model In contrast to REF approach, the model proposed by TREF does not compute rule probabilities directly. The lexicalized model proposed by TREF (henceforth Collins model) was re-implemented by one of the authors. The reader is referred to REF and TREF for details. Table 4: Linguistic features in the current model compared to the models of REF, TREF, and REF. In a second series of experiments, we investigated a more general way of dealing with the flatness of Negra, based on TREF model for nonrecursive NPs in the Penn Treebank (which are also flat). For non-recursive NPs, TREF does not use the probability function in (5), but instead substitutes Pr (and, by analogy, Pl) by: Pr(Ri (8) ,t(Ri),l(Ri)|P,Ri−1,t(Ri−1),l(Ri−1),d(i)). Table 4 shows the linguistic features of the resulting model compared to the models of REF, TREF, and REF. The TREF model does not use context-free rules, but generates the next category using zeroth order Markov chains (see Section 3.3), hence no information about the previous sisters is included.We first added sister-head dependencies for NPs (following TREF original proposal) and then for PPs, which are flat in Negra, and thus similar in structure to NPs (see Section 2.2). The progression in the probabilistic parsing literature has been to start with lexical head-head dependencies TREF and then add non-lexical sister information REF, as illustrated in Table 4. The work by REF and Bikel and REF has demonstrated the applicability of the TREF model for Czech and Chinese. However, the learning curve for Negra (see Figure 1) indicates that the performance of the TREF model is stable, even for small training sets. In Experiment 1, we applied three standard parsing models from the literature to Negra: an unlexicalized PCFG model (the baseline), REF head-lexicalized model, and TREF model based on head-head dependencies."
A,P97-1003,P03-1055,2,"However, such constructions prove to be difficult for stochastic parsers (Collins et al., 1999) and they either avoid tackling the problem (Charniak, 2000; Bod, 2003) or only deal with a subset of the problematic cases (Collins, 1997).  First, we extend the mechanism of adding gap variables for nodes dominating a site of discontinuity (Collins, 1997). For the parsing and antecedent recovery experiments, in the case of WH-traces (WH–     ) and controlled NP-traces (NP–NP), we follow the standard technique of marking nodes dominating the empty element up to but not including the parent of the antecedent as defective (missing an argument) with a gap feature (Gazdar et al., 1985; Collins, 1997). The idea of threading EEs to their antecedents in a stochastic parser was proposed by Collins (1997), following the GPSG tradition (Gazdar et al., 1985).","However, such constructions prove to be difficult for stochastic parsers REF and they either avoid tackling the problem REF or only deal with a subset of the problematic cases TREF. First, we extend the mechanism of adding gap variables for nodes dominating a site of discontinuity TREF. For the parsing and antecedent recovery experiments, in the case of WH-traces (WH–     ) and controlled NP-traces (NP–NP), we follow the standard technique of marking nodes dominating the empty element up to but not including the parent of the antecedent as defective (missing an argument) with a gap feature (GTREF). The idea of threading EEs to their antecedents in a stochastic parser was proposed by TREF, following the GPSG tradition REF."
A,P97-1003,P04-1043,1,"The sentences were processed using Collins’ parser (Collins, 1997) to generate parse-trees automatically.",The sentences were processed using Collins’ parser TREF to generate parse-trees automatically.
A,P97-1003,P04-1047,0,"Both techniques implement variations on the approaches of (Magerman, 1994) and (Collins, 1997) for the purpose of differentiating between complement and adjunct.",Both techniques implement variations on the approaches of REF and TREF for the purpose of differentiating between complement and adjunct.
A,P97-1003,P04-1082,0,"Previous approaches to the problem (Collins, 1997; Johnson, 2002; Dienes and Dubey, 2003a,b; Higgins, 2003) have all been learning-based; the primary difference between the present algorithm and earlier ones is that it is not learned, but explicitly incorporates principles of GovernmentBinding theory (Chomsky, 1981), since that theory underlies the annotation. Both Collins (1997: 19) and Higgins (2003: 100) are explicit about this predisposition. Collins’ (1997) Model 3 integrates the detection and resolution of WH-traces in relative clauses into a lexicalized PCFG.","Previous approaches to the problem (GTREF) have all been learning-based; the primary difference between the present algorithm and earlier ones is that it is not learned, but explicitly incorporates principles of GovernmentBinding theory REF, since that theory underlies the annotation. Both TREF and REF are explicit about this predisposition. TREF Model 3 integrates the detection and resolution of WH-traces in relative clauses into a lexicalized PCFG."
A,P97-1003,P05-1006,1,"In order to extract the linguistic features necessary for the models, all sentences containing the target word were automatically part-of-speech-tagged using a maximum entropy tagger (Ratnaparkhi, 1998) and parsed using the Collins parser (Collins, 1997).","In order to extract the linguistic features necessary for the models, all sentences containing the target word were automatically part-of-speech-tagged using a maximum entropy tagger REF and parsed using the Collins parser TREF."
A,P97-1003,P05-1018,1,"We employ a robust statistical parser (Collins, 1997) to determine the constituent structure for each sentence, from which subjects (s), objects (o), and relations other than subject or object (x) are identified.","We employ a robust statistical parser TREF to determine the constituent structure for each sentence, from which subjects (s), objects (o), and relations other than subject or object (x) are identified."
A,P97-1023,P02-1053,3,"The second step is to estimate the semantic orientation of each extracted phrase (Hatzivassiloglou & McKeown, 1997). Hatzivassiloglou and McKeown (1997) have also developed an algorithm for predicting semantic orientation. Related Work This work is most closely related to Hatzivassiloglou and McKeown’s (1997) work on predicting the semantic orientation of adjectives. As an example, they present the following three sentences (Hatzivassiloglou & McKeown, 1997): Hatzivassiloglou and McKeown (1997) use a four-step supervised learning algorithm to infer the semantic orientation of adjectives from constraints on conjunctions. Another area for future work is to empirically compare PMI-IR and the algorithm of Hatzivassiloglou and McKeown (1997). On the other hand, it would be interesting to evaluate PMI-IR on the collection of 1,336 hand-labeled adjectives that were used in the experiments of Hatzivassiloglou and McKeown (1997). Previous work on determining the semantic orientation of adjectives has used a complex algorithm that does not readily extend beyond isolated adjectives to adverbs or longer phrases (Hatzivassiloglou and McKeown, 1997).","The second step is to estimate the semantic orientation of each extracted phrase TREF. TREF have also developed an algorithm for predicting semantic orientation. Related Work This work is most closely related to TREF work on predicting the semantic orientation of adjectives. As an example, they present the following three sentences TREF. TREF use a four-step supervised learning algorithm to infer the semantic orientation of adjectives from constraints on conjunctions. Another area for future work is to empirically compare PMI-IR and the algorithm of TREF. On the other hand, it would be interesting to evaluate PMI-IR on the collection of 1,336 hand-labeled adjectives that were used in the experiments of TREF. Previous work on determining the semantic orientation of adjectives has used a complex algorithm that does not readily extend beyond isolated adjectives to adverbs or longer phrases TREF."
A,W01-0521,P04-1014,0,"Hockenmaier also found the dependencies to be very beneficial — in contrast to recent results from the lexicalised PCFG parsing literature (Gildea, 2001) — but did not gain from the use of distance measures.",Hockenmaier also found the dependencies to be very beneficial — in contrast to recent results from the lexicalised PCFG parsing literature TREF — but did not gain from the use of distance measures.
A,W01-0521,P06-1043,3,"However, the work which is most directly comparable to ours is that of (Ratnaparkhi, 1999; Hwa, 1999; Gildea, 2001; Bacchiani et al., 2006). We concentrate particularly on the work of (Gildea, 2001; Bacchiani et al., 2006) as they provide results which are directly comparable to those presented in this paper. Lacking alternatives, both (Gildea, 2001) and (Bacchiani et al., 2006) give up on adapting a pure WSJ trained system, instead looking at the issue of how much of an improvement one gets over a pure Brown system by adding WSJ data (as seen in the last two lines of Table 1). We use the same divisions as Bacchiani et al. (2006), who base their divisions on Gildea (2001). We have already seen the results (Gildea, 2001) and (Bacchiani et al., 2006) achieve in Table 1. The BROWN parsing model is naturally better than the WSJ model for this task, but combining the two training corpora results in a better model (as in Gildea (2001)).","However, the work which is most directly comparable to ours is that of (GTREF). We concentrate particularly on the work of (GTREF) as they provide results which are directly comparable to those presented in this paper. Lacking alternatives, both TREF and REF give up on adapting a pure WSJ trained system, instead looking at the issue of how much of an improvement one gets over a pure Brown system by adding WSJ data (as seen in the last two lines of Table 1). We use the same divisions as REF, who base their divisions on TREF. We have already seen the results TREF and REF achieve in Table 1. The BROWN parsing model is naturally better than the WSJ model for this task, but combining the two training corpora results in a better model (as in TREF))."
A,W01-0521,P10-1036,1,"Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). Cross-domain speed improvement When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001).",Previous work has shown that parsers typically perform poorly outside of their training domain TREF. Cross-domain speed improvement When applying parsers out of domain they are typically slower and less accurate TREF.
A,W01-0521,W05-1516,0,"It was found in (Gildea, 2001) that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output.",It was found in TREF that the removal of bi-lexical statistics from a state-of-the-art PCFG parser resulted very small change in the output.
A,W01-0521,D09-1085,0,"Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001).","Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains TREF."
A,W02-1001,W03-0422,2,"Collins (2002) adapted the perceptron learning algorithm to tagging tasks, via sentence-based global feedback.","TREF adapted the perceptron learning algorithm to tagging tasks, via sentence-based global feedback."
A,W02-1001,W04-0824,2,"The classifier consists of two components based on the averaged multiclass perceptron (Collins, 2002; Crammer and Singer, 2003). Here we used the averaged perceptron (Collins, 2002), where the weight matrix used to classify the test data is the average of all of the matrices posited during training, i.e.,   > 4 1 > 1 *3254  * . 4.","The classifier consists of two components based on the averaged multiclass perceptron (GTREF). Here we used the averaged perceptron TREF, where the weight matrix used to classify the test data is the average of all of the matrices posited during training, i.e.,   > 4 1 > 1 *3254  * . 4."
A,W02-1001,P12-1111,3,"We adopt the basic feature set used in (Ratnaparkhi, 1996) and (Collins, 2002). For parameter estimation of θ, we use the averaged perceptron as described in (Collins, 2002). Following most previous work, e.g. (Collins, 2002) and (Shen et al., 2007), we divide this corpus into training set (sections 0-18), development set (sections 19-21) and the final test set (sections 22-24). Our implementation of this feature set is basically the same as the version used in (Collins, 2002).","We adopt the basic feature set used in REF and TREF. For parameter estimation of θ, we use the averaged perceptron as described in TREF. Following most previous work, e.g. TREF and REF, we divide this corpus into training set (sections 0-18), development set (sections 19-21) and the final test set (sections 22-24). Our implementation of this feature set is basically the same as the version used in TREF."
A,W02-1001,P13-4016,1,"There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm (Collins, 2002), and regularized structured SVMs trained using FOBOS (Duchi and Singer, 2009).","There is also a preliminary implementation of online learning methods such as the structured perceptron algorithm TREF, and regularized structured SVMs trained using FOBOS REF."
A,W02-1001,N03-1033,2,"Indeed, as for the voted perceptron of Collins (2002), we can get performance gains by reducing the support threshold for features to be included in the model.","Indeed, as for the voted per_x005f_x005f_x0002_ceptron of TREF, we can get performance gains by reducing the support threshold for features to be included in the model."
A,W02-1001,D13-1033,0,"It is usually modeled as a sequence model, often in combination with part-of-speech tagging and lemmatization (Collins, 2002; Hajic, 2004; Smith et al., 2005; ˇ Chrupała et al., 2008, and others).","It is usually modeled as a sequence model, often in combination with part-of-speech tagging and lemmatization (GTREF, and others)."
A,W02-1001,E06-1011,2,"We use the common method of setting the final weight vector as the average of the weight vectors after each iteration (Collins, 2002), which has been shown to alleviate overfitting.  This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs Collins (2002).","We use the common method of setting the final weight vector as the average of the weight vectors after each iteration TREF, which has been shown to alleviate overfitting.  This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs TREF."
A,W03-0430,P04-1007,0,"Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003).","Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing REF, and tagging or segmentation tasks (GTREF)."
A,W03-0430,P05-1002,0,"CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003), simplified part-of-speech (POS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of tabular data (Pinto et al., 2003), among other tasks.","CRFs have been applied with impressive empirical results to the tasks of named entity recognition TREF, simplified part-of-speech (POS) tagging REF, noun phrase chunking REF and extraction of tabular data REF, among other tasks."
A,W03-0430,P05-1003,0,"1 Introduction In recent years, conditional random fields (CRFs) (Lafferty et al., 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004).  In order to make training time manageable4 , we collapse the number of POS tags from 48 to 5 following the procedure used in (McCallum et al., 2003).","1 Introduction In recent years, conditional random fields (CRFs) REF have shown success on a number of natural language processing (NLP) tasks, including shallow parsing REF, named entity recognition TREF and information extraction from research papers REF.  In order to make training time manageable4 , we collapse the number of POS tags from 48 to 5 following the procedure used in TREF."
A,W03-0430,P05-1044,0,"Abstract Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003).",Abstract Conditional random fields REF are quite effective at sequence labeling tasks like shallow parsing REF and namedentity extraction TREF.
A,W03-0430,P06-1028,0,"The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing (Sha and Pereira, 2003) and information extraction (McCallum and Li, 2003).","The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing REF and information extraction TREF."
A,W03-0430,P06-1078,0,"Many discriminative methods have been applied to NER, such as decision trees (Sekine et al., 1998), ME models (Borthwick, 1999; Chieu and Ng, 2003), and CRFs (McCallum and Li, 2003).","Many discriminative methods have been applied to NER, such as decision trees REF, ME models REF, and CRFs TREF."
A,W03-0430,P08-1056,0,"These belong to two main categories based on machine learning (Bikel et al., 1997; Borthwick, 1999; McCallum and Li, 2003) and language or domain specific rules (Grishman, 1995; Wakao et al., 1996).",These belong to two main categories based on machine learning (GTREF) and language or domain specific rules REF.
A,W03-0430,P08-1081,0,"Linear CRF model has been successfully applied in NLP and text mining tasks (McCallum and Li, 2003; Sha and Pereira, 2003).",Linear CRF model has been successfully applied in NLP and text mining tasks (GTREF).
A,W03-0430,P10-1136,0,"Information extraction Finally, there exist large bodies of work on information extraction using models based on Markov and semi-Markov CRFs (Lafferty et al., 2001; Sarawagi and Cohen, 2004), and in particular for the task of named entity recognition (McCallum and Li, 2003).","Information extraction Finally, there exist large bodies of work on information extraction using models based on Markov and semi-Markov CRFs REF, and in particular for the task of named entity recognition TREF."
A,W03-0430,P12-1055,0,"Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005).",Current studies of NER mainly focus on formal text such as news articles (GTREF).
A,W03-0430,W04-0705,0,"A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines.","A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models REF, Maximum Entropy methods REF, Decision Trees REF, Conditional Random Fields TREF, Class-based Language Model REF, Agent-based Approach REF and Support Vector Machines."
A,W03-0430,W04-1221,0,"Such models are well suited to sequence analysis, and CRFs in particular have been shown to be useful in partof-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), and named entity recognition for newswire data (McCallum and Li, 2003).","Such models are well suited to sequence analysis, and CRFs in particular have been shown to be useful in partof-speech tagging REF, shallow parsing REF, and named entity recognition for newswire data TREF."
A,W03-0430,W04-3230,0,"Note that our formulation of CRFs is different from the widely-used formulations (e.g., (Sha and Pereira, 2003; McCallum and Li, 2003; Peng et al., 2004; Pinto et al., 2003; Peng and McCallum, 2004)).","Note that our formulation of CRFs is different from the widely-used formulations (e.g., (GTREF)."
A,W03-0430,W04-3234,0,"While global statistical approaches, such as sequential averaged perceptrons or CRFs (McCallum and Li, 2003), appear better suited to the NER problem than local symbolic learners, the two approaches search different hypothesis spaces.","While global statistical approaches, such as sequential averaged perceptrons or CRFs TREF, appear better suited to the NER problem than local symbolic learners, the two approaches search different hypothesis spaces."
A,W04-2405,E12-1017,0,"Here we adapt self-training, a simple technique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006).","Here we adapt self-training, a simple technique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (GTREF)."
A,W04-3103,P07-1125,3,"The most clearly relevant study is Light et al. (2004) where the focus is on introducing the problem, exploring annotation issues and outlining potential applications rather than on the specificities of the ML approach, though they do present some results using a manually crafted substring matching classifier and a supervised SVM on a collection of Medline abstracts. To further elucidate the nature of the task and improve annotation consistency, we have developed a new set of guidelines, building on the work of Light et al. (2004). SVM classifier Baseline denotes substring matching classifier of (Light et al., 2004). Baselines As a baseline classifier we use the substring matching technique of (Light et al., 2004), which labels a sentence as spec if it contains one or more of the following:suggest, potential, likely, may, at least,in part, possibl, further investigation, unlikely, putative,insights, point toward, promise and propose.","The most clearly relevant study is TREF where the focus is on introducing the problem, exploring annotation issues and outlining potential applications rather than on the specificities of the ML approach, though they do present some results using a manually crafted substring matching classifier and a supervised SVM on a collection of Medline abstracts. To further elucidate the nature of the task and improve annotation consistency, we have developed a new set of guidelines, building on the work of TREF. SVM classifier Baseline denotes substring matching classifier of TREF. Baselines As a baseline classifier we use the substring matching technique of TREF, which labels a sentence as spec if it contains one or more of the following:suggest, potential, likely, may, at least,in part, possibl, further investigation, unlikely, putative,insights, point toward, promise and propose."
A,W04-3111,D07-1112,1,"The development data was 200 sentences of labeled biomedical oncology text (BIO, the ONCO portion of the Penn Biomedical Treebank), as well as 200K unlabeled sentences (Kulick et al., 2004).","The development data was 200 sentences of labeled biomedical oncology text (BIO, the ONCO portion of the Penn Biomedical Treebank), as well as 200K unlabeled sentences TREF."
A,W04-3111,D07-1128,0,"We have achieved average results in the CoNLL domain adaptation track open submission (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004; MacWhinney, 2000; Brown, 1973).",We have achieved average results in the CoNLL domain adaptation track open submission (GTREF).
A,W04-3111,D07-1119,1,"For the second adaptation task we were given a large collection of unlabeled data in the chemistry domain (Kulick et al, 2004) as well as a test set of 5000 tokens (200 sentences) to parse (english_pchemtbtb_test.conll).",For the second adaptation task we were given a large collection of unlabeled data in the chemistry domain TREF as well as a test set of 5000 tokens (200 sentences) to parse (english_pchemtbtb_test.conll).
A,W04-3111,W08-0605,0,"In biomedical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et., 2004), and GENETAG  (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address.","In the biomedical domain, for example, several annotated corpora such as GENIA REF, PennBioIE TREF, and GENETAG REF have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address."
A,W04-3111,D07-1126,0,"The pchemtb-closed shared task (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004) is used to illustrate our models.",The pchemtb-closed shared task (GTREF) is used to illustrate our models.
A,W04-3111,W08-0504,0,"Due in large part to the creation of biomedical treebanks (Kulick et al., 2004; Tateisi et al., 2005) and rapid progress of data-driven parsers (Lease and Charniak, 2005; Nivre et al., 2007), there are now fast, robust and accurate syntactic parsers for text in the biomedical domain.","Due in large part to the creation of biomedical treebanks (GTREF) and rapid progress of data-driven parsers REF, there are now fast, robust and accurate syntactic parsers for text in the biomedical domain."
A,W04-3111,P10-1089,1,"For out-of-domain data, we get 21K examples from the Brown portion of the Treebank and 6296 examples from tagged Medline abstracts in the PennBioIE corpus (Kulick et al., 2004).","For out-of-domain data, we get 21K examples from the Brown portion of the Treebank and 6296 examples from tagged Medline abstracts in the PennBioIE corpus TREF."
A,W04-3111,P07-1031,1,"Our annotation guidelines1 are based on those developed for annotating full sub-NP structure in the biomedical domain (Kulick et al., 2004). The annotation guidelines for this biomedical corpus (an addendum to the Penn Treebank guidelines) introduce the use of NML nodes to mark internal NP structure.",Our annotation guidelines1 are based on those developed for annotating full sub-NP structure in the biomedical domain REF. The annotation guidelines for this biomedical corpus (an addendum to the Penn Treebank guidelines) introduce the use of NML nodes to mark internal NP structure.
A,W04-3111,P09-1117,1,"From the sublanguage biology domain, we used the oncology part of the PENNBIOIE corpus (Kulick et al., 2004) and removed all but three gene entity subtypes (generic, protein, and rna).","From the sublanguage biology domain, we used the oncology part of the PENNBIOIE corpus TREF and removed all but three gene entity subtypes (generic, protein, and rna)."
A,W04-3111,I05-2038,1,"Similar attempt of constructing integrated corpora is being done in University of Pennsylvania, where a corpus of MEDLINE abstracts in CYP450 and oncology domains where annotated for named entities, POS, and tree structure of sentences (Kulick et al, 2004).","Similar attempt of constructing integrated corpora is being done in University of Pennsylvania, where a corpus of MEDLINE abstracts in CYP450 and oncology domains where annotated for named entities, POS, and tree structure of sentences TREF."
A,W04-3252,C08-1040,2,"For example it has been used to measure centrality in hyperlinked web pages networks (Brin and Page, 1998; Kleinberg, 1998), lexical networks (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Kurland and Lee, 2005; Kurland and Lee, 2006), and semantic networks (Mihalcea et al., 2004). Our method is based on the ones described in (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007). Computing Speaker Salience The method we used is similar to the methods described in (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Kurland and Lee, 2005), which were originally used for ranking sentences and documents in extractive summarization and information retrieval systems.","For example it has been used to measure centrality in hyperlinked web pages networks REF, lexical networks (GTREF), and semantic networks REF. Our method is based on the ones described in (GTREF).  Computing Speaker Salience The method we used is similar to the methods described in (GTREF), which were originally used for ranking sentences and documents in extractive summarization and information retrieval systems."
A,W06-1615,P07-1056,3,"First, we show how to extend the recently proposed structural correspondence learning (SCL) domain adaptation algorithm (Blitzer et al., 2006) for use in sentiment classification. Then, it models the correlations between the pivot features and all other features by training linear pivot predictors to predict occurrences of each pivot in the unlabeled data from both domains (Ando and Zhang, 2005; Blitzer et al., 2006). For the part of speech tagging problem studied by Blitzer et al. (2006), frequently-occurring words in both domains were good choices, since they often correspond to function words such as prepositions and determiners, which are good indicators of parts of speech. Ando and Zhang (2005) and Blitzer et al. (2006) suggest λ = 10−4 , µ = 0, which we have used in our results so far. We augment each labeled target instance xj with the label assigned by the source domain classifier (Florian et al., 2004; Blitzer et al., 2006). As we noted in Section 5, we are able to significantly outperform basic structural correspondence learning (Blitzer et al., 2006). We also note that while Florian et al. (2004) and Blitzer et al. (2006) observe that including the label of a source classifier as a feature on small amounts of target data tends to improve over using either the source alone or the target alone, we did not observe that for our data. Finally we note that while Blitzer et al. (2006) did combine SCL with labeled target domain data, they only compared using the label of SCL or non-SCL source classifiers as features, following the work of Florian et al. (2004). First, we showed that for a given source and target domain, we can significantly improve for sentiment classification the structural correspondence learning model of Blitzer et al. (2006).","First, we show how to extend the recently proposed structural correspondence learning (SCL) domain adaptation algorithm TREF for use in sentiment classification. Then, it models the correlations between the pivot features and all other features by training linear pivot predictors to predict occurrences of each pivot in the unlabeled data from both domains (GTREF). For the part of speech tagging problem studied by TREF, frequently-occurring words in both domains were good choices, since they often correspond to function words such as prepositions and determiners, which are good indicators of parts of speech. REF and TREF suggest λ = 10−4 , µ = 0, which we have used in our results so far. We augment each labeled target instance xj with the label assigned by the source domain classifier (GTREF). As we noted in Section 5, we are able to significantly outperform basic structural correspondence learning TREF. We also note that while REFand TREF observe that including the label of a source classifier as a feature on small amounts of target data tends to improve over using either the source alone or the target alone, we did not observe that for our data. Finally we note that while TREF did combine SCL with labeled target domain data, they only compared using the label of SCL or non-SCL source classifiers as features, following the work of REF. First, we showed that for a given source and target domain, we can significantly improve for sentiment classification the structural correspondence learning model of TREF."
A,W07-1412,E12-1004,0,"Most RTE systems are based on advanced NLP components, machine learning techniques, and/or syntactic transformations (Zanzotto et al., 2007; Kouleykov and Magnini, 2005).","Most RTE systems are based on advanced NLP components, machine learning techniques, and/or syntactic transformations (GTREF)."
A,W07-1412,P08-1118,0,"Most entailment systems function as weak proof theory (Hickl et al., 2006; MacCartney et al., 2006; Zanzotto et al., 2007), but contradictions require deeper inferences and model building.","Most entailment systems function as weak proof theory (GTREF), but contradictions require deeper inferences and model building."
A,W07-1412,P12-1047,2,"Zanzotto et al. (2007) used a kernel method on syntactic tree pairs (Moschitti and Zanzotto, 2007).",TREF used a kernel method on syntactic tree pairs (GTREF).
A,W07-1412,W09-2501,0,"Almost all existing RTE models align the linguistic material of the premise and hypothesis and base at least part of their decision on properties of this alignment (Burchardt et al., 2007; Hickl and Bensley, 2007; Iftene and Balahur-Dobrescu, 2007; Zanzotto et al., 2007).",Almost all existing RTE models align the linguistic material of the premise and hypothesis and base at least part of their decision on properties of this alignment (GTREF).
A,W07-1412,W10-1609,0,"In the past, RTEs Challenges machine learning algorithms were widely used for the task of recognizing textual entailment (Marneffe et al., 2006; Zanzotto et al., 2007; Castillo, 2009) and they have reported goods results for English language.","In the past, RTEs Challenges machine learning algorithms were widely used for the task of recognizing textual entailment (GTREF) and they have reported goods results for English language."
A,W07-1412,W11-0135,0,"Systems addressing TE exploiting machine learning techniques with a variety of features, including lexical-syntactic and semantic features (e.g. Kozareva and Montoyo (2006), Zanzotto et al. (2007)) tend towards the opposite extreme of this framework, since even if linguistic features are used, they bring information about a specific aspect relevant to the inference task but they do not provide an independent judgment on it.","Systems addressing TE exploiting machine learning techniques with a variety of features, including lexical-syntactic and semantic features (e.g. GTREF) tend towards the opposite extreme of this framework, since even if linguistic features are used, they bring information about a specific aspect relevant to the inference task but they do not provide an independent judgment on it."
A,W07-1412,W11-2404,0,"For instance, it is not clear whether the availability of larger amounts of training data correlates with better performance (Hickl et al., 2006) or not (Zanzotto et al., 2007; Hickl and Bensley, 2007), even within the same evaluation setting.","For instance, it is not clear whether the availability of larger amounts of training data correlates with better performance REF or not (GTREF), even within the same evaluation setting."
A,W09-0404,N10-1045,0,"First of all, we believe that research on CLTE can employ inference mechanisms and semantic knowledge sources to augment existing MT methods, leading to improvements in the translation quality (e.g. (Pado et al., 2009)).","First of all, we believe that research on CLTE can employ inference mechanisms and semantic knowledge sources to augment existing MT methods, leading to improvements in the translation quality (e.g. TREF)."
A,W09-0404,W11-2112,0,"Pado et al. (2009) uses Textual Entailment fea- ´ tures extracted from the Standford Entailment Recognizer (MacCartney et al., 2006).",TREF uses Textual Entailment fea- ´ tures extracted from the Standford Entailment Recognizer REF.
A,W96-0213,A00-1031,1,"Recent comparisons of approaches that can be trained on corpora (van Halteren et al., 1998; Volk and Schneider, 1998) have shown that in most cases statistical aproaches (Cutting et al., 1992; Schmid, 1995; Ratnaparkhi, 1996) yield better results than finite-state, rule-based, or memory-based taggers (Brill, 1993; Daelemans et al., 1996).","Recent comparisons of approaches that can be trained on corpora REF have shown that in most cases statistical aproaches (GTREF) yield better results than finite-state, rule-based, or memory-based taggers REF."
A,W96-0213,A00-2013,1,"English Part of Speech (POS) tagging has been widely described in the recent past, starting with the (Church, 1988) paper, followed by numerous others using various methods: neural networks (Julian Benello and Anderson, 1989), HMM tagging (Merialdo, 1992), decision trees (Schmid, 1994), transformation-based error-driven learning (Brill, 1995), and maximum entropy (Ratnaparkhi, 1996), to select just a few. However different the methods were, English dominated in these tests","English Part of Speech (POS) tagging has been widely described in the recent past, starting with the REF paper, followed by numerous others using various methods: neural networks REF, HMM tagging REF, decision trees REF, transformation-based error-driven learning REF, and maximum entropy TREF, to select just a few. However different the methods were, English dominated in these tests"
A,W96-0213,A00-2020,0,Abney et al. (1999) applied boosting to part of speech tagging. Adwait Ratnaparkhi (1996) estimates a probability distribution for tagging using a maximum entropy approach.,REF applied boosting to part of speech tagging. Ad_x005f_x005f_x0002_wait TREF estimates a probability distribution for tagging using a maximum entropy approach.
A,W96-0213,A97-1004,2,"Maxilnum Entropy The model used here for sentence-boundary detection is based on the maximum entropy model used for POS tagging in (Ratnaparkhi, 1996).",Maxilnum Entropy The model used here for sentence-boundary detection is based on the maximum entropy model used for POS tagging in TREF.
A,W96-0213,E03-1002,1,"Many statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001) are based on a history-based probability model (Black et al., 1993), where the probability of each decision in a parse is conditioned on the previous decisions in the parse.","Many statistical parsers (GTREF) are based on a history-based probability model REF, where the probability of each decision in a parse is conditioned on the previous decisions in the parse."
A,W96-0213,J01-3003,1,Our method was applied to 23 million words of the WSJ that were automatically tagged with Ratnaparkhi's maximum entropy tagger (Ratnaparkhi 1996) and chunked with the partial parser CASS (Abney 1996).,Our method was applied to 23 million words of the WSJ that were automatically tagged with Ratnaparkhi's maximum entropy tagger TREF and chunked with the partial parser CASS REF.
A,W96-0213,N04-1013,0,"For example, since the Collins parser depends on a prior part-of-speech tagger (Ratnaparkhi, 1996), we included the time for POS tagging in our Collins measurements.","For example, since the Collins parser depends on a prior part-of-speech tagger TREF, we included the time for POS tagging in our Collins measurements."
A,W96-0213,W02-0301,0,"Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996).","Support Vector Machines (SVMs) REF and Maximum Entropy (ME) method REF are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (TREF)."
A,W96-0213,W03-0424,2,The ME Tagger The ME tagger is based on Ratnaparkhi (1996)’s POS tagger and is described in Curran and Clark (2003) .,The ME Tagger The ME tagger is based on TREF’s POS tagger and is described in REF .
A,W96-0213,W03-0428,0,"Finally, in section 4 we add additional features to the maxent model, and chain these models into a conditional markov model (CMM), as used for tagging (Ratnaparkhi, 1996) or earlier NER work (Borthwick, 1999).","Finally, in section 4 we add additional features to the maxent model, and chain these models into a conditional markov model (CMM), as used for tag_x005f_x005f_x0002_ging TREF or earlier NER work REF."
A,W96-0213,P02-1055,0,"In modern lexicalized parsers, POS tagging is often interleaved with parsing proper instead of being a separate preprocessing module (Collins, 1996; Ratnaparkhi, 1997).","In modern lexicalized parsers, POS tagging is often interleaved with parsing proper instead of being a separate preprocessing module (GTREF)."
A,W96-0213,P99-1023,1,"Much research has been done to improve tagging accuracy using several different models and methods, including: hidden Markov models (HMMs) (Kupiec, 1992), (Charniak et al., 1993); rule-based systems (Brill, 1994), (Brill, 1995); memory-based systems (Daelemans et al., 1996); maximum-entropy systems (Ratnaparkhi, 1996); path voting constraint systems (Tiir and Oflazer, 1998); linear separator systems (Roth and Zelenko, 1998);  and majority voting systems (van Halteren et al., 1998).","Much research has been done to improve tagging accuracy using several different models and methods, including: hidden Markov models (HMMs) REF, REF; rule-based systems REF, REF; memory-based systems REF; maximum-entropy systems TREF; path voting con_x0002_straint systems REF; linear separator systems REF;  and majority voting systems REF."
A,W97-0703,W00-1438,3,"Barzilay and Elhadad (1997) dealt with some of tile limitations in Hirst and St-Onge's algorithm by examining every possible lexical chain which could be computed, not just those possible at a given point in the text. A Linear Time Algorithm for Computing Lexical Chains Overview Our research on lexical chains as an intermediate representation for automatic text summarization fol- lows the research of Barzilay and Elhadad (1997). Table 1 denotes sample metrics tuned to simulate the system devised by Barzilay and Elhadad (1997). Comparison with Previous Work As mentioned above, this research is based on the work of Barzilay and Elhadad (1997) on lexical chains.","TREF dealt with some of tile limitations in Hirst and St-Onge's algorithm by examining every possible lexical chain which could be computed, not just those possible at a given point in the text. A Linear Time Algorithm for Computing Lexical Chains Overview Our research on lexical chains as an intermediate representation for automatic text summarization fol- lows the research of TREF. Table 1 denotes sample metrics tuned to simulate the system devised by TREF. Comparison with Previous Work As mentioned above, this research is based on the work of TREF on lexical chains."
A,W97-0710,P04-3020,0,"Previous approaches include supervised learning (Teufel and Moens, 1997), vectorial similarity computed between an initial abstract and sentences in the given document, or intra-document similarities (Salton et al., 1997).","Previous approaches include supervised learning TREF, vectorial similarity computed between an initial abstract and sentences in the given document, or intra-document similarities REF."
A,W99-0613,P01-1008,3,"This method of co-training has been previously applied to a variety of natural language tasks, such as word sense disambiguation (Yarowsky, 1995), lexicon construction for information extraction (Riloff and Jones, 1999), and named entity classification (Collins and Singer, 1999). Our model is based on the DLCoTrain algorithm proposed by (Collins and Singer, 1999), which applies a co-training procedure to decision list classifiers for two independent sets of features. Following (Collins and Singer, 1999), filtering is based on the strength of the context and its frequency.","This method of co-training has been previously applied to a variety of natural language tasks, such as word sense disambiguation REF, lexicon construction for information extraction REF, and named entity classification TREF. Our model is based on the DLCoTrain algorithm proposed by TREF, which applies a co-training procedure to decision list classifiers for two independent sets of features. Following TREF filtering is based on the strength of the context and its frequency."
A,W99-0613,P02-1046,1,"The empirical investigations described here and below use the data set of (Collins and Singer, 1999). The task is to classify names in text as person, location, or organization.","The empirical investigations described here and below use the data set of TREF. The task is to classify names in text as person, location, or organization."
A,W99-0613,W11-0319,0,"Similar approaches are used among others in (Thelen and Riloff, 2002) for learning semantic lexicons, in (Collins and Singer, 1999) for namedentity recognition, and in (Fagni and Sebastiani, 2007) for hierarchical text categorization.","Similar approaches are used among others in REF for learning semantic lexicons, in TREF for namedentity recognition, and in REF for hierarchical text categorization."
A,W99-0613,W09-1116,0,Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition.,TREF and REF apply bootstrapping to the related task of named-entity recognition.
A,W99-0613,P07-1125,0,"Co-training has also been used for named entity recognition (NER) (Collins and Singer, 1999), coreference resolution (Ng and Cardie, 2003), text categorization (Nigam and Ghani, 2000) and improving gene name data (Wellner, 2005).","Co-training has also been used for named entity recognition (NER) TREF, coreference resolution REF, text categorization REF and improving gene name data REF."
A,W06-2925,D07-1101,2,"The first-order features φ1(x, h, m) are the exact same implementation as in previous CoNLL system (Carreras et al., 2006).","The first-order features φ1(x, h, m) are the exact same implementation as in previous CoNLL system TREF."
A,W06-2925,D08-1059,0,"Graph-based (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras et al., 2006) and transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms offer two different approaches to data-driven dependency parsing.",Graph-based (GTREF) and transition-based (REF) parsing algorithms offer two different approaches to data-driven dependency parsing.
A,W06-2925,J11-1005,2,"For testing, all training data are used for training, with the number of training iterations set to be the number which gave the highest accuracy during the development experiments. This method was used by Carreras, Surdeanu, and Marquez (2006) in their parsing model.","For testing, all training data are used for training, with the number of training iterations set to be the number which gave the highest accuracy during the development experiments. This method was used by TREF in their parsing model."
A,W06-2925,N12-1032,1,"Most of the participants took language-independent approaches toward leveraging this complexity into better performance: generating machine learning features based on each item in a token’s list of morphological attributes (Nivre et al., 2006b; Carreras et al., 2006); using the entire list as an atomic feature (Chang et al., 2006; Titov and Henderson, 2007); or generating features based on each pair of attributes in the cross-product of the lists of a potential head and dependent (McDonald et al., 2006; Nakagawa, 2007).",Most of the participants took language-independent approaches toward leveraging this complexity into better performance: generating machine learning features based on each item in a token’s list of morphological attributes (GTREF); using the entire list as an atomic feature REF; or generating features based on each pair of attributes in the cross-product of the lists of a potential head and de_x005fpendent REF.
A,W06-2925,P10-1002,0,"Both the graph-based (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras et al., 2006) and the transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms are related to our word-pair classification model.",Both the graph-based (GTREF) and the transition-based REF parsing algorithms are related to our word-pair classification model.
A,W06-2925,W07-2217,0,"The perceptron has been used in previous work on dependency parsing by Carreras et al. (2006), with a parser based on Eisner’s algorithm (Eisner, 2000), and also on incremental constituent parsing (Collins & Roark, 2006).","The perceptron has been used in previous work on dependency parsing by TREF, with a parser based on Eisner’s algorithm REF, and also on incremental constituent parsing REF."
A,W03-0428,D07-1016,0,"The ML classi- fier used for this experiment is a conditional Markov model tagger which is designed for, and proved successful in, named entity recognition in newspaper and biomedical text (Klein et al., 2003; Finkel et al., 2005).","The ML classi- fier used for this experiment is a conditional Markov model tagger which is designed for, and proved successful in, named entity recognition in newspaper and biomedical text (GTREF)."
A,W03-0428,D08-1003,0,"While multiple machine learning approaches have been proposed for information extraction in recent years (McCallum et al., 2000; Cohen and McCallum, 2003; Klein et al., 2003; Krishnan and Manning, 2006), manually created regexes remain a widely adopted practical solution for information extraction (Appelt and Onyshkevych, 1998; Fukuda et al., 1998; Cunningham, 1999; Tanabe and Wilbur, 2002; Li et al., 2006; DeRose et al., 2007; Zhu et al., 2007).","While multiple machine learning approaches have been proposed for information extraction in recent years (GTREF), manually created regexes remain a widely adopted practical solution for information extraction REF."
A,W03-0428,D11-1144,1,"The use of char N-gram (N-gram substring) features was inspired by the work of (Klein et al. 2003), where the introduction of such features has been shown to improve the overall F1 score by over 20%.","The use of char N-gram (N-gram substring) features was inspired by the work of TREF, where the introduction of such features has been shown to improve the overall F1 score by over 20%."
A,W03-0428,P05-1001,1,"Named entity chunking. F-measure on the test sets. Previous best results: FIJZ03 (Florian et al., 2003), CN03 (Chieu and Ng, 2003), KSNM03 (Klein et al., 2003).","Named entity chunking. F-measure on the test sets. Previous best results: FIJZ03 REF, CN03 REF, KSNM03 TREF."
A,W03-0428,P05-2023,0,"We experimented with a conditional Markov model tagger that performed well on language-independent NER (Klein et al., 2003) and the identification of gene and protein names (Finkel et al., 2005).",We experimented with a conditional Markov model tagger that performed well on language-independent NER TREF and the identification of gene and protein names REF.
A,W03-0428,P11-3019,0,"Another approach we will also focus is dividing words into characters and applying character-level models (Klein et al., 2003).",Another approach we will also focus is dividing words into characters and applying character-level models TREF.
A,W03-0428,W07-1712,0,"Sometimes, these types of features are referred to as wordexternal and word-internal (Klein et al., 2003)","Sometimes, these types of features are referred to as wordexternal and word-internal TREF"
A,C00-2105,D07-1091,0,"German morphological analysis and POS tagging was done using LoPar Schmidt and Schulte im Walde (2000), English POS tagging was done with Brill’s tagger (Brill, 1995), followed by a simple lemmatizer based on tagging results.","German morphological analysis and POS tagging was done using LoPar TREF English POS tagging was done with Brill’s tagger REF, followed by a simple lemmatizer based on tagging results."
A,P04-1084,C08-2026,0,"Inside-out alignments (Wu, 1997), such as the one in Example 1.3, cannot be induced by any of these theories; in fact, there seems to be no useful synchronous grammar formalisms available that handle inside-out alignments, with the possible exceptions of synchronous tree-adjoining grammars (Shieber and Schabes, 1990), Bertsch and Nederhof (2001) and generalized multitext grammars (Melamed et al., 2004), which are all way more complex than ITG, STSG and (2,2)-BRCG.","Inside-out alignments REF, such as the one in Example 1.3, cannot be induced by any of these theories; in fact, there seems to be no useful synchronous grammar formalisms available that handle inside-out alignments, with the possible exceptions of synchronous tree-adjoining grammars REF, Bertsch and REF and generalized multitext grammars TREF, which are all way more complex than ITG, STSG and (2,2)-BRCG."
A,P04-1084,C10-2157,0,"It differs from the more strict synchronous grammar formalisms (Wu, 1995; Melamed et al., 2004) because it does not try to perform simultaneous parsing on parallel grammars; instead, the model learns an augmented target-language grammar whose rules make “soft alignments” with a given source tree.","It differs from the more strict synchronous grammar formalisms (GTREF) because it does not try to perform simultaneous parsing on parallel grammars; instead, the model learns an augmented target-language grammar whose rules make “soft alignments” with a given source tree."
A,P04-1084,D07-1078,0,"Several researchers (Melamed et al., 2004; Zhang et al., 2006) have already proposed methods for binarizing synchronous grammars in the context of machine translation.",Several researchers (GTREF) have already proposed methods for binarizing synchronous grammars in the context of machine translation.
A,P04-1084,P04-1083,1,"We shall limit our attention to MTGs in Generalized Chomsky Normal Form (GCNF) (Melamed et al., 2004).",We shall limit our attention to MTGs in Generalized Chomsky Normal Form (GCNF) TREF.
A,P04-1084,P06-1123,2,Our measure of alignment complexity is analogous to what Melamed et al. (2004) call “fanout.”,Our measure of alignment complexity is analo_x0002_gous to what TREF call “fanout.”
A,P04-1084,P10-1146,0,"Wellington et al. therefore argue that in order to extract as many rules as possible, a more powerful formalism than synchronous CFG/TSG is required: for example, generalized multitext grammar (Melamed et al., 2004), which is equivalent to synchronous set-local multicomponent CFG/TSG (Weir, 1988).","REF therefore argue that in order to extract as many rules as possible, a more powerful formalism than synchronous CFG/TSG is required: for example, generalized multitext grammar TREF, which is equivalent to synchronous set-local multicom_x0002_ponent CFG/TSG REF."
A,P05-1045,C10-1064,1,"We utilized an offthe-shelf system, Stanford Named Entity Recognizer (Finkel et al., 2005) for detecting entity mentions on the English sentences.","We utilized an off the-shelf system, Stanford Named Entity Recognizer TREF for detecting entity mentions on the English sentences."
A,P05-1045,C10-1083,1,"We first extract named entities from scattered opinions DT using Stanford Named Entity Recognizer (Finkel et al., 2005).",We first extract named entities from scattered opinions DT using Stanford Named Entity Recognizer TREF.
A,P05-1045,C10-1087,1,"Preprocessing First, all documents are parsed and processed with standard tools for named entity recognition (Finkel et al., 2005) and coreference resolution.","Preprocessing First, all documents are parsed and processed with standard tools for named en_x0002_tity recognition TREF and coreference resolution."
A,P05-1045,C10-1091,1,"the annotations from the Stanford Named Entity Recognizer (Finkel et al. 2005) that labels PERSON, ORGANIZATION, and LOCATION entities.","the annotations from the Stanford Named Entity Recognizer TREF that labels PERSON, ORGANIZATION, and LOCATION entities."
A,P05-1045,C10-1105,1,"First, if NPi is an NE, we create a feature whose value is the NE label of NPi, as determined by the Stanford CRF-based NE recognizer (Finkel et al., 2005).","First, if NPi is an NE, we create a feature whose value is the NE label of NPi, as determined by the Stanford CRF-based NE recognizer TREF."
A,P05-1045,C10-1131,1,"A large body of literature in joint learning has demonstrated that such an approach can suffer from cascaded errors at testing, and does not benefit from the potential for joint learning (Finkel et al., 2006).","A large body of literature in joint learning has demonstrated that such an approach can suffer from cascaded errors at testing, and does not benefit from the potential for joint learning TREF."
A,P05-1045,C10-2058,0,"Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (Downey et al., 2005; Sutton and McCallum, 2004; Finkel et al., 2005; Mann, 2007) experimented with information fusion of relations across multiple documents.",Some prior work REF demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (GTREF) experimented with information fusion of relations across multiple documents.
A,P05-1045,C10-2078,1,"To compute the features which we extract in the next section, all instances in our data sets were part-of-speech tagged by the MXPOST tagger (Ratnaparkhi, 1996), parsed with the MaltParser, and named entity tagged with the Stanford NE tagger (Finkel et al., 2005).","To compute the features which we extract in the next section, all instances in our data sets were part-of-speech tagged by the MXPOST tagger REF, parsed with the Malt Parser, and named entity tagged with the Stan_x0002_ford NE tagger TREF."
A,P05-1045,C10-3011,1,"Named Entity Recognizer tagging (NER): We integrated Stanford’s NER tagger (Finkel et al., 2005).",Named Entity Recognizer tagging (NER): We integrated Stanford’s NER tagger TREF.
A,P05-1045,D07-1033,2,"For example, non-local features such as “same phrases in a document do not have different entity classes” were shown to be useful in named entity recognition (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005; Krishnan and Manning, 2006).","For example, non-local features such as “same phrases in a document do not have different entity classes” were shown to be useful in named entity recognition (GTREF)."
A,P05-1045,D08-1035,0,"The use of annealing to obtain a maximum a posteriori (MAP) configuration from sampling-based inference is common (e.g., Finkel 2005; Goldwater 2007).","The use of annealing to obtain a maximum a posteriori (MAP) configuration from sampling-based in_x0002_ference is common (e.g., GTREF)."
A,P05-1045,D09-1016,1,Finkel et al. (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents.,TREF and REF incorporate global information by enforcing event role or label consistency over a document or across related documents.
A,P05-1045,D09-1057,1,"However, due to the lack of a fine grained NER tool at hand, we employ the Stanford NER package (Finkel et al., 2005) which identifies only four types of named entities.","However, due to the lack of a fine grained NER tool at hand, we employ the Stanford NER package TREF which identifies only four types of named entities."
A,P05-1045,D09-1101,1,"Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer (Finkel et al., 2005).",Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer TREF.
A,P05-1045,D09-1119,0,"As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (Zhang et al., 2001; Sha and Pereira, 2003; Finkel et al., 2005; Ratinov and Roth, 2009).","As discussed above, all state-of-the-art published methods rely on lexical features for such tasks TREF."
A,P05-1045,D09-1120,1,"While much research (Ng and Cardie, 2002; Culotta et al., 2007; Haghighi and Klein, 2007; Poon and Domingos, 2008; Finkel and Manning, 2008) has explored how to reconcile pairwise decisions to form coherent clusters, we simply take the transitive closure of our pairwise decision (as in Ng and Cardie (2002) and Bengston and Roth (2008)) which can and does cause system errors.","While much research (GTREF) has explored how to reconcile pairwise decisions to form coherent clusters, we simply take the transitive closure of our pairwise decision (as in Ng and REF and Bengston and REF) which can and does cause system errors."
A,P05-1045,D09-1158,1,"Previous work in domain adaptation can be classified into two categories: [S+T+], where a small, labeled target domain data is available, e.g. (Blitzer et al., 2006; Jiang and Zhai, 2007; Daume III, 2007; Finkel and Manning, 2009), or [S+T-], where no labeled target domain data is available, e.g. (Blitzer et al., 2006; Jiang and Zhai, 2007).","Previous work in domain adaptation can be classified into two categories: [S+T+], where a small, labeled target do_x0002_main data is available, e.g. (GTREF), or [S+T-], where no labeled target domain data is available, e.g. REF."
A,P05-1045,D10-1048,1,"Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (Bengston and Roth, 2008; Finkel and Manning, 2008; Haghighi and Klein, 2009; Stoyanov, 2010).",Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (GTREF).
A,P05-1045,D10-1099,1,"We preprocess our textual data as follows: We first use the Stanford named entity recognizer (Finkel et al., 2005) to find entity mentions in the corpus.",We preprocess our textual data as follows: We first use the Stanford named entity recognizer TREF to find entity mentions in the corpus.
A,P05-1045,D11-1034,1,"Then, we use the Stanford Named Entity Recognizer (Finkel et al., 2005) to identify named entities, which we replace with a unique token (‘NE’).","Then, we use the Stanford Named Entity Recognizer TREF to identify named entities, which we replace with a unique token (‘NE’)."
A,P05-1045,D11-1072,1,We first identify noun phrases that potentially denote named entities. We use the Stanford NER Tagger (Finkel05) to discover these and segment the text accordingly.,We first identify noun phrases that potentially denote named entities. We use the Stanford NER Tagger TREF to discover these and segment the text accordingly.
A,P05-1045,D11-1075,0,"We used as candidates all strings labeled in the annotated data as well as all named entities found by the Stanford NER tagger for CoNLL (Finkel et al., 2005).",We used as candidates all strings labeled in the annotated data as well as all named entities found by the Stanford NER tagger for CoNLL TREF.
A,P05-1045,D11-1135,1,"First we employ Stanford tools to tokenize, sentence-split and Part-Of-Speech tag (Toutanova et al., 2003) a document. Next we recognize named entities (Finkel et al., 2005) by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags.","First we employ Stanford tools to tokenize, sentence-split and Part-Of-Speech tag REF a document. Next we recognize named entities TREF by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags."
A,P05-1045,D11-1141,1,"Compared with the state-of-the-art newstrained Stanford Named Entity Recognizer (Finkel et al., 2005), T-SEG obtains a 52% increase in F1 score.","Compared with the state-of-the-art newstrained Stanford Named Entity Recognizer TREF, T-SEG obtains a 52% increase in F1 score."
A,P05-1045,D11-1144,0,Various modifications to CRF have recently been introduced to take into account of non-local dependencies (Krishnan and Manning 2006) or broader context beyond training data (Du et al. 2010),Various modifications to CRF have recently been introduced to take into account of non-local dependencies TREF or broader context beyond training data REF
A,P05-1045,D12-1009,1,"The sampling distributions are annealed, as a search technique to find the best configuration of assignments (Finkel et al., 2005).","The sampling distributions are annealed, as a search technique to find the best configuration of assignments TREF."
A,P05-1045,D12-1042,0,"They used the Stanford named entity recognizer (Finkel et al., 2005) to find entity mentions in text and constructed relation mentions only between entity mentions in the same sentence.",They used the Stanford named entity recog_x0002_nizer TREF to find entity mentions in text and constructed relation mentions only between entity mentions in the same sentence.
A,P05-1045,D12-1072,0,"To identify potential speakers they used the Stanford NER tagger (Finkel et al., 2005) and a method outlined in Davis et al. (2003) that allowed them to find nominal character references.",To identify potential speakers they used the Stanford NER tagger TREF and a method out_x0002_lined in REF that allowed them to find nominal character references.
A,P05-1045,D12-1076,1,"We use Stanford Named Entity Recognizer(Finkel et al., 2005) to collect named entities which are not in the Wikipedia list.",We use Stanford Named Entity Recognizer TREF to collect named entities which are not in the Wikipedia list.
A,P05-1045,D12-1080,1,"We use the Stanford Core NLP suite (Toutanova et al., 2003; Finkel et al., 2005; Klein and Manning, 2003; Lee et al., 2011) to annotate each document with POS and NER tags, parse trees, and coreference chains.","We use the Stanford Core NLP suite (GTREF) to annotate each document with POS and NER tags, parse trees, and coreference chains."
A,P05-1045,D12-1092,0,"In the literature, most studies focus on English event extraction and have achieved certain success (e.g. Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006; Maslennikov and Chua, 2007; Finkel et al., 2005; Ji and Grishman, 2008; Patwardhan and Riloff, 2009, 2011; Liao and Grishman 2010; Hong et al., 2011).","In the literature, most studies focus on English event extraction and have achieved certain success (GTREF)."
A,P05-1045,D12-1131,0,Finkel et al. (2005) also integrated non-local information into entity annotation algorithms using Gibbs sampling.,TREF also integrated non-local information into entity annotation algorithms using Gibbs sampling.
A,P05-1045,D13-1040,1,"After post-processing (tokeniza￾tion, sentence-splitting, and part-of-speech tagging), named entities were automatically recognized and labeled with PER, ORG, LOC, and MISC (Finkel et al., 2005).","After post-processing (tokeniza￾tion, sentence-splitting, and part-of-speech tagging), named entities were automatically recognized and labeled with PER, ORG, LOC, and MISC TREF."
A,P05-1045,D13-1042,1,"But human editorial judgment being the bottleneck, we sampled 50% or 50,000 snippets, whichever was smaller. Starting with about 752,450 pages, we ran the Stanford NER (Finkel et al., 2005) to mark person spans.","But human editorial judgment being the bottleneck, we sampled 50% or 50,000 snippets, whichever was smaller. Starting with about 752,450 pages, we ran the Stanford NER TREF to mark person spans."
A,P05-1045,D13-1043,1,"EXEMPLAR employs the Stanford NER (Finkel et al., 2005) to recognize named entities.",EXEMPLAR employs the Stanford NER TREF to recognize named entities.
A,P05-1045,D13-1103,1,"The QA-SYS performs Part of Speech tagging using Stanford POS tagger (Toutanova et al., 2003), and Named Entity Recognition using Stanford NER (Finkel et al., 2005), and then builds a Lucene index over the set of input documents.","The QA-SYS performs Part of Speech tagging using Stanford POS tagger REF, and Named Entity Recognition using Stanford NER TREF, and then builds a Lucene index over the set of input documents."
A,P05-1045,D13-1117,2,"For training the CRF model, we used a comprehensive set of features from Finkel et al. (2005) that gives state-of-the-art results on this task.","For training the CRF model, we used a comprehensive set of features from TREF that gives state-of-the-art results on this task."
A,P05-1045,D13-1136,1,"Entities were found using the Stanford named entity tagger (Finkel et al., 2005), and were matched to their name in Freebase.","Entities were found using the Stanford named entity tagger TREF, and were matched to their name in Freebase."
A,P05-1045,D13-1142,0,"NER is a fairly researched field (Finkel et al., 2005; Ratinov et al., 2011; Bunescu and Pasca, 2006; Kulkarni et al., 2009; Milne and Witten, 2008) and is also used in several commercial applications such as Zemanta, OpenCalais and AlchemyAPI4 , which are able to automatically insert links for a NE pointing to a knowledge base such as Wikipedia or IMDB.","NER is a fairly researched field (GTREF) and is also used in several commercial applications such as Zemanta, OpenCalais and AlchemyAPI4 , which are able to automatically insert links for a NE pointing to a knowledge base such as Wikipedia or IMDB."
A,P05-1045,D13-1159,0,"A common approach is to utilize a Named Entity Recognition (NER) system like Stanford NER (Finkel et al., 2005), which recognizes the names of things (e.g., person and product names) from texts.","A common approach is to utilize a Named Entity Recognition (NER) system like Stan_x0002_ford NER TREF, which recognizes the names of things (e.g., person and product names) from texts."
A,P05-1045,D13-1178,1,"To assign types to arguments, we apply Stanford Named Entity Recognizer (Finkel et al., 2005), and also look up the argument in WordNet 2","To assign types to arguments, we apply Stanford Named Entity Recognizer TREF, and also look up the argument in WordNet 2"
A,P05-1045,D13-1191,1,"We identified these mentions of persons using Stanford NER (Finkel et al., 2005) and treated each person mention as a single token.",We identified these mentions of persons using Stanford NER TREF and treated each person mention as a single token.
A,P05-1045,E09-1007,0,"Stanford NER (or in short Stanford) associated to the following model provided by the tool and which was trained on different news corpora (CoNLL, MUC6, MUC7 and ACE):
ner-eng-ie.crf-3-all2008-distsim.ser.gz (Finkel et al., 2005)","Stanford NER (or in short Stanford) associated to the following model provided by the tool and which was trained on different news corpora (CoNLL, MUC6, MUC7 and ACE):
ner-eng-ie.crf-3-all2008-distsim.ser.gz TREF"
A,P05-1045,E09-1011,1,"We parse the data using the Collins Parser (Collins, 1997), and then tag person, location and organization names using the Stanford Named Entity Recognizer (Finkel et al., 2005).","We parse the data using the Collins Parser REF, and then tag person, location and organization names using the Stanford Named Entity Recognizer TREF."
A,P05-1045,E09-1037,0,"Some stem from work on graphical models, including loopy belief propagation (Sutton andMcCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006). Also relevant are stacked learning (Cohen and Carvalho, 2005), interpretable as approximation of non-local feature values (Martins et al., 2008), and M-estimation (Smith et al., 2007), which allows training without inference.","Some stem from work on graphical models, including loopy belief propagation REF, Gibbs sampling TREF, sequential Monte Carlo methods such as particle filtering REF, and variational inference TREF. Also relevant are stacked learning REF, interpretable as approximation of non-local feature values REF, and M-estimation REF, which allows training without inference."
A,P05-1045,E09-1091,1,"In all the experiments, our source side language is English, and the Stanford Named Entity Recognizer (Finkel et al, 2005) was used to extract NEs from the source side article.","In all the experiments, our source side language is English, and the Stanford Named Entity Recognizer TREF was used to extract NEs from the source side article."
A,P05-1045,E12-1029,1,"The features include the NP head noun and its premodifiers. We also use the Stanford NER tagger (Finkel et al., 2005) to identify Named Entities within the NP.",The features include the NP head noun and its premodifiers. We also use the Stanford NER tagger TREF to identify Named Entities within the NP.
A,P05-1045,E12-1033,1,"For semantic role labeling we use SWIRL9 , for chunk parsing CASS (Abney, 1991) and for constituency parsing Stanford Parser (Klein and Manning, 2003). Named-entity information is provided by Stanford Tagger (Finkel et al., 2005).","For semantic role labeling we use SWIRL9 , for chunk parsing CASS REF and for constituency parsing Stanford Parser REF. Named-entity information is provided by Stanford Tagger TREF."
A,P05-1045,E12-1054,1,"Named Entity The replaced token should not be part of a named entity. For this purpose, we applied the Stanford NER (Finkel et al., 2005).","Named Entity The replaced token should not be part of a named entity. For this purpose, we applied the Stanford NER TREF."
A,P05-1045,I08-4013,0,"In the first approach, heuristic rules are used to find the dependencies (Bunescu and Mooney, 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al., 2005).","In the first approach, heuristic rules are used to find the dependencies REF or penalties for label inconsistency are required to handset ad-hoc TREF."
A,P05-1045,I08-5014,1,"We have used the freely available Stanford Named Entity Recognizer (Finkel, Grenager, and Manning, 2005) in our engine.",We have used the freely available Stanford Named Entity Recognizer (GTREF) in our engine.
A,P05-1045,I08-6004,1,"From the above corpora, we first extracted all the NEs from the English side, using the Stanford NER tool [Finkel et al, 2005].","From the above corpora, we first extracted all the NEs from the English side, using the Stanford NER tool TREF."
A,P05-1045,J11-1002,0,"Furthermore, as our tasks can be regarded as a sequential labeling problem (to label if a word is an opinion word, a target, or an ordinary word), we experimented with the conditional random fields (CRF) technique (Lafferty, McCallum, and Pereira 2001) for extraction, which is a popular information extraction method and has been successfully used in labeling tasks such as POS tagging (Lafferty, McCallum, and Pereira 2001) and Named Entity Recognition (Finkel, Grenager, and Manning 2005).","Furthermore, as our tasks can be regarded as a sequential labeling problem (to label if a word is an opinion word, a target, or an ordinary word), we experimented with the conditional random fields (CRF) technique REF for extraction, which is a popular information extraction method and has been successfully used in labeling tasks such as POS tagging REF and Named Entity Recognition (GTREF)."
A,P05-1045,J12-4004,1,"Then, we use the Stanford Named Entity Recognizer (Finkel, Grenager, and Manning 2005) to identify named entities, which we replace with a unique token (‘NE’).","Then, we use the Stanford Named Entity Recognizer (GTREF) to identify named entities, which we replace with a unique token (‘NE’)."
A,P05-1045,J13-2001,1,"First, we test the NE alignment performance with the same Chinese NE recognizer (Wu’s system, adopted earlier) but with different English NE recognizers that include the Mallet toolkit (used before), the Stanford NE recognizer (Finkel, Grenager, and Manning 2005), and Minor Third (Cohen 2004).","First, we test the NE alignment performance with the same Chinese NE recognizer (Wu’s system, adopted earlier) but with different English NE recognizers that include the Mallet toolkit (used before), the Stanford NE recognizer (GTREF), and Minor Third REF."
A,P05-1045,N06-1054,1,"Such techniques include Gibbs sampling (Finkel et al., 2005), a general-purpose Monte Carlo method, and integer linear programming (ILP), (Roth and Yih, 2005), a general-purpose exact framework for NP-complete problems.","Such techniques include Gibbs sampling TREF, a general-purpose Monte Carlo method, and integer linear programming (ILP), REF, a general-purpose exact framework for NP-complete problems."
A,P05-1045,N07-1009,0,"Undirected graphical models such as Conditional Random Fields (CRFs) (Lafferty et al., 2001) have shown great success for problems involving structured output variables (e.g. Wellner et al. (2004), Finkel et al. (2005)).","Undirected graphical models such as Conditional Random Fields (CRFs) REF have shown great success for problems involving structured output variables (e.g. REF,TREF))."
A,P05-1045,N07-1011,0,"Others have attempted to train global scoring functions using Gibbs sampling (Finkel et al., 2005), message propagation, (Bunescu and Mooney, 2004; Sutton and McCallum, 2004), and integer linear programming (Roth and Yih, 2004)","Others have attempted to train global scoring functions using Gibbs sampling TREF, message propagation, REF, and integer linear pro_x0002_gramming REF"
A,P05-1045,N07-1042,0,"More distantly related, Sutton and McCallum (2004) and Finkel et al. (2005) propose graphical models for combining information about a given entity from multiple mentions.","More distantly related, REF and TREF propose graphical models for combining information about a given entity from multiple mentions."
A,P05-1045,N07-2046,0,"Recent applications of statistical coreference models are beginning to show promise (Finkel et al, 2005; Ji & Grishman, 2005).",Recent applications of statistical coreference models are beginning to show promise (GTREF).
A,P05-1045,N09-1037,1,"We construct our joint model as an extension to the discriminatively trained, feature-rich, conditional random field-based, CRF-CFG parser of (Finkel and Manning, 2008).","We construct our joint model as an extension to the discriminatively trained, feature-rich, conditional random field-based, CRF-CFG parser of TREF."
A,P05-1045,N09-1068,1,"To our knowledge, the discriminatively trained dependency model we used has not been previously published, but it is very similar to recent work on discriminative constituency parsing (Finkel and Manning, 2008).","To our knowledge, the discriminatively trained dependency model we used has not been previously published, but it is very similar to recent work on discriminative constituency parsing TREF."
A,P05-1045,N10-1068,0,"We focus on training using Gibbs sampling (Geman and Geman, 1984), because it has been popularly applied in the natural language literature, e.g., (Finkel et al., 2005; DeNero et al., 2008; Blunsom et al., 2009).","We focus on training using Gibbs sampling REF, because it has been popularly applied in the natural language literature, e.g., (GTREF)."
A,P05-1045,N10-1072,1,"C’s nameEntitySet represents the set of all named entities in C’s article labeled by Stanford NER (Finkel et al., 2005).",C’s nameEntitySet represents the set of all named entities in C’s article labeled by Stanford NER TREF.
A,P05-1045,N10-1117,0,"One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling (Finkel et al., 2005), another is (loopy) sum-product belief propagation (Smith and Eisner, 2008).","One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling TREF, another is (loopy) sum-product belief propagation REF."
A,P05-1045,N10-1121,1,"Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005).",Namedentity information was obtained by the Stanford tag_x0002_ger TREF.
A,P05-1045,N12-1008,1,"Global Constraints Previous work demonstrated the benefits of applying declarative constraints in information extraction (Finkel et al., 2005; Roth and tau Yih, 2004; Chang et al., 2007; Druck and McCallum, 2010).",Global Constraints Previous work demonstrated the benefits of applying declarative constraints in information extraction (GTREF).
A,P05-1045,N12-1013,2,"The second application, information extraction from seminar announcements, has been modeled previously with skip-chain CRFs (Sutton and McCallum, 2005; Finkel et al., 2005).","The second application, information extraction from seminar announcements, has been modeled previously with skip-chain CRFs (GTREF)."
A,P05-1045,N12-1065,1,"The Stanford named entity recognition (NER) software (Finkel et al., 2005) is an implementation of linear chain Conditional Random Field (CRF) sequence models, which includes a three class (person, organization, location and other) named entity recognizer for English.","The Stanford named entity recognition (NER) software TREF is an implementation of linear chain Conditional Random Field (CRF) sequence models, which includes a three class (person, organization, location and other) named entity recognizer for English."
A,P05-1045,N12-1080,1,"For identification of named entities, we use Stanford NER (Finkel et al., 2005).","For identification of named entities, we use Stanford NER TREF."
A,P05-1045,N12-1084,0,"It uses Named Entity Recognition (NER) (Finkel, Grenager, and Manning, 2005; Ratinov and Roth, 2009; Ritter et al., 2011) as a subroutine to identify named person entities, though we are also interested in unnamed persons such as “my teacher” and pronouns.","It uses Named Entity Recognition (NER) (GTREF) as a subroutine to identify named person entities, though we are also interested in unnamed persons such as “my teacher” and pronouns."
A,P05-1045,N12-1085,0,"More specifically, we seek a posterior distribution over latent variables that partition words in a sentence into flow and inert groups; we estimate this posterior using Gibbs sampling (Finkel et al., 2005).","More specifically, we seek a posterior distribution over latent variables that partition words in a sentence into flow and inert groups; we estimate this posterior using Gibbs sampling TREF."
A,P05-1045,N13-1006,1,English features were taken from Finkel et al. (2005).,English features were taken from TREF.
A,P05-1045,N13-1007,0,"We used Stanford NER (Finkel et al., 2005) for English named entity recognition (NER), KNP for Japanese NER, and BaseNER (Zhao and Kit, 2008) for Chinese NER.","We used Stanford NER TREF for English named entity recognition (NER), KNP for Japanese NER, and BaseNER REF for Chinese NER."
A,P05-1045,N13-1037,0,"In part-of-speech tagging, the accuracy of the Stanford tagger (Toutanova et al., 2003) falls from 97% on Wall Street Journal text to 85% accuracy on Twitter (Gimpel et al., 2011). In named entity recognition, the CoNLL-trained Stanford recognizer achieves 44% F-measure (Ritter et al., 2011), down from 86% on the CoNLL test set (Finkel et al., 2005).","In part-of-speech tagging, the accuracy of the Stanford tagger REF falls from 97% on Wall Street Journal text to 85% accuracy on Twitter REF. In named entity recognition, the CoNLL-trained Stanford recognizer achieves 44% F-measure REF, down from 86% on the CoNLL test set TREF."
A,P05-1045,N13-1059,1,"For syntactic analysis we use the Stanford Parser (Finkel et al., 2005).",For syntactic analysis we use the Stanford Parser TREF.
A,P05-1045,P06-1059,2,"In NER tasks, however, informa￾tion about a distant entity is often more useful than information about the previous state (Finkel et al., 2005).","In NER tasks, however, informa￾tion about a distant entity is often more useful than information about the previous state TREF."
A,P05-1045,P06-1089,2,"Global information is known to be useful in other NLP tasks, especially in the named entity recognition task, and several studies successfully used global features (Chieu and Ng, 2002; Finkel et al., 2005).","Global information is known to be useful in other NLP tasks, especially in the named entity recognition task, and several studies success_x0002_fully used global features (GTREF)."
A,P05-1045,P06-1141,3,"Finkel et al. (2005) hand-set penalties for inconsistency in entity labeling at different occurrences in the text, based on some statistics from training data.","TREF hand-set penalties for inconsistency in entity labeling at different occurrences in the text, based on some statistics from training data."
A,P05-1045,P06-2054,0,"An additional consistent edge of a linear-chain conditional random field (CRF) explicitly models the dependencies between distant occurrences of similar words (Sutton and McCallum, 2004; Finkel et al., 2005).",An additional consistent edge of a linear-chain conditional random field (CRF) explicitly models the dependencies between distant occurrences of similar words (GTREF).
A,P05-1045,P08-1031,0,"We employ Gibbs sampling, previously used in NLP by Finkel et al. (2005) and Goldwater et al. (2006), among others.","We employ Gibbs sampling, previously used in NLP by TREF and REF, among others."
A,P05-1045,P08-2012,1,We added named entity (NE) tags to the data using the tagger of Finkel et al. (2005).,We added named entity (NE) tags to the data using the tagger of TREF.
A,P05-1045,P08-4003,1,"Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution.","Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger REF, the YamCha chunker REF and the Stanford Named Entity Rec_x005fognizer TREF, the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser REF to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser REF, yielding an easy-to-use Java-only solution."
A,P05-1045,P09-1113,1,"We perform named entity tagging using the Stanford four-class named entity tagger (Finkel et al., 2005).",We perform named entity tagging using the Stanford four-class named entity tagger TREF.
A,P05-1045,P09-2041,1,"To implement this method, we first use the Stanford Named Entity Recognizer (Finkel et al., 2005) to identify the set of person and organisation entities, E, from each article in the corpus.","To implement this method, we first use the Stanford Named Entity Recognizer TREF to identify the set of person and organisation entities, E, from each article in the corpus."
A,P05-1045,P10-1015,1,"We processed each novel with the Stanford NER tagger (Finkel et al., 2005) and extracted noun phrases that were categorized as persons or organizations.",We processed each novel with the Stanford NER tagger TREF and extracted noun phrases that were categorized as persons or organizations.
A,P05-1045,P10-1056,1,"We run the Stanford Named Entity Recognizer (Finkel et al., 2005) and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs.","We run the Stanford Named Entity Recognizer TREF and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs."
A,P05-1045,P10-1148,1,"That is, a non-animate noun can hardly constitute an experience. In order to make a distinction, we use the dependency parser and a named-entity recognizer (Finkel et al., 2005) that can recognize person pronouns and person names.","That is, a non-animate noun can hardly constitute an experience. In order to make a distinction, we use the dependency parser and a named-entity recognizer TREF that can recognize person pronouns and person names."
A,P05-1045,P10-2049,1,"To reproduce the system by Zhuang et al. (2006), we substitute the cast and crew list employed by them (see Section 3.2), with a NER component (Finkel et al., 2005).","To reproduce the system by REF, we substitute the cast and crew list employed by them (see Section 3.2), with a NER compo_x005f_x0002_nent TREF."
A,P05-1045,P11-1037,1,"For example, the average F1 of the Stanford NER (Finkel et al., 2005) , which is trained on the CoNLL03 shared task data set and achieves state-of-the-art performance on that task, drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets.","For example, the average F1 of the Stan_x0002_ford NER TREF , which is trained on the CoNLL03 shared task data set and achieves state-of-the-art performance on that task, drops from 90.8% REF to 45.8% on tweets."
A,P05-1045,P11-1082,0,"Since knowledge extraction from webbased encyclopedia is typically noisy (Ponzetto andPoesio, 2009), we use YAGO to determine whether two NPs have a relation only if one NP is a named entity (NE) of type person, organization, or location according to the Stanford NE recognizer (Finkel et al., 2005) and the other NP is a common noun.","Since knowledge extraction from webbased encyclopedia is typically noisy REF, we use YAGO to determine whether two NPs have a relation only if one NP is a named entity (NE) of type person, organization, or location according to the Stanford NE recognizer TREF and the other NP is a common noun."
A,P05-1045,P11-1113,0,"Finkel et al. (2005) used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models.","TREF used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models."
A,P05-1045,P11-1114,1,"Semantic features: we use the Stanford NER tagger (Finkel et al., 2005) to determine if the targeted NP is a named entity, and we use the Sundance parser (Riloff and Phillips, 2004) to assign semantic class labels to each NP’s head noun.","Semantic features: we use the Stanford NER tag_x005f_x0002_ger TREF to determine if the targeted NP is a named entity, and we use the Sundance parser REF to assign semantic class labels to each NP’s head noun."
A,P05-1045,P11-3004,1,"To create d we extract all NEs from the text using the StanfordNE Recognizer (Finkel et al., 2005) and represent each NE by its Wikipedia URI.",To create d we extract all NEs from the text using the StanfordNE Recognizer TREF and represent each NE by its Wikipedia URI.
A,P05-1045,P12-1042,1,"We use the Stanford Named Entity Recognizer (Finkel et al., 2005) for this purpose.",We use the Stanford Named Entity Recognizer TREF for this purpose.
A,P05-1045,P12-1072,1,"Due to the large size of the corpora, we uniformly sampled a subset of documents for each corpus and ran the Stanford NER tagger (Finkel et al., 2005), which tagged named entities mentions as person, location, and organization.","Due to the large size of the corpora, we uniformly sampled a subset of documents for each corpus and ran the Stanford NER tagger TREF, which tagged named entities mentions as person, location, and organization."
A,P05-1045,P12-1073,0,"The Figure also shows the results of the Stanford NER tagger for English (Finkel et al., 2005) (we used the MUC-7 classifier).",The Figure also shows the results of the Stanford NER tagger for English TREF (we used the MUC-7 classifier).
A,P05-1045,P12-1075,1,"Following (Yao et al., 2011), we filter out noisy documents and use natural language packages to annotate the documents, including NER tagging (Finkelet al., 2005) and dependency parsing (Nivre et al., 2004).","Following REF, we filter out noisy documents and use natural language packages to annotate the documents, including NER tagging TREF and dependency parsing REF."
A,P05-1045,P12-1087,1,"Run Stanford CoreNLP with POS tagging and named entity recognition (Finkel et al., 2005)",Run Stanford CoreNLP with POS tagging and named entity recognition TREF
A,P05-1045,P12-1089,2,"Mainly, predetermined word-level dependencies were represented as links in the underlying graphical model (Sutton and McCallum, 2004; Finkel et al., 2005).","Mainly, predetermined word-level dependencies were represented as links in the underlying graphical model (GTREF)."
A,P05-1045,P12-2013,1,"Accordingly, we extract all the named entities in a sentence using Stanford’s Name Entity Recognizer (Finkel et al., 2005).","Accordingly, we extract all the named entities in a sentence using Stanford’s Name Entity Recognizer (TREF)."
A,P05-1045,P12-2024,1,"Similarly, the semantic phrase table (SPT) has been extracted from the same corpora annotated with the Stanford NE tagger (Faruqui and Pado, 2010; Finkel et al., 2005).","Similarly, the semantic phrase table (SPT) has been extracted from the same corpora annotated with the Stanford NE tagger (GTREF)."
A,P05-1045,P12-2064,0,"We adopted the feature set investigatedin De Felice (2008) for article error correction. We use the Stanford coreNLP toolkit1 (Toutanova and Manning, 2000; Klein and Manning, 2003a; Klein and Manning, 2003b; Finkel et al, 2005) to extract the features.",We adopted the feature set investigatedin REF for article error correction. We use the Stanford coreNLP toolkit1 (GTREF) to extract the features.
A,P05-1045,P12-2069,1,"It includes: cleaning up and normalization of the input using regular expressions, sentence segmentation, tokenization and lemmatization using GATE (Cunningham et al., 2002), syntactical parsing and dependency parsing (collapsed) using the Stanford Parser (de Marneffe et al., 2006), and Named Entity Recognition using Stanford NER (Finkel et al., 2005).","It includes: cleaning up and normalization of the input using regular expressions, sentence segmentation, tokenization and lemmatization using GATE REF, syntactical parsing and dependency parsing (collapsed) using the Stanford Parser REF, and Named Entity Recognition using Stanford NER TREF."
A,P05-1045,P12-3003,0,"For example, the average F1 of the Stanford NER (Finkel et al., 2005) drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets, while Liu et al. (2010) report that the F1 score of a state-ofthe-art SRL system (Meza-Ruiz and Riedel, 2009) falls to 42.5% on tweets as apposed to 75.5% on news.","For example, the average F1 of the Stanford NER TREF drops from 90.8% REF to 45.8% on tweets, while Liu et al. (2010) report that the F1 score of a state-ofthe-art SRL system REF falls to 42.5% on tweets as apposed to 75.5% on news."
A,P05-1045,P12-3013,1,"BIUTEE provides state-of-the-art pre-processing utilities: Easy-First parser (Goldberg and Elhadad, 2010), Stanford named-entity-recognizer (Finkel et al., 2005) and ArkRef coreference resolver (Haghighi and Klein, 2009), as well as utilities for sentencesplitting and numerical-normalizations.","BIUTEE provides state-of-the-art pre-processing utilities: Easy-First parser REF, Stanford named-entity-recognizer TREF and ArkRef coreference resolver REF, as well as utilities for sentencesplitting and numerical-normalizations."
A,P05-1045,P13-1106,1,"The Stanford CRF-based NER tagger was used as the monolingual component in our models (Finkel et al., 2005).",The Stanford CRF-based NER tagger was used as the monolingual component in our models TREF.
A,P05-1045,P13-1129,0,"We utilize the Stanford tools (Toutanova et al., 2003; Finkel et al., 2005; Marneffe et al., 2006).",We utilize the Stanford tools (GTREF).
A,P05-1045,P13-1146,0,"Most well-known is the Stanford named entity recognition (NER) tagger (Finkel 2005) which assigns coarse-grained types like person, organization, location, and other to noun phrases that are likely to denote entities.","Most well-known is the Stanford named entity recognition (NER) tagger TREF which assigns coarse-grained types like person, organization, location, and other to noun phrases that are likely to denote entities."
A,P05-1045,P13-1166,0,"When training the Stanford NER system (Finkel et al., 2005) on just the tokens from the Freire data set and the parameters from english.all.3class.distsim.prop (included in the Stanford NER release, see also Van Erp and Van der Meij (2013)), our F-scores come very close to those reported by Freire et al. (2012), but mostly with a higher recall and lower precision.","When training the Stanford NER system TREF on just the tokens from the Freire data set and the parameters from english.all.3class.distsim.prop (included in the Stanford NER release, see also REF), our F-scores come very close to those reported by REF, but mostly with a higher recall and lower precision."
A,P05-1045,P13-2036,1,"To identify entities, we use a CRF-based named entity tagger (Finkel et al., 2005) and a Chinese word breaker (Gao et al., 2003) for English and Chinese corpora, respectively.","To identify entities, we use a CRF-based named entity tagger TREF and a Chinese word breaker REF for English and Chinese corpora, respectively."
A,P05-1045,P13-2116,0,"We validate the hypothesis that using linguistic features, e.g., part-of-speech tags (Toutanova and Manning, 2000), named-entity tags (Finkel et al., 2005), and dependency trees (de Marneffe et al., 2006), helps improve the quality of our approach, called Joint.","We validate the hypothesis that using linguistic features, e.g., part-of-speech tags REF, named-entity tags TREF, and dependency trees REF, helps improve the quality of our approach, called Joint."
A,P05-1045,P13-2128,0,"Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005).","Similarly, distributional features support generalization in Named Entity Recognition TREF."
A,P05-1045,P13-2141,1,"We used 500,000 Wikipedia articles (2,000,000 sentences) for generating training data for the NED component. We used Open NLP POS tagger, Standford NER (Finkel et al., 2005) and MaltParser (Nivre et al., 2006) to label/tag sentences.","We used 500,000 Wikipedia articles (2,000,000 sentences) for generating training data for the NED component. We used Open NLP POS tagger, Standford NER TREF and MaltParser REF to label/tag sentences."
A,P05-1045,P13-4023,0,"State-of-the-art tools for named entity recognition such as the Stanford NER Tagger (Finkel et al., 2005) compute semantic tags only for a small set of coarse-grained types: Person, Location, and Organization (plus tags for non-entity phrases of type time, money, percent, and date).","State-of-the-art tools for named entity recognition such as the Stanford NER Tagger TREF compute semantic tags only for a small set of coarse-grained types: Person, Location, and Organization (plus tags for non-entity phrases of type time, money, percent, and date)."
A,P05-1045,Q13-1028,1,"The first is named entity (NE) tags (PERSON, ORGANIZATION and LOCATION) returned by the Stanford NE recognition tool (Finkel et al., 2005).","The first is named entity (NE) tags (PERSON, ORGANIZATION and LOCATION) returned by the Stanford NE recognition tool TREF."
A,P05-1045,S10-1020,1,"Corry has only participated in the “open” setting, as it has already a number of preprocessing modules integrated into the system: the Stanford NLP toolkit for parsing (Klein and Manning, 2003) and NE-tagging (Finkel et al., 2005), Wordnet for semantic classes and the U.S. census data for assigning gender values to person names.","Corry has only participated in the “open” setting, as it has already a number of preprocessing modules integrated into the system: the Stanford NLP toolkit for parsing REF and NE-tagging TREF, Wordnet for semantic classes and the U.S. census data for assigning gender values to person names."
A,P05-1045,S12-1035,1,"Named Entities using the Stanford named entity recognizer recognizer (Finkel et al., 2005)",Named Entities using the Stanford named entity recognizer recognizer TREF
A,P05-1045,S12-1082,1,"Initially all sentences are pre-processed by the CoreNLP (Finkel et al., 2005; Toutanova et al., 2003) suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming.","Initially all sentences are pre-processed by the CoreNLP (GTREF) suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming."
A,P05-1045,S12-1085,1,"For capturing and normalizing the above mentioned expressions, we make use of the Stanford NER Toolkit (Finkel et al., 2005).","For capturing and normalizing the above mentioned expressions, we make use of the Stanford NER Toolkit TREF."
A,P05-1045,S13-1010,1,"Named entity recognition The Stanford Named Entity Recogniser (Finkel et al., 2005) was run over the BNC and any nouns that occur in the corpus with both named entity and non-named entity tags are extracted to form the list Stanford.",Named entity recognition The Stanford Named Entity Recogniser TREF was run over the BNC and any nouns that occur in the corpus with both named entity and non-named entity tags are extracted to form the list Stanford.
A,P05-1045,S13-1014,0,"Initially all sentences are pre-processed by the CoreNLP (Finkel et al., 2005; Toutanova et al., 2003) suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming.","Initially all sentences are pre-processed by the CoreNLP (GTREF) suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming."
A,P05-1045,S13-1018,0,"We analysed the text in the metadata, performing lemmatization, PoS tagging, named entity recognition and classification (NERC) and date detection using Stanford CoreNLP (Finkel et al., 2005; Toutanova et al., 2003).","We analysed the text in the metadata, performing lemmatization, PoS tagging, named entity recognition and classification (NERC) and date detection using Stanford CoreNLP (GTREF)."
A,P05-1045,S13-1021,1,"In our experiments, we performed named entity recognition with the Stanford NER tool using the standard English model (Finkel et al., 2005).","In our experiments, we performed named entity recognition with the Stanford NER tool using the standard English model TREF."
A,P05-1045,S13-1023,1,"We used the Stanford Named Entity Recognizer by (Finkel et al., 2005), with the 7 class model trained for MUC: Time, Location, Organization, Person, Money, Percent, Date.","We used the Stanford Named Entity Recognizer by TREF, with the 7 class model trained for MUC: Time, Location, Organization, Person, Money, Percent, Date."
A,P05-1045,W06-1643,0,"Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques, including TRP (Sutton and McCallum, 2004) and Gibbs sampling (Finkel et al., 2005).","Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques, including TRP REF and Gibbs sampling TREF."
A,P05-1045,W06-1655,0,"There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (Finkel et al., 2005; Culotta et al., 2005; Sha and Pereira, 2003).","There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (GTREF)."
A,P05-1045,W09-0422,1,"One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer (Finkel et al., 2005).",One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer TREF.
A,P05-1045,W09-1119,1,"The results we obtained on the CoNLL03 test set were consistent with what was reported in (Finkel et al., 2005).",The results we obtained on the CoNLL03 test set were consistent with what was reported in TREF.
A,P05-1045,W09-1218,0,"Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007) and dependency parsing (Nakagawa, 2007) with a great deal of success.",Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (GTREF) and dependency parsing REF with a great deal of success.
A,P05-1045,W10-0725,1,"The texts we use in our experiments are the development set of the RTE-5 challenge (Bentivogli et al., 2009), and we preprocess the data using the Stanford named-entity recognizer (Finkel et al., 2005).","The texts we use in our experiments are the development set of the RTE-5 challenge REF, and we preprocess the data using the Stanford named-entity recognizer TREF."
A,P05-1045,W10-1902,0,Huang et al. (2007) com￾bines a linear-chain CRF and two SVM models to enhance the recall. Finkel et al. (2005) used Gibbs Sampling to add non-local dependencies into linear-chain CRF model for information extraction.,REF com￾bines a linear-chain CRF and two SVM models to enhance the recall. TREF used Gibbs Sampling to add non-local dependencies into linear-chain CRF model for information extraction.
A,P05-1045,W10-2420,0,"We use SYNERGY (Shah et al., 2010), an ensemble NER system that combines the UIUC NER (Ritanov and Roth, 2009) and Stanford NER (Finkel et al., 2005) systems, to produce GNM and ONM from G and O by selecting named mentions.","We use SYNERGY REF, an ensemble NER system that combines the UIUC NER TREF and Stanford NER TREF systems, to produce GNM and ONM from G and O by selecting named mentions."
A,P05-1045,W10-2906,1,"We used the Stanford NER tagger (Finkel et al., 2005) with its default configuration as our full monolingual model for each language.",We used the Stanford NER tagger TREF with its default configuration as our full monolingual model for each language.
A,P05-1045,W10-3102,1,"The Stanford Named Entity Recognizer (NER) is based on the machine learning algorithm Conditional Random Fields (Finkel et al., 2005) and has been used extensively for identifying named entities in news text.",The Stanford Named Entity Recognizer (NER) is based on the machine learning algorithm Conditional Random Fields TREF and has been used extensively for identifying named entities in news text.
A,P05-1045,W11-0219,1,"These include several off-the-shelf statisical NLP tools such as the Stanford POS tagger (Toutanova and Manning, 2000), the Stanford named-entity recognizer (NER) (Finkel et al., 2005) and the Stanford Parser (Klein and Manning, 2003).","These include several off-the-shelf statisical NLP tools such as the Stanford POS tagger REF, the Stanford named-entity recognizer (NER) TREF and the Stanford Parser REF."
A,P05-1045,W11-0810,2,"For example, Finkel et al. (2005) note that in the CoNLL dataset, the same term can be used for a location and for the name of a sports team.","For example, TREF note that in the CoNLL dataset, the same term can be used for a location and for the name of a sports team."
A,P05-1045,W11-0902,1,"For EMD we used the Stanford named entity recognizer (Finkel et al., 2005).",For EMD we used the Stanford named entity recognizer TREF.
A,P05-1045,W11-1513,1,"Stanford NER system first detects sentences in the data then labels four classes of named entities: PERSON, ORGANIZATION, LOCATION, and MISCELLANEOUS (Finkel et al., 2005).","Stanford NER system first detects sentences in the data then labels four classes of named entities: PERSON, ORGANIZATION, LOCATION, and MIS_x005fCELLANEOUS TREF."
A,P05-1045,W11-1723,0,"The merged news file is acted upon by a log-linear part of speech tagger we obtained from the Stanford NLP webpage (Manning,2000)",The merged news file is acted upon by a log-linear part of speech tagger we obtained from the Stanford NLP webpage TREF
A,P05-1045,W11-1907,1,"We use the Stanford MaxentTagger (Toutanova et al., 2003) for partof-speech tagging, and the Stanford Named Entity Recognizer (Finkel et al., 2005) for annotating named entities.","We use the Stanford MaxentTagger REF for partof-speech tagging, and the Stanford Named En_x0002_tity Recognizer TREF for annotating named entities."
A,P05-1045,W11-1908,1,"NP chunks are computed from the parse trees provided in the CoNLL distribution, Named entities are extracted with the Stanford NER tool (Finkel et al., 2005).","NP chunks are computed from the parse trees provided in the CoNLL distribution, Named entities are extracted with the Stanford NER tool TREF."
A,P05-1045,W11-2202,1,"We ran the Stanford Named Entity Recognition system (Finkel et al., 2005) to obtain a set of 25,000 candidate mentions which the system judged to be names of people.","We ran the Stanford Named Entity Recognition system TREF to obtain a set of 25,000 candidate mentions which the system judged to be names of people."
A,P05-1045,W11-2705,1,"For the learning of patterns we used the top 64 documents retrieved by Google and to recognize the named entities in the pattern we apply several strategies, namely: 1) the Stanford’s Conditional Random-Field-based named entity recognizer (Finkel et al., 2005) to detect entities of type HUMAN; 2) regular expressions to detect NUMERIC and DATE type entities; 3) gazetteers to detect entities of type LOCATION.","For the learning of patterns we used the top 64 documents retrieved by Google and to recognize the named entities in the pattern we apply several strategies, namely: 1) the Stanford’s Conditional Random-Field-based named entity recognizer TREF to detect entities of type HUMAN; 2) regular expressions to detect NUMERIC and DATE type entities; 3) gazetteers to detect entities of type LOCATION."
A,P05-1045,W12-0102,1,"We use Stanford named entity recognizer to extract named entities from the texts (Finkel et al., 2005).",We use Stanford named entity recognizer to extract named entities from the texts TREF.
A,P05-1045,W12-0103,1,"For the annotation pipeline we use the TnT POS tagger (Brants, 2000), WordNet (Fellbaum, 1998), the YamCha chunker (Kudo and Matsumoto, 2003), the Stanford NERC (Finkel et al., 2005), and an in-house temporal expressions recogniser.","For the annotation pipeline we use the TnT POS tagger REF, WordNet REF, the YamCha chun_x005f_x0002_ker REF, the Stanford NERC TREF, and an in-house temporal expressions recogniser."
A,P05-1045,W12-0604,1,"Both the blogs and the Wikipedia articles were tagged using the Stanford Named Entity Recognizer (Finkel et al., 2005), which labels the entities according to these types: Time, Location, Organization, Person, Money, Percent, Date, and Miscellaneous.","Both the blogs and the Wikipedia articles were tagged using the Stanford Named Entity Recog_x0002_nizer TREF, which labels the entities according to these types: Time, Location, Organization, Person, Money, Percent, Date, and Miscellaneous."
A,P05-1045,W12-2007,1,"We use the Stanford Named Entity Recognizer (Finkel et al., 2005) that tags named entities as people, locations, organizations, and miscellaneous.","We use the Stanford Named Entity Recognizer TREF that tags named entities as people, locations, organizations, and miscellaneous."
A,P05-1045,W12-2408,1,"One successful and freely available named entity recognizer is the Stanford NER system (Finkel et al., 2005), which provides an implementation of linear chain CRF sequence models, coupled with well-engineered feature extractors for NER, and trained with newswire documents.","One successful and freely available named entity recognizer is the Stanford NER system TREF, which provides an implementation of linear chain CRF sequence models, coupled with well-engineered feature extractors for NER, and trained with newswire documents."
A,P05-1045,W12-2903,1,"The Stanford tools perform part of speech tagging (Toutanova et al., 2003), constituent and dependency parsing (Klein and Manning, 2003), named entity recognition (Finkel et al., 2005), and coreference resolution (Lee et al., 2011).","The Stanford tools perform part of speech tagging REF, constituent and dependency parsing REF, named entity recognition TREF, and coreference resolution REF."
A,P05-1045,W12-3109,1,"We used the Stanford NER Tagger (Finkel et al., 2005) to detect words that belong to one of four groups: Person, Location, Organization and Misc.","We used the Stanford NER Tagger REF to detect words that belong to one of four groups: Person, Location, Organization and Misc."
A,P05-1045,W13-0905,1,"Our first experiment utilizes WN, VN, and the Stanford Parser (de Marneffe et al., 2006) and Named Entity Recognizer (Finkel et al., 2005).","Our first experiment utilizes WN, VN, and the Stanford Parser REF and Named Entity Recognizer TREF."
A,P05-1045,W13-1101,0,"Given the preponderance of terms on the web, using a named entity recognizer (e.g., Finkel et al., 2005) for preprocessing may also provide benefits.","Given the preponderance of terms on the web, using a named entity recognizer (e.g., TREF) for preprocessing may also provide benefits."
A,P05-1045,W13-2114,1,"We use Stanford NER (Finkel et al., 2005) for named entity recognition.",We use Stanford NER TREF for named entity recognition.
A,P05-1045,W13-2234,1,"We extract further information indicating whether a named entity, as identified by the Stanford NE Recognizer (Finkel et al., 2005) begins at wi.","We extract further information indicating whether a named entity, as identified by the Stanford NE Recognizer TREF begins at wi."
A,P05-1045,W13-2414,0,"In the first more common (Finkel et al., 2005) schema, all PNs are divided into four MUC categories, i.e. person, organization, location and other.","In the first more common TREF schema, all PNs are divided into four MUC categories, i.e. person, organization, location and other."
A,P05-1045,W13-3516,1,"We retrained the Stanford named entity recognizer (Finkel et al., 2005) on the OntoNotes data.",We retrained the Stanford named entity recognizer TREF on the OntoNotes data.
A,P05-1045,W13-3616,1,"External tools we used include WordNet (Fellbaum, 1998) for word base form and noun category generation, Morphg (Minnen et al., 2000) to generate inflections of nouns and verbs, matetools (Bohnet, 2010) for SRL, Stanford-ner (Finkel et al., 2005) for name entity extraction and Longman online dictionary for generation  of noun countability and verb transitivity.","External tools we used include WordNet REF for word base form and noun category generation, Morphg REF to generate inflections of nouns and verbs, mate_x0002_tools REF for SRL, Stanford-ner TREF for name entity extraction and Longman online dictionary for generation  of noun countability and verb transitivity."
A,P05-1045,W13-4008,1,"To extract the extended targets, we capture named entities (NE) from the Wikipedia page of the debate topic (fetched using jsoup java library) using the Stanford Named Entity Recognizer (Finkel et al., 2005) and sort them based on their page occurrence count.","To extract the extended targets, we capture named entities (NE) from the Wikipedia page of the debate topic (fetched using jsoup java library) using the Stanford Named Entity Recognizer TREF and sort them based on their page occurrence count."
B,P05-1045,C10-1064,1,"We utilized an offthe-shelf system, Stanford Named Entity Recognizer (Finkel et al., 2005) for detecting entity mentions on the English sentences.","We utilized an offthe-shelf system, Stanford Named Entity Recog_x0002_nizer TREF for detecting entity mentions on the English sentences."
B,P05-1045,C10-1083,1,"We first extract named entities from scattered opinions DT using Stanford Named Entity Recognizer (Finkel et al., 2005).",We first extract named entities from scattered opinions DT using Stanford Named Entity Recognizer TREF.
B,P05-1045,C10-1087,1,"First, all documents are parsed and processed with standard tools for named entity recognition (Finkel et al., 2005) and coreference resolution.","First, all documents are parsed and processed with standard tools for named entity recognition TREF and coreference resolution."
B,P05-1045,C10-1091,1,"the annotations from the Stanford Named Entity Recognizer (Finkel et al. 2005) that labels PERSON, ORGANIZATION, and LOCATION entities.","the annotations from the Stanford Named Entity Recognizer TREF that labels PERSON, ORGANIZATION, and LOCATION entities."
B,P05-1045,C10-1105,1,"First, if NPi is an NE, we create a feature whose value is the NE label of NPi, as determined by the Stanford CRF-based NE recognizer (Finkel et al., 2005).","First, if NPi is an NE, we create a feature whose value is the NE label of NPi, as determined by the Stanford CRF-based NE recognizer TREF."
B,P05-1045,C10-1131,1,"A large body of literature in joint learning has demonstrated that such an approach can suffer from cascaded errors at testing, and does not benefit from the potential for joint learning (Finkel et al., 2006).","A large body of literature in joint learning has demonstrated that such an approach can suffer from cascaded errors at testing, and does not benefit from the potential for joint learning TREF."
B,P05-1045,C10-2058,0,"Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (Downey et  al., 2005; Sutton and McCallum, 2004; Finkel et  al., 2005; Mann, 2007) experimented with information fusion of relations across multiple  documents.",Some prior work REF demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (GTREF) experimented with information fusion of relations across multiple  documents.
B,P05-1045,C10-2078,1,"To compute the features which we extract in the next section, all instances in our data sets were part-of-speech tagged by the MXPOST tagger (Ratnaparkhi, 1996), parsed with the MaltParser, and named entity tagged with the Stanford NE tagger (Finkel et al., 2005).","To compute the features which we extract in the next section, all instances in our data sets were part-of-speech tagged by the MXPOST tag_x0002_ger REF, parsed with the MaltParser, and named entity tagged with the Stan_x0002_ford NE tagger TREF."
B,P05-1045,C10-3011,1,"Named Entity Recognizer tagging (NER): We integrated Stanford’s NER tagger (Finkel et al., 2005).",Named Entity Recognizer tagging (NER): We integrated Stanford’s NER tagger TREF.
B,P05-1045,D07-1033,3,"For example, non-local features such as “same phrases in a document do not have different entity classes” were shown to be useful in named entity recognition (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005; Krishnan and Manning, 2006).","For example, non-local features such as “same phrases in a document do not have different entity classes” were shown to be useful in named entity recognition (GTREF)."
B,P05-1045,D08-1035,0,"The use of annealing to obtain a maximum a posteriori (MAP) configuration from sampling-based inference is common (e.g., Finkel 2005; Goldwater 2007).","The use of annealing to obtain a maximum a posteriori (MAP) configuration from sampling-based inference is common (e.g., GTREF)."
B,P05-1045,D09-1016,1,Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Finkel et al. (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents.,REF use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. TREF and REF incorporate global information by enforcing event role or label consistency over a document or across related documents.
B,P05-1045,D09-1057,1,"However, due to the lack of a fine grained NER tool at hand, we employ the Stanford NER package (Finkel et al., 2005) which identifies only four types of named entities.","However, due to the lack of a fine grained NER tool at hand, we employ the Stanford NER package TREF which identifies only four types of named entities."
B,P05-1045,D09-1101,1,"Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer (Finkel et al., 2005).",Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer TREF.
B,P05-1045,D09-1119,0,"As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (Zhang et al., 2001; Sha and Pereira, 2003; Finkel et al., 2005; Ratinov and Roth, 2009)","As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (GTREF)"
B,P05-1045,D09-1120,1,"On the MUC6-TEST dataset, our system outperforms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning(2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures","On the MUC6-TEST dataset, our system outperforms both REF (an unsupervised Markov Logic Network system which uses explicit constraints) and TREF (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures"
B,P05-1045,D09-1158,1,"We used the features generated by the CRF package (Finkel et al., 2005). These features include the word string feature, the case feature for the current word, the context words for the current word and their cases, the presence in dictionaries for the current word, the position of the current word in the sentence, prefix and suffix of the current word as well as the case information of the multiple occurrences of the current word.","We used the features generated by the CRF pack_x0002_age TREF. These features include the word string feature, the case feature for the current word, the context words for the current word and their cases, the presence in dictionaries for the current word, the position of the current word in the sentence, prefix and suffix of the current word as well as the case information of the multiple occurrences of the current word."
B,P05-1045,D10-1048,1,"For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al., 2005).","For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) TREF."
B,P05-1045,D10-1099,1,"We preprocess our textual data as follows: We first use the Stanford named entity recognizer (Finkel et al., 2005) to find entity mentions in the corpus. The NER tagger segments each document into sentences and classifies each token into four categories: PERSON, ORGANIZATION, LOCATION and NONE.","We preprocess our textual data as follows: We first use the Stanford named entity recognizer TREF to find entity mentions in the corpus. The NER tagger segments each document into sentences and classifies each token into four categories: PERSON, ORGANIZATION, LOCATION and NONE."
B,P05-1045,D11-1034,1,"Then, we use the Stanford Named Entity Recognizer (Finkel et al., 2005) to identify named entities, which we replace with a unique token (‘NE’).  Next, we replace all nouns with their POS tag; we use the Stanford POS Tagger (Toutanova and Manning, 2000).","Then, we use the Stanford Named Entity Recognizer TREF to identify named entities, which we replace with a unique token (‘NE’).  Next, we replace all nouns with their POS tag; we use the Stanford POS Tagger REF."
B,P05-1045,D11-1072,1,We use the Stanford NER Tagger (Finkel05) to discover these and segment the text accordingly.,We use the Stanford NER Tagger TREF to discover these and segment the text accordingly.
B,P05-1045,D11-1075,1,"We used as candidates all strings labeled in the annotated data as well as all named entities found by the Stanford NER tagger for CoNLL (Finkel et al., 2005).",We used as candidates all strings labeled in the annotated data as well as all named entities found by the Stanford NER tagger for CoNLL TREF.
B,P05-1045,D11-1135,1,"Next we recognize named entities (Finkel et al., 2005) by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags.","Next we recognize named entities TREF by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags."
B,P05-1045,D11-1141,2,"Compared with the state-of-the-art newstrained Stanford Named Entity Recognizer (Finkel et al., 2005), T-SEG obtains a 52% increase in F1 score. In addition there has been been work on SkipChain CRFs (Sutton, 2004; Finkel et al., 2005) which enforce consistency when classifying multiple occurrences of an entity within a document.","Compared with the state-of-the-art newstrained Stanford Named Entity Recognizer TREF, T-SEG obtains a 52% increase in F1 score. In addition there has been been work on SkipChain CRFs (GTREF) which enforce consistency when classifying multiple occurrences of an entity within a document."
B,P05-1045,D11-1144,0,Various modifications to CRF have recently been introduced to take into account of non-local dependencies (Krishnan and Manning 2006) or broader context beyond training data (Du et al. 2010).,Various modifications to CRF have recently been introduced to take into account of non-local dependencies REF or broader context beyond training data TREF.
B,P05-1045,D12-1009,0,"The sampling distributions are annealed, as a search technique to find the best configuration of assignments (Finkel et al., 2005).","The sampling distributions are annealed, as a search technique to find the best configuration of assignments TREF."
B,P05-1045,D12-1042,1,"They used the Stanford named entity recognizer (Finkel et al., 2005) to find entity mentions in text and constructed relation mentions only between entity mentions in the same sentence.",They used the Stanford named entity recognizer TREF to find entity mentions in text and constructed relation mentions only between entity mentions in the same sentence.
B,P05-1045,D12-1072,0,"To identify potential speakers they used the Stanford NER tagger (Finkel et al., 2005) and a method outlined in Davis et al. (2003) that allowed them to find nominal character references.",To identify potential speakers they used the Stanford NER tagger TREF and a method outlined in REF that allowed them to find nominal character references.
B,P05-1045,D12-1076,1,"We use Stanford Named Entity Recognizer(Finkel et al., 2005) to collect named entities which are not in the Wikipedia list.",We use Stanford Named Entity Recognizer TREF to collect named entities which are not in the Wikipedia list.
B,P05-1045,D12-1080,0,"We use the Stanford Core NLP suite (Toutanova et al., 2003; Finkel et al., 2005; Klein and Manning, 2003; Lee et al., 2011) to annotate each document with POS and NER tags, parse trees, and coreference chains.","We use the Stanford Core NLP suite (GTREF) to annotate each document with POS and NER tags, parse trees, and coreference chains."
B,P05-1045,D12-1092,0,"While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Liao and Grishman, 2010; Gupta and Ji, 2009) and crossentity (Hong et al., 2011) information.","While earlier studies focus on sentence-level extraction REF, later ones turn to employ high-level information, such as document (GTREF), cross-event REF and crossentity REF information."
B,P05-1045,D12-1131,0,"For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011).","For parser adaptation, self-training REF, using weakly annotated data from the target domain REF, ensemble learning REF, hierarchical bayesian models TREF and co-training REF achieve substantial performance gains. For a recent survey see REF."
B,P05-1045,D13-1040,1,"After post-processing (tokeniza￾tion, sentence-splitting, and part-of-speech tagging), named entities were automatically recognized and labeled with PER, ORG, LOC, and MISC (Finkel et al., 2005).","After post-processing (tokeniza￾tion, sentence-splitting, and part-of-speech tagging), named entities were automatically recognized and labeled with PER, ORG, LOC, and MISC TREF."
B,P05-1045,D13-1042,1,"Starting with about 752,450 pages, we ran the Stanford NER (Finkel et al., 2005) to mark person spans.","Starting with about 752,450 pages, we ran the Stanford NER TREF to mark person spans."
B,P05-1045,D13-1043,1,"Extracting Named Entities. EXEMPLAR employs the Stanford NER (Finkel et al., 2005) to recognize named entities.",Extracting Named Entities. EXEMPLAR employs the Stanford NER TREF to recognize named entities.
B,P05-1045,D13-1103,1,"The QA-SYS performs Part of Speech tagging using Stanford POS tagger (Toutanova et al., 2003), and Named Entity Recognition using Stanford NER (Finkel et al., 2005), and then builds a Lucene index over the set of input documents.","The QA-SYS performs Part of Speech tagging using Stanford POS tagger REF, and Named Entity Recognition using Stanford NER TREF, and then builds a Lucene index over the set of input documents."
B,P05-1045,D13-1117,1,"For training the CRF model, we used a comprehensive set of features from Finkel et al. (2005) that gives state-of-the-art results on this task.","For training the CRF model, we used a comprehensive set of features from TREF that gives state-of-the-art results on this task."
B,P05-1045,D13-1136,1,"Entities were found using the Stanford named entity tagger (Finkel et al., 2005), and were matched to their name in Freebase.","Entities were found using the Stanford named entity tagger TREF, and were matched to their name in Freebase."
B,P05-1045,D13-1142,0,"NER is a fairly researched field (Finkel et al., 2005; Ratinov et al., 2011; Bunescu and Pasca, 2006; Kulkarni et al., 2009; Milne and Witten, 2008) and is also used in several commercial applications such as Zemanta, OpenCalais and AlchemyAPI , which are able to automatically insert links for a NE pointing to a knowledge base such as Wikipedia or IMDB.","NER is a fairly researched field (GTREF) and is also used in several commercial applications such as Zemanta, OpenCalais and AlchemyAPI , which are able to automatically insert links for a NE pointing to a knowledge base such as Wikipedia or IMDB."
B,P05-1045,D13-1159,2,"A common approach is to utilize a Named Entity Recognition (NER) system like Stanford NER (Finkel et al., 2005), which recognizes the names of things (e.g., person and product names) from texts.","A common approach is to utilize a Named Entity Recognition (NER) system like Stanford NER TREF, which recognizes the names of things (e.g., person and product names) from texts."
B,P05-1045,D13-1178,1,"To assign types to arguments, we apply Stanford Named Entity Recognizer (Finkel et al., 2005) , and also look up the argument in WordNet 2.1 and record the first three senses if they map to our target semantic types.","To assign types to arguments, we apply Stanford Named Entity Recognizer TREF, and also look up the argument in WordNet 2.1 and record the first three senses if they map to our target semantic types."
B,P05-1045,D13-1191,1,"We identified these mentions of persons using Stanford NER (Finkel et al., 2005) and treated each person mention as a single token.",We identified these mentions of persons using Stanford NER TREF and treated each person mention as a single token.
B,P05-1045,E09-1007,2,"Stanford NER (or in short Stanford) associated to the following model provided by the tool and which was trained on different news corpora (CoNLL, MUC6, MUC7 and ACE): ner-eng-ie.crf-3-all2008-distsim.ser.gz (Finkel et al., 2005)","Stanford NER (or in short Stanford) associated to the following model provided by the tool and which was trained on different news corpora (CoNLL, MUC6, MUC7 and ACE): ner-eng-ie.crf-3-all2008-distsim.ser.gz TREF"
B,P05-1045,E09-1011,1,"We then proceed to split the data into smaller sentences and tag them using Ratnaparkhi’s Maximum Entropy Tagger (Ratnaparkhi, 1996). We parse the data using the Collins Parser (Collins, 1997), and then tag person, location and organization names using the Stanford Named Entity Recognizer (Finkel et al., 2005).","We then proceed to split the data into smaller sentences and tag them using Ratnaparkhi’s Maximum Entropy Tag_x0002_ger REF. We parse the data using the Collins Parser REF, and then tag person, location and organization names using the Stanford Named Entity Recognizer TREF."
B,P05-1045,E09-1037,0,"Some stem from work on graphical models, including loopy belief propagation (Sutton and McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006).","Some stem from work on graphical models, including loopy belief propagation REF, Gibbs sampling TREF, sequential Monte Carlo methods such as particle filtering REF, and variational inference REF."
B,P05-1045,E09-1091,1,"In all the experiments, our source side language is English, and the Stanford Named Entity Recognizer (Finkel et al, 2005) was used to extract NEs from the source side article.","In all the experiments, our source side language is English, and the Stanford Named Entity Recognizer TREF was used to extract NEs from the source side article."
B,P05-1045,E12-1029,1,"We also use the Stanford NER tagger (Finkel et al., 2005) to identify Named Entities within the NP.",We also use the Stanford NER tagger TREF to identify Named Entities within the NP.
B,P05-1045,E12-1033,1,"Named-entity information is provided by Stanford Tagger (Finkel et al., 2005).",Named-entity information is provided by Stanford Tagger TREF.
B,P05-1045,E12-1054,1,"Named Entity The replaced token should not be part of a named entity. For this purpose, we applied the Stanford NER (Finkel et al., 2005).","Named Entity The replaced token should not be part of a named entity. For this purpose, we applied the Stanford NER TREF."
B,P05-1045,I08-4013,0,"In the first approach, heuristic rules are used to find the dependencies (Bunescu and Mooney, 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al., 2005).","In the first approach, heuristic rules are used to find the dependencies REF or penalties for label inconsistency are required to handset ad-hoc TREF."
B,P05-1045,I08-5014,1,"We have used the freely available Stanford Named Entity Recognizer (Finkel, Grenager, and Manning, 2005) in our engine.",We have used the freely available Stanford Named Entity Recognizer (GTREF) in our engine.
B,P05-1045,I08-6004,1,"From the above corpora, we first extracted all the NEs from the English side, using the Stanford NER tool [Finkel et al, 2005].","From the above corpora, we first extracted all the NEs from the English side, using the Stanford NER tool TREF."
B,P05-1045,J11-1002,0,"Furthermore, as our tasks can be regarded as a sequential labeling problem (to label if a word is an opinion word, a target, or an ordinary word), we experimented with the conditional random fields (CRF) technique (Lafferty, McCallum, and Pereira 2001) for extraction, which is a popular information extraction method and has been successfully used in labeling tasks such as POS tagging (Lafferty, McCallum, and Pereira 2001) and Named Entity Recognition (Finkel, Grenager, and Manning 2005).","Furthermore, as our tasks can be regarded as a sequential labeling problem (to label if a word is an opinion word, a target, or an ordinary word), we experimented with the conditional random fields (CRF) technique REF for extraction, which is a popular information extraction method and has been successfully used in labeling tasks such as POS tagging REF and Named Entity Recognition (GTREF)."
B,P05-1045,J12-4004,1,"Then, we use the Stanford Named Entity Recognizer (Finkel, Grenager, and Manning 2005) to identify named entities, which we replace with a unique token (‘NE’).","Then, we use the Stanford Named Entity Recognizer (GTREF) to identify named entities, which we replace with a unique token (‘NE’)."
B,P05-1045,J13-2001,2,"First, we test the NE alignment performance with the same Chinese NE recognizer (Wu’s system, adopted earlier) but with different English NE recognizers that include the Mallet toolkit (used before), the Stanford NE recognizer (Finkel, Grenager, and Manning 2005), and Minor Third (Cohen 2004).","First, we test the NE alignment performance with the same Chinese NE recognizer (Wu’s system, adopted earlier) but with different English NE recognizers that include the Mallet toolkit (used before), the Stanford NE recognizer (GTREF), and Minor Third REF."
B,P05-1045,N06-1054,2,"Such techniques include Gibbs sampling (Finkel et al., 2005), a general-purpose Monte Carlo method, and integer linear programming (ILP), (Roth and Yih, 2005), a general-purpose exact framework for NP-complete problems. For named entity recognition, a phrase that appears multiple times should tend to get the same label each time (Finkel et al., 2005). In seminar announcements, a given field (speaker, start time, etc.) should appear with at most one value in each announcement, although the field and value may be repeated (Finkel et al., 2005).","Such techniques include Gibbs sampling TREF, a general-purpose Monte Carlo method, and integer linear programming (ILP), REF, a general-purpose exact framework for NP-complete problems. For named entity recognition, a phrase that appears multiple times should tend to get the same label each time TREF. In seminar announcements, a given field (speaker, start time, etc.) should appear with at most one value in each announcement, although the field and value may be repeated TREF."
B,P05-1045,N07-1009,0,"Undirected graphical models such as Conditional Random Fields (CRFs) (Lafferty et al., 2001) have shown great success for problems involving structured output variables (e.g. Wellner et al. (2004), Finkel et al. (2005)).","Undirected graphical models such as Conditional Random Fields (CRFs) REF have shown great success for problems involving structured output variables (e.g. REF, TREF)."
B,P05-1045,N07-1011,0,"Others have attempted to train global scoring functions using Gibbs sampling (Finkel et al., 2005), message propagation, (Bunescu and Mooney, 2004; Sutton and McCallum, 2004), and integer linear programming (Roth and Yih, 2004).","Others have attempted to train global scoring functions using Gibbs sampling TREF, message propagation, REF, and integer linear programming REF."
B,P05-1045,N07-1042,0,"More distantly related, Sutton and McCallum (2004) and Finkel et al. (2005) propose graphical models for combining information about a given entity from multiple mentions.","More distantly related, REF and TREF propose graphical models for combining information about a given entity from multiple mentions."
B,P05-1045,N07-2046,0,"Recent applications of statistical coreference models are beginning to show promise (Finkel et al, 2005; Ji & Grishman, 2005).",Recent applications of statistical coreference models are beginning to show promise (GTREF).
B,P05-1045,N09-1037,1,"For the named entity features, we used a fairly standard feature set, similar to those described in (Finkel et al., 2005).","For the named entity features, we used a fairly standard feature set, similar to those described in TREF."
B,P05-1045,N09-1068,1,"Our features were based on those in (Finkel et al., 2005).",Our features were based on those in TREF.
B,P05-1045,N10-1068,0,"We focus on training using Gibbs sampling (Geman and Geman, 1984), because it has been popularly applied in the natural language literature, e.g., (Finkel et al., 2005; DeNero et al., 2008; Blunsom et al., 2009).","We focus on training using Gibbs sampling REF, because it has been popularly applied in the natural language literature, e.g., (GTREF)."
B,P05-1045,N10-1072,1,"C’s nameEntitySet represents the set of all named entities in C’s article labeled by Stanford NER (Finkel et al., 2005).",C’s nameEntitySet represents the set of all named entities in C’s article labeled by Stanford NER TREF.
B,P05-1045,N10-1117,0,"One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling (Finkel et al., 2005), another is (loopy) sum-product belief propagation (Smith and Eisner, 2008).","One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling TREF, another is (loopy) sum-product belief propagation REF."
B,P05-1045,N10-1121,1,"Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005).",Namedentity information was obtained by the Stanford tag_x0002_ger TREF.
B,P05-1045,N12-1008,2,"Global Constraints Previous work demonstrated the benefits of applying declarative constraints in information extraction (Finkel et al., 2005; Roth and tau Yih, 2004; Chang et al., 2007; Druck and McCallum, 2010).",Global Constraints Previous work demonstrated the benefits of applying declarative constraints in information extraction (GTREF).
B,P05-1045,N12-1013,1,"Global Constraints Previous work demonstrated the benefits of applying declarative constraints in information extraction (Finkel et al., 2005; Roth and tau Yih, 2004; Chang et al., 2007; Druck and McCallum, 2010).",Global Constraints Previous work demonstrated the benefits of applying declarative constraints in information extraction (GTREF).
B,P05-1045,N12-1065,1,"The Stanford named entity recognition (NER) software (Finkel et al., 2005) is an implementation of linear chain Conditional Random Field (CRF) sequence models, which includes a three class (person, organization, location and other) named entity recognizer for English.","The Stanford named entity recognition (NER) software TREF is an implementation of linear chain Conditional Random Field (CRF) sequence models, which includes a three class (person, organization, location and other) named entity recognizer for English."
B,P05-1045,N12-1080,1,"For identification of named entities, we use Stanford NER (Finkel et al., 2005).","For identification of named entities, we use Stanford NER TREF."
B,P05-1045,N12-1084,1,"It uses Named Entity Recognition (NER) (Finkel, Grenager, and Manning, 2005; Ratinov and Roth, 2009; Ritter et al., 2011) as a subroutine to identify named person entities, though we are also interested in unnamed persons such as “my teacher” and pronouns.","It uses Named Entity Recognition (NER) (GTREF) as a subroutine to identify named person entities, though we are also interested in unnamed persons such as “my teacher” and pronouns."
B,P05-1045,N12-1085,0,"More specifically, we seek a posterior distribution over latent variables that partition words in a sentence into flow and inert groups; we estimate this posterior using Gibbs sampling (Finkel et al., 2005).","More specifically, we seek a posterior distribution over latent variables that partition words in a sentence into flow and inert groups; we estimate this posterior using Gibbs sampling TREF."
B,P05-1045,N13-1006,1,English features were taken from Finkel et al. (2005).,English features were taken from TREF.
B,P05-1045,N13-1007,1,"We used Stanford NER (Finkel et al., 2005) for English named entity recognition (NER), KNP for Japanese NER, and BaseNER (Zhao and Kit, 2008) for Chinese NER.","We used Stanford NER TREF for English named entity recognition (NER), KNP for Japanese NER, and BaseNER REF for Chinese NER."
B,P05-1045,N13-1037,0,"In part-of-speech tagging, the accuracy of the Stanford tagger (Toutanova et al., 2003) falls from 97% on Wall Street Journal text to 85% accuracy on Twitter (Gimpel et al., 2011). In named entity recognition, the CoNLL-trained Stanford recognizer achieves 44% F-measure (Ritter et al., 2011), down from 86% on the CoNLL test set (Finkel et al., 2005).","In part-of-speech tagging, the accuracy of the Stanford tagger REF falls from 97% on Wall Street Journal text to 85% accuracy on Twitter REF. In named entity recognition, the CoNLL-trained Stanford recognizer achieves 44% F-measure REF, down from 86% on the CoNLL test set TREF."
B,P05-1045,N13-1059,1,"For syntactic analysis we use the Stanford Parser (Finkel et al., 2005).",For syntactic analysis we use the Stanford Parser TREF.
B,P05-1045,P06-1059,2,"In NER tasks, however, informa￾tion about a distant entity is often more useful than information about the previous state (Finkel et al.,2005).","In NER tasks, however, informa￾tion about a distant entity is often more useful than information about the previous state TREF."
B,P05-1045,P06-1089,2,"Global information is known to be useful in other NLP tasks, especially in the named entity recognition task, and several studies successfully used global features (Chieu and Ng, 2002; Finkel et al., 2005)","Global information is known to be useful in other NLP tasks, especially in the named entity recognition task, and several studies successfully used global features TREF"
B,P05-1045,P06-1141,3,"Finkel et al. (2005) hand-set penalties for inconsistency in entity labeling at different occurrences in the text, based on some statistics from training data.","TREF hand-set penalties for inconsistency in entity labeling at different occurrences in the text, based on some statistics from training data."
B,P05-1045,P06-2054,0,"An additional consistent edge of a linear-chain conditional random field (CRF) explicitly models the dependencies between distant occurrences of similar words (Sutton and McCallum, 2004; Finkel et al., 2005).",An additional consistent edge of a linear-chain conditional random field (CRF) explicitly models the dependencies between distant occurrences of similar words (GTREF).
B,P05-1045,P08-1031,0,"We employ Gibbs sampling, previously used in NLP by Finkel et al. (2005) and Goldwater et al. (2006), among others.","We employ Gibbs sampling, previously used in NLP by TREF and REF, among others."
B,P05-1045,P08-2012,1,We added named entity (NE) tags to the data using the tagger of Finkel et al. (2005).,We added named entity (NE) tags to the data using the tagger of TREF.
B,P05-1045,P08-4003,1,"Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors.","Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger REF, the YamCha chunker REF and the Stanford Named Entity Rec_x0002_ognizer TREF, the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser REF to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors."
B,P05-1045,P09-1113,1,"We perform named entity tagging using the Stanford four-class named entity tagger (Finkel et al., 2005).",We perform named entity tagging using the Stanford four-class named entity tagger TREF.
B,P05-1045,P09-2041,1,"To implement this method, we first use the Stanford Named Entity Recognizer (Finkel et al., 2005) to identify the set of person and organisation entities, E, from each article in the corpus.","To implement this method, we first use the Stanford Named Entity Recognizer TREF to identify the set of person and organisation entities, E, from each article in the corpus."
B,P05-1045,P10-1015,1,"We processed each novel with the Stanford NER tagger (Finkel et al., 2005) and extracted noun phrases that were categorized as persons or organizations.",We processed each novel with the Stanford NER tagger TREF and extracted noun phrases that were categorized as persons or organizations.
B,P05-1045,P10-1056,1,"We run the Stanford Named Entity Recognizer (Finkel et al., 2005) and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs.","We run the Stanford Named Entity Recognizer TREF and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs."
B,P05-1045,P10-1148,1,"That is, a non-animate noun can hardly  constitute an experience. In order to make a distinction, we use the dependency parser and a named-entity recognizer (Finkel et al., 2005) that can recognize person pronouns and person names.","That is, a non-animate noun can hardly  constitute an experience. In order to make a distinction, we use the dependency parser and a named-entity recognizer TREF that can recognize person pronouns and person names."
B,P05-1045,P10-2029,1,"NE OpenNLP (Baldridge, J., 2005) Recognizer Stanford (Finkel et al., 2005)",NE OpenNLP REF Recognizer Stanford TREF
B,P05-1045,P10-2049,1,"To reproduce the system by Zhuang et al. (2006), we substitute the cast and crew list employed by them (see Section 3.2), with a NER component (Finkel et al., 2005).","To reproduce the system by REF, we substitute the cast and crew list employed by them (see Section 3.2), with a NER component TREF."
B,P05-1045,P11-1037,1,"Proposed solutions to NER fall into three categories: 1) The rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010) ; and 3) hybrid methods (Jansche and Abney, 2002).",Proposed solutions to NER fall into three categories: 1) The rule-based REF; 2) the machine learning based (TREF) ; and 3) hybrid methods REF.
B,P05-1045,P11-1082,1,"Since knowledge extraction from webbased encyclopedia is typically noisy (Ponzetto and Poesio, 2009), we use YAGO to determine whether two NPs have a relation only if one NP is a named entity (NE) of type person, organization, or location according to the Stanford NE recognizer (Finkel et al., 2005) and the other NP is a common noun.","Since knowledge extraction from webbased encyclopedia is typically noisy REF, we use YAGO to determine whether two NPs have a relation only if one NP is a named entity (NE) of type person, organization, or location according to the Stanford NE recognizer TREF and the other NP is a common noun."
B,P05-1045,P11-1113,0,"Finkel et al. (2005) used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models.","TREF used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models."
B,P05-1045,P11-1114,1,"Semantic features: we use the Stanford NER tagger (Finkel et al., 2005) to determine if the targeted NP is a named entity, and we use the Sundance parser (Riloff and Phillips, 2004) to assign semantic class labels to each NP’s head noun.","Semantic features: we use the Stanford NER tag_x0002_ger TREF to determine if the targeted NP is a named entity, and we use the Sundance parser REF to assign semantic class labels to each NP’s head noun."
B,P05-1045,P11-3004,1,"To created we extract all NEs from the text using the Stanford NE Recognizer (Finkel et al., 2005) and represent each NE by its Wikipedia URI.",To created we extract all NEs from the text using the Stanford NE Recognizer TREF and represent each NE by its Wikipedia URI.
B,P05-1045,P12-1042,1,"We use the Stanford Named Entity Recognizer (Finkel et al., 2005) for this purpose.",We use the Stanford Named Entity Recognizer TREF for this purpose.
B,P05-1045,P12-1072,1,"Due to the large size of the corpora, we uniformly sampled a subset of documents for each corpus and ran the Stanford NER tagger (Finkel et al., 2005), which tagged named entities mentions as person, location, and organization.","Due to the large size of the corpora, we uniformly sampled a subset of documents for each corpus and ran the Stanford NER tagger TREF, which tagged named entities mentions as person, location, and organization."
B,P05-1045,P12-1073,1,"The Figure also shows the results of the Stanford NER tagger for English (Finkel et al., 2005) (we used the MUC-7 classifier).",The Figure also shows the results of the Stanford NER tagger for English TREF (we used the MUC-7 classifier).
B,P05-1045,P12-1075,1,"Following (Yao et al., 2011), we filter out noisy documents and use natural language packages to annotate the documents, including NER tagging (Finkel et al., 2005) and dependency parsing (Nivre et al., 2004).","Following REF, we filter out noisy documents and use natural language packages to annotate the documents, including NER tagging TREF and dependency parsing REF."
B,P05-1045,P12-1087,1,"Run Stanford CoreNLP with POS tagging and named entity recognition (Finkel et al., 2005)",Run Stanford CoreNLP with POS tagging and named entity recognition TREF
B,P05-1045,P12-1089,3,"Mainly, predetermined word-level dependencies were represented as links in the underlying graphical model (Sutton and McCallum, 2004; Finkel et al., 2005).","Mainly, predetermined word-level dependencies were represented as links in the underlying graphical model (GTREF)."
B,P05-1045,P12-2013,1,"Accordingly, we extract all the named entities in a sentence using Stanford’s Name Entity Recognizer (Finkel et al., 2005).","Accordingly, we extract all the named entities in a sentence using Stanford’s Name Entity Recognizer TREF."
B,P05-1045,P12-2024,1,"Similarly, the semantic phrase table (SPT) has been ex-tracted from the same corpora annotated with the Stanford NE tagger (Faruqui and Pado, 2010; Finkel et al., 2005).","Similarly, the semantic phrase table (SPT) has been ex-tracted from the same corpora annotated with the Stanford NE tagger (GTREF)."
B,P05-1045,P12-2064,1,"We use the Stanford coreNLP toolkit1 (Toutanova and Manning, 2000; Klein and Manning, 2003a; Klein and Manning, 2003b; Finkel et al, 2005) to extract  the features.",We use the Stanford coreNLP toolkit1 (GTREF) to extract  the features.
B,P05-1045,P12-2069,1,"It includes: cleaning up and normalization of the input using regular expressions, sentence segmentation, tokenization and lemmatization using GATE (Cunningham et al., 2002), syntactical parsing and dependency parsing (collapsed) using the Stanford Parser (de Marneffe et al., 2006), and Named Entity Recognition using Stanford NER (Finkel et al., 2005).","It includes: cleaning up and normalization of the input using regular expressions, sentence segmentation, tokenization and lemmatization using GATE REF, syntactical parsing and dependency parsing (collapsed) using the Stanford Parser REF, and Named Entity Recognition using Stanford NER TREF."
B,P05-1045,P12-3003,0,"For example, the average F1 of the Stanford NER (Finkel et al., 2005) drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets, while Liu et al. (2010) report that the F1 score of a state-ofthe-art SRL system (Meza-Ruiz and Riedel, 2009) falls to 42.5% on tweets as apposed to 75.5% on news.","For example, the average F1 of the Stanford NER TREF drops from 90.8% REF to 45.8% on tweets, while Liu et al. (2010) report that the F1 score of a state-ofthe-art SRL system REF falls to 42.5% on tweets as apposed to 75.5% on news."
B,P05-1045,P12-3013,1,"BIUTEE provides state-of-the-art pre-processing utilities: Easy-First parser (Goldberg and Elhadad, 2010), Stanford named-entity-recognizer (Finkel et al., 2005) and ArkRef coreference resolver (Haghighi and Klein, 2009), as well as utilities for sentencesplitting and numerical-normalizations.","BIUTEE provides state-of-the-art pre-processing utilities: Easy-First parser REF, Stanford named-entity-recognizer TREF and ArkRef coreference resolver REF, as well as utilities for sentencesplitting and numerical-normalizations."
B,P05-1045,P13-1106,1,"The Stanford CRF-based NER tagger was used as the monolingual component in our models (Finkel et al., 2005).",The Stanford CRF-based NER tagger was used as the monolingual component in our models TREF.
B,P05-1045,P13-1129,1,"We utilize the Stanford tools (Toutanova et al., 2003; Finkel et al., 2005; Marneffe et al., 2006).",We utilize the Stanford tools (GTREF).
B,P05-1045,P13-1146,0,"Most well-known is the Stanford named entity recognition (NER) tagger (Finkel 2005) which assigns coarse-grained types like person, organization, location, and other to noun  phrases that are likely to denote entities.","Most well-known is the Stanford named entity recognition (NER) tagger TREF which assigns coarse-grained types like person, organization, location, and other to noun  phrases that are likely to denote entities."
B,P05-1045,P13-1166,1,"When training the Stanford NER system (Finkel et al., 2005) on just the tokens from the Freire data set and the parameters from english.all.3class.distsim.prop (included in the Stanford NER release, see also Van Erp and Van der Meij (2013)), our F-scores come very close to those reported by Freire et al. (2012), but mostly with a higher recall and lower precision.","When training the Stanford NER system TREF on just the tokens from the Freire data set and the parameters from english.all.3class.distsim.prop (included in the Stanford NER release, see also REF), our F-scores come very close to those reported by REF, but mostly with a higher recall and lower precision."
B,P05-1045,P13-2036,1,"To identify entities, we use a CRF-based named entity tagger (Finkel et al., 2005) and a Chinese word breaker (Gao et al., 2003) for English and Chinese corpora, respectively.","To identify entities, we use a CRF-based named entity tagger TREF and a Chinese word breaker REF for English and Chinese corpora, respectively."
B,P05-1045,P13-2116,1,"We validate the hypothesis that using linguistic features, e.g., part-of-speech tags (Toutanova and Manning, 2000), named-entity tags (Finkel et al., 2005), and dependency trees (de Marneffe et al., 2006),helps improve the quality of our approach, called Joint.","We validate the hypothesis that using linguistic features, e.g., part-of-speech tags REF, named-entity tags TREF, and dependency trees REF,helps improve the quality of our approach, called Joint."
B,P05-1045,P13-2128,0,"Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005).","Similarly, distributional features support generalization in Named Entity Recognition TREF."
B,P05-1045,P13-2141,1,"We used Open NLP POS tagger, Standford NER (Finkel et al., 2005) and MaltParser (Nivre et al., 2006) to label/tag sentences.","We used Open NLP POS tagger, Standford NER TREF and MaltParser REF to label/tag sentences."
B,P05-1045,P13-4023,0,"State-of-the-art tools for named entity recognition such as the Stanford NER Tagger (Finkel et al., 2005) compute semantic tags only for a small set of coarse-grained types: Person, Location, and Organization (plus tags for non-entity phrases of type time, money, percent, and date).","State-of-the-art tools for named entity recognition such as the Stanford NER Tagger TREF compute semantic tags only for a small set of coarse-grained types: Person, Location, and Organization (plus tags for non-entity phrases of type time, money, percent, and date)."
B,P05-1045,Q13-1028,1,"The first is named entity (NE) tags (PERSON, ORGANIZATION and LOCATION) returned by the Stanford NE recognition tool (Finkel et al., 2005).","The first is named entity (NE) tags (PERSON, ORGANIZATION and LOCATION) returned by the Stanford NE recognition tool TREF."
B,P05-1045,S10-1020,1,"Corry has only participated in the “open” setting, as it has already a number of preprocessing modules integrated into the system: the Stanford NLP toolkit for parsing (Klein and Manning, 2003) and NE-tagging (Finkel et al., 2005), Wordnet for semantic classes and the U.S. census data for assigning gender values to person names.","Corry has only participated in the “open” setting, as it has already a number of preprocessing modules integrated into the system: the Stanford NLP toolkit for parsing REF and NE-tagging TREF, Wordnet for semantic classes and the U.S. census data for assigning gender values to person names."
B,P05-1045,S12-1035,1,"Named Entities using the Stanford named entity recognizer recognizer (Finkel et al., 2005)",Named Entities using the Stanford named entity recognizer recognizer TREF
B,P05-1045,S12-1082,1,"The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002).","The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (GTREF)."
B,P05-1045,S12-1085,1,"For capturing and normalizing the above mentioned expressions, we make use of the Stanford NER Toolkit (Finkel et al., 2005).","For capturing and normalizing the above mentioned expressions, we make use of the Stanford NER Toolkit TREF."
B,P05-1045,S13-1010,1,"Named entity recognition The Stanford Named Entity Recogniser (Finkel et al., 2005) was run over the BNC and any nouns that occur in the corpus with both named entity and non-named entity tags are extracted to form the list Stanford.",Named entity recognition The Stanford Named Entity Recogniser TREF was run over the BNC and any nouns that occur in the corpus with both named entity and non-named entity tags are extracted to form the list Stanford.
B,P05-1045,S13-1014,1,"The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002).","The Ia(i, j) metric using the estimated value of α = 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (GTREF)."
B,P05-1045,S13-1018,1,"We analysed the text in the metadata, performing lemmatization, PoS tagging, named entity recognition and classification (NERC) and date detection using Stanford CoreNLP (Finkel et al., 2005; Toutanova et al., 2003).","We analysed the text in the metadata, performing lemmatization, PoS tagging, named entity recognition and classification (NERC) and date detection using Stanford CoreNLP (GTREF)."
B,P05-1045,S13-1021,1,"In our experiments, we performed named entity recognition with the Stanford NER tool using the standard English model (Finkel et al., 2005).","In our experiments, we performed named entity recognition with the Stanford NER tool using the standard English model TREF."
B,P05-1045,S13-1023,1,"We used the Stanford Named Entity Recognizer by (Finkel et al., 2005), with the 7 class model trained for MUC: Time, Location, Organization, Person, Money, Percent, Date.","We used the Stanford Named Entity Recognizer by TREF, with the 7 class model trained for MUC: Time, Location, Organization, Person, Money, Percent, Date."
B,P05-1045,W06-1643,0,"Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques, including TRP (Sutton and McCallum, 2004) and Gibbs sampling (Finkel et al., 2005).","Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques, including TRP REF and Gibbs sampling TREF."
B,P05-1045,W06-1655,0,"There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (Finkel et al., 2005; Culotta et al., 2005; Sha and Pereira, 2003).","There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (GTREF)."
B,P05-1045,W09-0422,1,"One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer (Finkel et al., 2005).",One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer TREF.
B,P05-1045,W09-1119,2,"We have chosen to compare against the Stanford tagger because to the best of our knowledge, it is the best publicly available system which is trained on the same data. We have downloaded the Stanford NER tagger and used the strongest provided model trained on the CoNLL03 data with distributional similarity features. The results we obtained on the CoNLL03 test set were consistent with what was reported in (Finkel et al., 2005).","We have chosen to compare against the Stanford tagger because to the best of our knowledge, it is the best publicly available system which is trained on the same data. We have downloaded the Stanford NER tagger and used the strongest provided model trained on the CoNLL03 data with distributional similarity features. The results we obtained on the CoNLL03 test set were consistent with what was reported in TREF."
B,P05-1045,W09-1218,0,"We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007) and dependency parsing (Nakagawa, 2007) with a great deal of success.",We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (GTREF) and dependency parsing REF with a great deal of success.
B,P05-1045,W10-0725,1,"The texts we use in our experiments are the development set of the RTE-5 challenge (Bentivogli et al., 2009), and we preprocess the data using the Stanford named-entity recognizer (Finkel et al., 2005).","The texts we use in our experiments are the development set of the RTE-5 challenge REF, and we preprocess the data using the Stanford named-entity recognizer TREF."
B,P05-1045,W10-1902,0,Finkel et al. (2005) used Gibbs Sampling to add non-local dependencies into linear-chain CRF model for information extraction.,TREF used Gibbs Sampling to add non-local dependencies into linear-chain CRF model for information extraction.
B,P05-1045,W10-2420,1,"In comparison, there are nu￾merous Named Entity recognition (NER) sys￾tems, both general-purpose and specialized, and many of them achieve scores better than F1 = 0.95 (Ratinov and Roth, 2009; Finkel et al., 2005)","In comparison, there are nu￾merous Named Entity recognition (NER) sys￾tems, both general-purpose and specialized, and many of them achieve scores better than F1 = 0.95 (GTREF)"
B,P05-1045,W10-2906,1,"We used the Stanford NER tagger (Finkel et al., 2005) with its default configuration as our full monolingual model for each language.",We used the Stanford NER tagger TREF with its default configuration as our full monolingual model for each language.
B,P05-1045,W10-3102,2,"The Stanford Named Entity Recognizer (NER) is based on the machine learning algorithm Conditional Random Fields (Finkel et al., 2005) and has been used extensively for identifying named entities in news text.",The Stanford Named Entity Recognizer (NER) is based on the machine learning algorithm Conditional Random Fields TREF and has been used extensively for identifying named entities in news text.
B,P05-1045,W11-0219,1,"These include several off-the-shelf statisical NLP tools such as the Stanford POS tagger 
(Toutanova and Manning, 2000), the Stanford named-entity recognizer (NER) (Finkel et al., 2005) and the Stanford Parser (Klein and Manning, 2003).","These include several off-the-shelf statisical NLP tools such as the Stanford POS tagger 
REF, the Stanford named-entity recognizer (NER) TREF and the Stanford Parser REF."
B,P05-1045,W11-0810,2,"The Stanford Tagger is based on Conditional Random Fields (Finkel et al., 2005).",The Stanford Tagger is based on Conditional Random Fields TREF.
B,P05-1045,W11-0902,1,"For EMD we used the Stanford named entity recognizer (Finkel et al., 2005).",For EMD we used the Stanford named entity recognizer TREF.
B,P05-1045,W11-1513,1,"Next, the spelling corrected dataset is run through the Stanford Named Entity Recognizer (NER). Stanford NER system first detects sentences in the data then labels four classes of named entities: PERSON, ORGANIZATION, LOCATION, and MISCELLANEOUS (Finkel et al., 2005).","Next, the spelling corrected dataset is run through the Stanford Named Entity Recognizer (NER). Stanford NER system first detects sentences in the data then labels four classes of named entities: PERSON, ORGANIZATION, LOCATION, and MISCELLANEOUS TREF."
B,P05-1045,W11-1723,0,"The preprocessor merges all the files into one but defines start/end delimiters for each file in the merged file, to enable bulk processing. The merged news file is acted upon by a log-linear part of speech tagger we obtained from the Stanford NLP webpage(Manning,2000).","The preprocessor merges all the files into one but defines start/end delimiters for each file in the merged file, to enable bulk processing. The merged news file is acted upon by a log-linear part of speech tagger we obtained from the Stanford NLP webpage TREF."
B,P05-1045,W11-1907,1,"We use the Stanford MaxentTagger (Toutanova et al., 2003) for partof-speech tagging, and the Stanford Named Entity Recognizer (Finkel et al., 2005) for annotating named entities.","We use the Stanford MaxentTagger REF for partof-speech tagging, and the Stanford Named Entity Recognizer TREF for annotating named entities."
B,P05-1045,W11-1908,1,"First, we create a list of candidate mentions by merging basic NP chunks with named entities. NP chunks are computed from the parse trees provided in the CoNLL distribution, Named entities are extracted with the Stanford NER tool (Finkel et al., 2005).","First, we create a list of candidate mentions by merging basic NP chunks with named entities. NP chunks are computed from the parse trees provided in the CoNLL distribution, Named entities are extracted with the Stanford NER tool TREF."
B,P05-1045,W11-2202,1,"Evaluation was performed on a corpus of blogs describing United States politics in 2008 (Eisenstein and Xing, 2010). We ran the Stanford Named Entity Recognition system (Finkel et al., 2005) to obtain a set of 25,000 candidate mentions which the system judged to be names of people.","Evaluation was performed on a corpus of blogs describing United States politics in 2008 REF. We ran the Stanford Named Entity Recognition system TREF to obtain a set of 25,000 candidate mentions which the system judged to be names of people."
B,P05-1045,W11-2705,1,"For the learning of patterns we used the top 64 documents retrieved by Google and to recognize the named entities in the pattern we apply several strategies, namely: 1) the Stanford’s Conditional Random-Field-based named entity recognizer (Finkel et al., 2005) to detect entities of type HUMAN; 2) regular expressions to detect NUMERIC and DATE type entities; 3) gazetteers to detect entities of type LOCATION.","For the learning of patterns we used the top 64 documents retrieved by Google and to recognize the named entities in the pattern we apply several strategies, namely: 1) the Stanford’s Conditional Random-Field-based named entity recog_x0002_nizer TREF to detect entities of type HUMAN; 2) regular expressions to detect NUMERIC and DATE type entities; 3) gazetteers to detect entities of type LOCATION."
B,P05-1045,W12-0102,1,"If more named entities co-occur in two documents, they are very likely to talk about the same event or subject and thus should be more comparable. We use Stanford named entity recognizer to extract named entities from the texts (Finkel et al.,2005)","If more named entities co-occur in two documents, they are very likely to talk about the same event or subject and thus should be more comparable. We use Stanford named entity recognizer to extract named entities from the texts TREF"
B,P05-1045,W12-0103,1,"For the annotation pipeline we use the TnT POS tagger (Brants, 2000), WordNet (Fellbaum, 1998), the YamCha chunker (Kudo and Matsumoto, 2003), the Stanford NERC (Finkel et al., 2005), and an in-house temporal expressions recogniser.","For the annotation pipeline we use the TnT POS tagger REF, WordNet REF, the YamCha chun_x0002_ker REF, the Stanford NERC TREF, and an in-house temporal expressions recogniser."
B,P05-1045,W12-0604,1,"Both the blogs and the Wikipedia articles were tagged using the Stanford Named Entity Recognizer (Finkel et al., 2005), which labels the entities according to these types: Time, Location, Organization, Person, Money, Percent, Date, and Miscellaneous.","Both the blogs and the Wikipedia articles were tagged using the Stanford Named Entity Recognizer TREF, which labels the entities according to these types: Time, Location, Organization, Person, Money, Percent, Date, and Miscellaneous."
B,P05-1045,W12-2007,1,"We use the Stanford Named Entity Recognizer (Finkel et al., 2005) that tags named entities as people, locations, organizations, and miscellaneous.","We use the Stanford Named Entity Recognizer TREF that tags named entities as people, locations, organizations, and miscellaneous."
B,P05-1045,W12-2408,1,"One successful and freely available named entity recognizer is the Stanford NER system (Finkel et al., 2005), which provides an implementation of linear chain CRF sequence models, coupled with well-engineered feature extractors for NER, and trained with newswire documents.","One successful and freely available named entity recognizer is the Stanford NER system TREF, which provides an implementation of linear chain CRF sequence models, coupled with well-engineered feature extractors for NER, and trained with newswire documents."
B,P05-1045,W12-2903,1,"Each dialog was processed using the Stanford Core NLP tools. The Stanford tools perform part of speech tagging (Toutanova et al., 2003), constituent and dependency parsing (Klein and Manning, 2003), named entity recognition (Finkel et al., 2005), and coreference resolution (Lee et al., 2011)","Each dialog was processed using the Stanford Core NLP tools. The Stanford tools perform part of speech tagging REF, constituent and dependency parsing REF, named entity recognition TREF, and coreference resolution REF"
B,P05-1045,W12-3109,1,"We used the Stanford NER Tagger (Finkel et al., 2005) to detect words that belong to one of four groups: Person, Location, Organization and Misc. Each group is represented by a binary feature.","We used the Stanford NER Tagger TREF to detect words that belong to one of four groups: Person, Location, Organization and Misc. Each group is represented by a binary feature."
B,P05-1045,W13-0905,1,"Our first experiment utilizes WN, VN, and the Stanford Parser (de Marneffe et al., 2006) and Named Entity Recognizer (Finkel et al., 2005).","Our first experiment utilizes WN, VN, and the Stanford Parser REF and Named Entity Recognizer TREF."
B,P05-1045,W13-1101,0,"In general, language models could be used for more context-sensitive spelling correction. Given the preponderance of terms on the web, using a named entity recognizer (e.g., Finkel et al., 2005) for preprocessing may also provide benefits.","In general, language models could be used for more context-sensitive spelling correction. Given the preponderance of terms on the web, using a named entity recognizer (e.g., TREF) for preprocessing may also provide benefits."
B,P05-1045,W13-2114,1,"We use Stanford NER (Finkel et al., 2005) for named entity recognition. Figure 1 shows a sentence and its corresponding semantic pattern.",We use Stanford NER TREF for named entity recognition. Figure 1 shows a sentence and its corresponding semantic pattern.
B,P05-1045,W13-2234,1,"We extract further information indicating whether a named entity, as identified by the Stanford NE Recognizer (Finkel et al., 2005) begins at wi.","We extract further information indicating whether a named entity, as identified by the Stanford NE Recognizer TREF begins at wi."
B,P05-1045,W13-2414,0,"We have evaluated two schemas with a limited number of the NE categories. In the first more common (Finkel et al., 2005) schema, all PNs are divided into four MUC categories, i.e. person, organization, location and other","We have evaluated two schemas with a limited number of the NE categories. In the first more common TREF schema, all PNs are divided into four MUC categories, i.e. person, organization, location and other"
B,P05-1045,W13-3516,1,"We retrained the Stanford named entity recognizer (Finkel et al., 2005) on the OntoNotes data.",We retrained the Stanford named entity recognizer TREF on the OntoNotes data.
B,P05-1045,W13-3616,1,"External tools we used include WordNet (Fellbaum, 1998) for word base form and noun category generation, Morphg (Minnen et al., 2000) to generate inflections of nouns and verbs, matetools (Bohnet, 2010) for SRL, Stanford-ner (Finkel et al., 2005) for name entity extraction and Longman online dictionary for generation of noun countability and verb transitivity","External tools we used include WordNet REF for word base form and noun category generation, Morphg TREF to generate inflections of nouns and verbs, mate tools REF for SRL, Stanford-ner TREF for name entity extraction and Longman online dictionary for generation of noun countability and verb transitivity"
B,P05-1045,W13-4008,1,"To extract the extended targets, we capture named entities (NE) from the Wikipedia page of the debate topic (fetched using jsoup java library) using the Stanford Named Entity Recognizer (Finkel et al., 2005) and sort them based on their page occurrence count. Out of top-k (k = 20) NEs, some can belong to both of the debate topics","To extract the extended targets, we capture named entities (NE) from the Wikipedia page of the debate topic (fetched using jsoup java library) using the Stanford Named Entity Recognizer TREF and sort them based on their page occurrence count. Out of top-k (k = 20) NEs, some can belong to both of the debate topics"
